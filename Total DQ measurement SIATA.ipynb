{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to install if not installed yet\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install sklearn\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install haversine\n",
    "!pip install -U wxPython \n",
    "!pip install google\n",
    "!pip install google-api-core\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install google-cloud\n",
    "!pip install google-cloud-vision\n",
    "!pip install google.cloud.bigquery\n",
    "!pip install google.cloud.storage\n",
    "!pip install google-auth-oauthlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sn\n",
    "import requests\n",
    "import json\n",
    "import haversine as hs\n",
    "import wx\n",
    "import webbrowser\n",
    "\n",
    "import DQ2# Own defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Setup file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights\n",
    "mu                =0.9\n",
    "Accuracy          =0.20\n",
    "Precision         =0.07\n",
    "Confidence        =0.16\n",
    "Completeness      =0.10\n",
    "Timeliness        =0.12\n",
    "Data_Volume       =0.16\n",
    "Data_Redundancy   =0.02\n",
    "Concordance       =0.16\n",
    "\n",
    "Utility           =0.12\n",
    "Accessibility     =0.16\n",
    "Interpretability  =0.28\n",
    "Reputation        =0.12\n",
    "Artificiality     =0.20\n",
    "Access_Security   =0.12\n",
    "\n",
    "\n",
    "#Period\n",
    "start_time =\"2020-02-01 00:00:00\"\n",
    "end_time   =\"2020-02-29 23:59:00\"\n",
    "\n",
    "#Variable Inicialization\n",
    "node=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps should be like:\n",
    "\n",
    "0. Clean the whole dataset (the variables of interest):DONE\n",
    "1. For each citizen science (CC) node, get the groups (HOURLY GROUPS).\n",
    "2. For each group (hour in a CC node data), calculate the Dimension's DQ. (The functions should be applied to each group instead)\n",
    "3. Save the result file in the form: Node, Group (hour), DQ_1, DQ_2, DQ3, ... , DQIndex  (This is new)\n",
    "4. Average the previous result over the whole time to get Node, DQ_1_time_mean, DQ_2_time_mean, DQ3_ave, ... , DQIndex_time_mean (This it what we have currently)\n",
    "5. Average the previous result over all the nodes to get DQ_1_node_mean, DQ_2_node_mean, DQ3_node_mean, ... , DQIndex_node_mean (This is new)\n",
    "6. Export 3, 4 and 5 to a Google Sheets page each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citizen Scientist:  [1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 259, 261, 262, 265, 266, 267]\n",
      "Siata Stations:  [11, 12, 25, 28, 31, 37, 38, 44, 46, 48, 69, 6, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 94]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #Read Data from February\n",
    "    header_CC=[\"codigoSerial\", \"fecha\", \"hora\", \"fechaHora\", \"temperatura\", \"humedad_relativa\", \"pm1_df\", \"pm10_df\", \"pm25_df\", \"pm1_nova\", \"pm10_nova\", \"pm25_nova\", \"calidad_temperatura\", \"calidad_humedad_relativa\", \"calidad_pm1_df\", \"calidad_pm10_df\", \"calidad_pm25_df\", \"calidad_pm1_nova\", \"calidad_pm10_nova\", \"calidad_pm25_nova\"]\n",
    "    datatypes_CC={\"codigoSerial\":np.uint16, \"temperatura\":np.float16, \"humedad_relativa\":np.float16, \"pm1_df\":np.float32, \"pm10_df\":np.float32, \"pm25_df\":np.float32, \"pm1_nova\":np.float32, \"pm10_nova\":np.float32, \"pm25_nova\":np.float32}\n",
    "    path_for_CC_data=get_path('*.csv',\"Select Citizen Scientist *.csv file\")\n",
    "    df_CC = pd.read_csv(path_for_CC_data, header=None, names=header_CC, usecols=header_CC , dtype=datatypes_CC,parse_dates=[\"fecha\",\"hora\",\"fechaHora\"])\n",
    "    df_CC.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "    print(\"Source path for Citizen Science nodes data: \",path_for_CC_data)\n",
    "\n",
    "    #Data includes January, February and March\n",
    "    header_SS=[\"Fecha_Hora\",\"codigoSerial\",\"pm25\",\"calidad_pm25\",\"pm10\",\"calidad_pm10\"]\n",
    "    datatypes_SS={\"codigoSerial\":np.uint16,\"pm25\":np.float32,\"pm10\":np.float32}\n",
    "    path_for_SS_data=get_path('*.csv',\"Select SIATA Stations *.csv file\")\n",
    "    df_SS = pd.read_csv(path_for_SS_data, header=None,names=header_SS, usecols=header_SS , dtype=datatypes_SS,parse_dates=[\"Fecha_Hora\"])\n",
    "    df_SS.sort_values(by=['codigoSerial','Fecha_Hora'],ignore_index=True)\n",
    "    print(\"Source path for Siata Stations data: \",path_for_SS_data)\n",
    "        \n",
    "except:\n",
    "    print(\"An exception occurred, it is possible that wrong files were chosen, please run again\")\n",
    "\n",
    "datatypesDistances={\"codigoSerial_CC\":np.uint16,\"codigoSerial_ES\":np.uint16,\"Distancia_a_ES\":np.float16,\"codigoSerial_ES2\":np.uint16}\n",
    "Distances = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestr√≠a/DQ in IOT/Datasets/Distances and positions/Distancias_2.csv\", header=0, dtype=datatypesDistances,index_col=\"codigoSerial_CC\")\n",
    "\n",
    "#DATA CLEANING\n",
    "CC, SS=clean_sort_data(df_CC, df_SS)\n",
    "del df_CC\n",
    "del df_SS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Code with parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2021-09-27 23:13:29.733153\n",
      "Number of avaliables CPUs:  6\n",
      "End Time:  2021-09-27 23:20:38.388171\n",
      "Elapsed Time:  428.67064237594604\n"
     ]
    }
   ],
   "source": [
    "t0= time.time()\n",
    "print(\"Start time: \", datetime.fromtimestamp(t0))\n",
    "\n",
    "\n",
    "dim_time = pd.DataFrame(\n",
    "        columns =[\"codigoSerial\",\n",
    "                  \"fechaHora\",\n",
    "                  \"precision_df_time\",\n",
    "                  \"precision_nova_time\",\n",
    "                  \"uncertainty_time\",\n",
    "                  \"accuracy_df_time\",\n",
    "                  \"accuracy_nova_time\",\n",
    "                  \"completeness_df_time\",\n",
    "                  \"completeness_nova_time\",\n",
    "                  \"concordance_df_nova_time\",\n",
    "                  \"concordance_df_siata\",#MAYBE NEED TO BE CALCULATED ON A DAILY BASIS\n",
    "                  \"concordance_df_hum_time\",\n",
    "                  \"concordance_df_temp_time\",\n",
    "                  \"concordance_nova_siata\",#MAYBE NEED TO BE CALCULATED ON A DAILY BASIS\n",
    "                  \"concordance_nova_hum_time\",\n",
    "                  \"concordance_nova_temp_time\"])\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    print(\"Number of avaliable CPUs: \",mp.cpu_count())\n",
    "    pool=mp.Pool(processes = mp.cpu_count())\n",
    "    arguments=[]\n",
    "    #results=pool.map(DQ.eval_dq,[nodes for nodes in CC.keys()])\n",
    "    results=pool.map(DQ2.eval_dq,([[nodes, CC, SS, Distances, start_time, end_time] for nodes in CC.keys()]))\n",
    "    \n",
    "    #results[10]\n",
    "\n",
    "    for i in range(0,len(results)):\n",
    "        dim_time=dim_time.append(results[i], ignore_index = True)\n",
    "    \n",
    "print(\"End Time: \", datetime.fromtimestamp(time.time()))\n",
    "t1 = time.time() - t0\n",
    "print(\"Elapsed Time: \", t1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spreadsheetId': '1gkU9I2EqgJCLm-XemWq_oFenkOcsaHRKwDpO2kK7vd8', 'clearedRange': 'DQ_TIME!A1:Z60470'}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "import pandas as pd\n",
    "\n",
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "# The ID and range of a sample spreadsheet.\n",
    "SAMPLE_SPREADSHEET_ID = '1gkU9I2EqgJCLm-XemWq_oFenkOcsaHRKwDpO2kK7vd8'\n",
    "#SAMPLE_RANGE_NAME = 'Class Data!A2:E'\n",
    "\n",
    "def main():\n",
    "    \"\"\"Shows basic usage of the Sheets API.\n",
    "    Prints values from a sample spreadsheet.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "    # Call the Sheets API\n",
    "    \n",
    "    dim_time.replace(np.nan, '', inplace=True)\n",
    "    dim_time['fechaHora'] = dim_time['fechaHora'].astype(str)\n",
    "    \n",
    "    print(\"Clearing the page in the google Spread sheet\")\n",
    "    sheet = service.spreadsheets()\n",
    "    #request = service.spreadsheets().values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=range_, body=clear_values_request_body)\n",
    "    result = sheet.values().clear(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        range='DQ_TIME!A1:Z1000000',\n",
    "        \n",
    "    ).execute()\n",
    "    \n",
    "    print(\"Exporting Data to Google Sheets\")\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().update(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='DQ_TIME!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=dim_time.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "    print(\"Exporting Data Finished\")\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Open the Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webbrowser.open('https://docs.google.com/spreadsheets/d/1gkU9I2EqgJCLm-XemWq_oFenkOcsaHRKwDpO2kK7vd8/edit?usp=sharing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Open the Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webbrowser.open('https://datastudio.google.com/s/ietWLq_iL-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f295ec510066dddcc717ae6d4935b3ad0a45b533511561df7ff2b4bcd803903f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
