{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to install if not installed yet\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install sklearn\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install haversine\n",
    "!pip install -U wxPython \n",
    "!pip install google\n",
    "!pip install google-api-core\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install google-cloud\n",
    "!pip install google-cloud-vision\n",
    "!pip install google.cloud.bigquery\n",
    "!pip install google.cloud.storage\n",
    "!pip install google-auth-oauthlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sn\n",
    "import requests\n",
    "import json\n",
    "import haversine as hs\n",
    "import wx\n",
    "import webbrowser\n",
    "\n",
    "from __future__ import print_function\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "import DQ2# Own defined\n",
    "#hola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Setup file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights\n",
    "#mu                =0.9\n",
    "#Accuracy          =0.20\n",
    "#Precision         =0.07\n",
    "#Confidence        =0.16\n",
    "#Completeness      =0.10\n",
    "#Timeliness        =0.12\n",
    "#Data_Volume       =0.16\n",
    "#Data_Redundancy   =0.02\n",
    "#Concordance       =0.16\n",
    "#\n",
    "#Utility           =0.12\n",
    "#Accessibility     =0.16\n",
    "#Interpretability  =0.28\n",
    "#Reputation        =0.12\n",
    "#Artificiality     =0.20\n",
    "#Access_Security   =0.12\n",
    "\n",
    "\n",
    "mu              = 1.0\n",
    "Waccuracy        = 0.3506311521\n",
    "Wprecision       = 0.09875987987\n",
    "Wconfidence      = 0.1880884436\n",
    "Wcompleteness    = 0.148093351\n",
    "Wdata_Redundancy = 0.03756434625\n",
    "Wconcordance     = 0.1768628272\n",
    "pcmWeights = [Waccuracy,Wprecision,Wconfidence,Wcompleteness,Wdata_Redundancy,Wconcordance]\n",
    "\n",
    "#Period\n",
    "start_time =\"2019-12-20 00:00:00\"\n",
    "end_time   =\"2019-12-29 23:59:00\"\n",
    "\n",
    "#Variable Inicialization\n",
    "p=99 # P for the CI used in the confidence calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps should be like:\n",
    "\n",
    "0. Clean the whole dataset (the variables of interest):DONE\n",
    "1. For each citizen science (CC) node, get the groups (HOURLY GROUPS).\n",
    "2. For each group (hour in a CC node data), calculate the Dimension's DQ. (The functions should be applied to each group instead)\n",
    "3. Save the result file in the form: Node, Group (hour), DQ_1, DQ_2, DQ3, ... , DQIndex  (This is new)\n",
    "4. Average the previous result over the whole time to get Node, DQ_1_time_mean, DQ_2_time_mean, DQ3_ave, ... , DQIndex_time_mean (This it what we have currently)\n",
    "5. Average the previous result over all the nodes to get DQ_1_node_mean, DQ_2_node_mean, DQ3_node_mean, ... , DQIndex_node_mean (This is new)\n",
    "6. Export 3, 4 and 5 to a Google Sheets page each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path for Citizen Science nodes data:  C:\\Users\\julio\\Documents\\UDEA\\Maestría\\DQ in IOT\\Datasets\\Dataset for testing\\CC_PM_December.csv\n",
      "Source path for Siata Stations data:  C:\\Users\\julio\\Documents\\UDEA\\Maestría\\DQ in IOT\\Datasets\\Dataset for testing\\SS_PM December 2019 till March 2020.csv\n",
      "The distance files was read\n",
      "Citizen Scientist:  [1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 90, 91, 92, 94, 95, 96, 97, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 149, 150, 151, 152, 153, 154, 156, 157, 158, 160, 161, 162, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 179, 180, 181, 182, 185, 187, 188, 189, 190, 191, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 219, 221, 222, 223, 224, 225, 226, 228, 229, 230, 231, 232, 233, 234, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 257, 259, 261, 262, 265, 266, 267]\n",
      "Siata Stations:  [11, 12, 25, 28, 31, 37, 38, 44, 46, 48, 69, 6, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 94]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "#Read Data from February\n",
    "    header_CC=[\"codigoSerial\", \"fecha\", \"hora\", \"fechaHora\", \"temperatura\", \"humedad_relativa\", \"pm1_df\", \"pm10_df\", \"pm25_df\", \"pm1_nova\", \"pm10_nova\", \"pm25_nova\", \"calidad_temperatura\", \"calidad_humedad_relativa\", \"calidad_pm1_df\", \"calidad_pm10_df\", \"calidad_pm25_df\", \"calidad_pm1_nova\", \"calidad_pm10_nova\", \"calidad_pm25_nova\"]\n",
    "    datatypes_CC={\"codigoSerial\":np.uint16, \"temperatura\":np.float16, \"humedad_relativa\":np.float16, \"pm1_df\":np.float32, \"pm10_df\":np.float32, \"pm25_df\":np.float32, \"pm1_nova\":np.float32, \"pm10_nova\":np.float32, \"pm25_nova\":np.float32}\n",
    "    path_for_CC_data=DQ2.get_path('*.csv',\"Select Citizen Scientist *.csv file\")\n",
    "    df_CC = pd.read_csv(path_for_CC_data, header=None, names=header_CC, usecols=header_CC , dtype=datatypes_CC,parse_dates=[\"fecha\",\"hora\",\"fechaHora\"])\n",
    "    df_CC.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "    print(\"Source path for Citizen Science nodes data: \",path_for_CC_data)\n",
    "    \n",
    "    #Data includes January, February and March\n",
    "    header_SS=[\"Fecha_Hora\",\"codigoSerial\",\"pm25\",\"calidad_pm25\",\"pm10\",\"calidad_pm10\"]\n",
    "    datatypes_SS={\"codigoSerial\":np.uint16,\"pm25\":np.float32,\"pm10\":np.float32}\n",
    "    path_for_SS_data=DQ2.get_path('*.csv',\"Select SIATA Stations *.csv file\")\n",
    "    df_SS = pd.read_csv(path_for_SS_data, header=None,names=header_SS, usecols=header_SS , dtype=datatypes_SS,parse_dates=[\"Fecha_Hora\"])\n",
    "    df_SS.sort_values(by=['codigoSerial','Fecha_Hora'],ignore_index=True)\n",
    "    print(\"Source path for Siata Stations data: \",path_for_SS_data)\n",
    "    \n",
    "    \n",
    "    datatypesDistances={\"codigoSerial_CC\":np.uint16,\"codigoSerial_ES\":np.uint16,\"Distancia_a_ES\":np.float16,\"codigoSerial_ES2\":np.uint16}\n",
    "    path_for_distance_files=DQ2.get_path('*.csv',\"Select the Nodes to Siata Stations distances *.csv file\")\n",
    "    Distances = pd.read_csv(path_for_distance_files, header=0, dtype=datatypesDistances,index_col=\"codigoSerial_CC\")\n",
    "    print(\"The distance files was read\")\n",
    "    \n",
    "        \n",
    "except:\n",
    "    print(\"An exception occurred, it is possible that wrong files were chosen, please run again\")\n",
    "\n",
    "\n",
    "\n",
    "#DATA CLEANING\n",
    "CC, SS=DQ2.clean_sort_data(df_CC, df_SS)\n",
    "del df_CC\n",
    "del df_SS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Code with parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2021-10-05 16:23:14.959601\n",
      "Number of avaliable CPUs:  6\n",
      "End Time:  2021-10-05 16:26:20.082072\n",
      "Elapsed Time:  185.12612056732178  Seconds, or  3.0854353427886965  Minutes\n"
     ]
    }
   ],
   "source": [
    "import DQ2# Own defined\n",
    "t0= time.time()\n",
    "print(\"Start time: \", datetime.fromtimestamp(t0))\n",
    "\n",
    "\n",
    "dim_time = pd.DataFrame(\n",
    "        columns =[\"codigoSerial\",\n",
    "                  \"fechaHora\",\n",
    "                  \"precision_df_time\",\n",
    "                  \"precision_nova_time\",\n",
    "                  \"uncertainty_time\",\n",
    "                  \"accuracy_df_time\",\n",
    "                  \"accuracy_nova_time\",\n",
    "                  \"completeness_df_time\",\n",
    "                  \"completeness_nova_time\",\n",
    "                  \"concordance_df_nova_time\",\n",
    "                  \"concordance_df_siata_time\",\n",
    "                  \"concordance_df_hum_time\",\n",
    "                  \"concordance_df_temp_time\",\n",
    "                  \"concordance_nova_siata_time\",\n",
    "                  \"concordance_nova_hum_time\",\n",
    "                  \"concordance_nova_temp_time\",\n",
    "                  \"vm_df\",\n",
    "                  \"vm_nova\",\n",
    "                  \"v\",\n",
    "                  \"duplicates_time\",\n",
    "                  \n",
    "                  \"confi_df_time\",\n",
    "                  \"confi_nova_time\"])\n",
    "\n",
    "dim_node = pd.DataFrame(\n",
    "        columns =[\"codigoSerial\",\n",
    "                  \"precision_df_node\",\n",
    "                  \"precision_nova_node\",\n",
    "                  \"uncertainty_node\",\n",
    "                  \"accuracy_df_node\",\n",
    "                  \"accuracy_nova_node\",\n",
    "                  \"completeness_df_node\",\n",
    "                  \"completeness_nova_node\",\n",
    "                  \"concordance_df_nova_node\",\n",
    "                  \"concordance_df_siata_node\",\n",
    "                  \"concordance_df_hum_node\",\n",
    "                  \"concordance_df_temp_node\",\n",
    "                  \"concordance_nova_siata_node\",\n",
    "                  \"concordance_nova_hum_node\",\n",
    "                  \"concordance_nova_temp_node\",\n",
    "                  \n",
    "                  \"duplicates_node\",\n",
    "                  \n",
    "                  \"confi_df_node\",\n",
    "                  \"confi_nova_node\",\n",
    "                  \"DQ_INDEX_NODE\"])\n",
    "\n",
    "dim_DQ = pd.DataFrame(\n",
    "        columns =[\"precision_df_total\",\n",
    "                  \"precision_nova_total\",\n",
    "                  \"uncertainty_total\",\n",
    "                  \"accuracy_df_total\",\n",
    "                  \"accuracy_nova_total\",\n",
    "                  \"completeness_df_total\",\n",
    "                  \"completeness_nova_total\",\n",
    "                  \"concordance_df_nova_total\",\n",
    "                  \"concordance_df_siata_total\",#MAYBE NEED TO BE CALCULATED ON A DAILY BASIS\n",
    "                  \"concordance_df_hum_total\",\n",
    "                  \"concordance_df_temp_total\",\n",
    "                  \"concordance_nova_siata_total\",#MAYBE NEED TO BE CALCULATED ON A DAILY BASIS\n",
    "                  \"concordance_nova_hum_total\",\n",
    "                  \"concordance_nova_temp_total\",\n",
    "                  \n",
    "                  \"duplicates_total\",\n",
    "                  \n",
    "                  \"confi_df_total\",\n",
    "                  \"confi_nova_total\",\n",
    "                  \"DQ_INDEX_TOTAL\"])\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    print(\"Number of avaliable CPUs: \",mp.cpu_count())\n",
    "    pool=mp.Pool(processes = mp.cpu_count())\n",
    "    arguments=[]\n",
    "    #results=pool.map(DQ.eval_dq,[nodes for nodes in CC.keys()])\n",
    "    results=pool.map(DQ2.eval_dq,([[nodes, CC, SS, Distances, start_time, end_time, p] for nodes in CC.keys()]))\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(0,len(results)):\n",
    "        dim_time=dim_time.append(results[i][0], ignore_index = True)\n",
    "        dim_node=dim_node.append(results[i][1], ignore_index = True)\n",
    "\n",
    "        \n",
    "    cols =[       \"precision_df_node\",\n",
    "                  \"precision_nova_node\",\n",
    "                  \"uncertainty_node\",\n",
    "                  \"accuracy_df_node\",\n",
    "                  \"accuracy_nova_node\",\n",
    "                  \"completeness_df_node\",\n",
    "                  \"completeness_nova_node\",\n",
    "                  \"concordance_df_nova_node\",\n",
    "                  \"concordance_df_siata_node\",\n",
    "                  \"concordance_df_hum_node\",\n",
    "                  \"concordance_df_temp_node\",\n",
    "                  \"concordance_nova_siata_node\",\n",
    "                  \"concordance_nova_hum_node\",\n",
    "                  \"concordance_nova_temp_node\",\n",
    "                  \n",
    "                  \"duplicates_node\",\n",
    "                  \n",
    "                  \"confi_df_node\",\n",
    "                  \"confi_nova_node\"]    \n",
    "    dim_DQ= dim_node[cols].mean()\n",
    "    dim_DQ.rename({'precision_df_node':          'precision_df_total', \n",
    "                   'precision_nova_node':        'precision_nova_total' , \n",
    "                   'uncertainty_node':           'uncertainty_total' , \n",
    "                   'accuracy_df_node':           'accuracy_df_total', \n",
    "                   'accuracy_nova_node':         'accuracy_nova_total', \n",
    "                   'completeness_df_node':       'completeness_df_total', \n",
    "                   'completeness_nova_node':     'completeness_nova_total', \n",
    "                   'concordance_df_nova_node':   'concordance_df_nova_total', \n",
    "                   'concordance_df_siata_node':  'concordance_df_siata_total', \n",
    "                   'concordance_df_hum_node':    'concordance_df_hum_total', \n",
    "                   'concordance_df_temp_node':   'concordance_df_temp_total', \n",
    "                   'concordance_nova_siata_node':'concordance_nova_siata_total', \n",
    "                   'concordance_nova_hum_node':  'concordance_nova_hum_total', \n",
    "                   'concordance_nova_temp_node': 'concordance_nova_temp_total', \n",
    "                   'duplicates_node':            'duplicates_total', \n",
    "                   'confi_df_node':              'confi_df_total', \n",
    "                   'confi_nova_node':            'confi_nova_total'        }, axis=1, inplace=True)\n",
    "    \n",
    "print(\"End Time: \", datetime.fromtimestamp(time.time()))\n",
    "t1 = time.time() - t0\n",
    "print(\"Elapsed Time: \", t1, \" Seconds, or \",t1/60,\" Minutes\")\n",
    "#print(dim_node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQ_INDEX by a weighted average function. The weights come from the Pair-Wise Comparison Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_node[\"DQ_INDEX_NODE\"]=  Wprecision*dim_node.loc[:,[\"precision_df_node\",\"precision_nova_node\"]].mean(axis=1,skipna=False)+\\\n",
    "                            Waccuracy*dim_node.loc[:,[\"accuracy_df_node\",\"accuracy_nova_node\"]].mean(axis=1,skipna=False)+\\\n",
    "                            Wcompleteness*dim_node.loc[:,[\"completeness_df_node\",\"completeness_nova_node\"]].mean(axis=1,skipna=False)+\\\n",
    "                            Wconfidence*dim_node.loc[:,[\"confi_df_node\",\"confi_nova_node\"]].mean(axis=1,skipna=False)+\\\n",
    "                            Wconcordance*dim_node.loc[:,[\"concordance_df_nova_node\",\"concordance_df_siata_node\",\"concordance_nova_siata_node\"]].mean(axis=1,skipna=False)+\\\n",
    "                            Wdata_Redundancy*dim_node.loc[:,[\"duplicates_node\"]].mean(axis=1,skipna=False)\n",
    "\n",
    "#dim_node[\"precision_node\"]= Wprecision*dim_node.loc[:,[\"precision_df_node\",\"precision_nova_node\"]].mean(axis=1)\n",
    "#dim_node[\"accuracy_node\"]= Waccuracy*dim_node.loc[:,[\"accuracy_df_node\",\"accuracy_nova_node\"]].mean(axis=1)\n",
    "#dim_node[\"completeness_node\"]= Wcompleteness*dim_node.loc[:,[\"completeness_df_node\",\"completeness_nova_node\"]].mean(axis=1)\n",
    "#dim_node[\"confidence_node\"]= Wconfidence*dim_node.loc[:,[\"confi_df_node\",\"confi_nova_node\"]].mean(axis=1)\n",
    "#dim_node[\"concordance_node\"]= Wconcordance*dim_node.loc[:,[\"concordance_df_nova_node\",\"concordance_df_siata_node\",\"concordance_nova_siata_node\"]].mean(axis=1)\n",
    "#dim_node[\"redundancy_node\"]= Wdata_Redundancy*dim_node.loc[:,[\"duplicates_node\"]].mean(axis=1)\n",
    "#dim_node[\"DQ_INDEX_NODE\"]=dim_node[[\"precision_node\",\"accuracy_node\",\"completeness_node\",\"confidence_node\",\"concordance_node\",\"redundancy_node\"]].sum(axis=1)\n",
    "\n",
    "dim_DQ[\"DQ_INDEX_TOTAL\"]=   Wprecision*dim_DQ[[\"precision_df_total\",\"precision_nova_total\"]].mean()+ \\\n",
    "                            Waccuracy*dim_DQ[[\"accuracy_df_total\",\"accuracy_nova_total\"]].mean()+ \\\n",
    "                            Wcompleteness*dim_DQ[[\"completeness_df_total\",\"completeness_nova_total\"]].mean()+ \\\n",
    "                            Wconfidence*dim_DQ[[\"confi_df_total\",\"confi_nova_total\"]].mean()+ \\\n",
    "                            Wconcordance*dim_DQ[[\"concordance_df_nova_total\",\"concordance_df_siata_total\",\"concordance_nova_siata_total\"]].mean()+ \\\n",
    "                            Wdata_Redundancy*dim_DQ[[\"duplicates_total\"]].mean()\n",
    "#dim_node.head(6)\n",
    "#dim_node.loc[dim_node.codigoSerial==49,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing the DQ_TIME sheet in the Google Spreadsheet\n",
      "Clearing the DQ_NODE sheet in the Google Spreadsheet\n",
      "Clearing the DQ_TOTAL sheet in the Google Spreadsheet\n",
      "Exporting Data to Google Sheets\n",
      "Exporting Data Finished\n"
     ]
    }
   ],
   "source": [
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "# The ID and range of a sample spreadsheet.\n",
    "SAMPLE_SPREADSHEET_ID = '1gkU9I2EqgJCLm-XemWq_oFenkOcsaHRKwDpO2kK7vd8'\n",
    "#SAMPLE_RANGE_NAME = 'Class Data!A2:E'\n",
    "\n",
    "def main():\n",
    "    \"\"\"Shows basic usage of the Sheets API.\n",
    "    Prints values from a sample spreadsheet.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "    # Call the Sheets API\n",
    "    \n",
    "    dim_time.replace(np.nan, '', inplace=True)\n",
    "    dim_time['fechaHora'] = dim_time['fechaHora'].astype(str)\n",
    "    #dim_time.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "    dim_time.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "    \n",
    "    dim_node.replace(np.nan, '', inplace=True)\n",
    "    #dim_time.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "    dim_node.sort_values(by=['codigoSerial'],ignore_index=True)\n",
    "    \n",
    "    dim_DQ.replace(np.nan, '', inplace=True)\n",
    "    #dim_time.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "        \n",
    "    \n",
    "    print(\"Clearing the DQ_TIME sheet in the Google Spreadsheet\")\n",
    "    sheet = service.spreadsheets()\n",
    "    #request = service.spreadsheets().values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=range_, body=clear_values_request_body)\n",
    "    result = sheet.values().clear(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        range='DQ_TIME!A1:Z1000000',\n",
    "        \n",
    "    ).execute()\n",
    "    \n",
    "    print(\"Clearing the DQ_NODE sheet in the Google Spreadsheet\")\n",
    "    sheet = service.spreadsheets()\n",
    "    #request = service.spreadsheets().values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=range_, body=clear_values_request_body)\n",
    "    result = sheet.values().clear(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        range='DQ_NODE!A1:Z1000000',\n",
    "        \n",
    "    ).execute()\n",
    "    \n",
    "    print(\"Clearing the DQ_TOTAL sheet in the Google Spreadsheet\")\n",
    "    sheet = service.spreadsheets()\n",
    "    #request = service.spreadsheets().values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=range_, body=clear_values_request_body)\n",
    "    result = sheet.values().clear(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        range='DQ_TOTAL!A1:Z1000000',\n",
    "        \n",
    "    ).execute()\n",
    "    \n",
    "    print(\"Exporting Data to Google Sheets\")\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().update(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='DQ_TIME!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=dim_time.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "    \n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().update(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='DQ_NODE!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=dim_node.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "    \n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().update(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='DQ_TOTAL!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=dim_DQ.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "    \n",
    "    print(\"Exporting Data Finished\")\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Open the Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webbrowser.open('https://docs.google.com/spreadsheets/d/1gkU9I2EqgJCLm-XemWq_oFenkOcsaHRKwDpO2kK7vd8/edit?usp=sharing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Open the Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webbrowser.open('https://datastudio.google.com/s/ietWLq_iL-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1=\"2020-02-03 23:59:59\"\n",
    "T2=\"2020-02-03 00:00:00\"\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "print(pd.Timestamp(T1)>=pd.Timestamp(T2).floor('1D') and pd.Timestamp(T1)<(pd.Timestamp(T2)+timedelta(minutes=1)).ceil('1D'))\n",
    "print(\"Range: [\",pd.Timestamp(T2).floor('1D'),\",\", (pd.Timestamp(T2)+timedelta(minutes=1)).ceil('1D'),\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arguments=[nodes=10, CC=CC, SS=CC, Distances=Distances, start_time=start_time, end_time=end_time]\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sn\n",
    "import requests\n",
    "import json\n",
    "import haversine as hs\n",
    "import wx\n",
    "#hola\n",
    "\n",
    "#FUNCTION TO READ THE PATH WITH A DIALOG BOX\n",
    "def get_path(wildcard, title):\n",
    "    app = wx.App(None)\n",
    "    style = wx.FD_OPEN | wx.FD_FILE_MUST_EXIST\n",
    "    dialog = wx.FileDialog(None, title, wildcard=wildcard, style=style)\n",
    "    if dialog.ShowModal() == wx.ID_OK:\n",
    "        path = dialog.GetPath()\n",
    "    else:\n",
    "        path = None\n",
    "    dialog.Destroy()\n",
    "    return path\n",
    "\n",
    "#FUNCTION TO CLEAN AND SORT THE READ DATA.\n",
    "def clean_sort_data(df_CC, df_SS):\n",
    "    \n",
    "    #ClEAN AND SORT THE CITIZEN SCIENCE DATASET\n",
    "    #Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999], [-40,70], [1,100]\n",
    "    #For PM2.5, Temperature and Relative Humidity Respectively.\n",
    "    df_CC=df_CC.copy()\n",
    "    df_CC.loc[df_CC[\"pm25_nova\"]>999,\"pm25_nova\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"pm25_nova\"]<0,\"pm25_nova\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"pm25_df\"]>999,\"pm25_df\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"pm25_df\"]<0,\"pm25_df\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"temperatura\"]>70,\"temperatura\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"temperatura\"]<-40,\"temperatura\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"humedad_relativa\"]>100,\"humedad_relativa\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"humedad_relativa\"]<1,\"humedad_relativa\"]=np.nan\n",
    "\n",
    "    \n",
    "    #Remove data above the whiskers of the boxplot: i.e. anomaly data\n",
    "    Q1 = df_CC['pm25_df'].quantile(0.25)\n",
    "    Q3 = df_CC['pm25_df'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_CC.loc[df_CC[\"pm25_df\"]>=Q3 + 1.5 *IQR,\"pm25_df\"]=np.nan\n",
    "    \n",
    "    Q1 = df_CC['pm25_nova'].quantile(0.25)\n",
    "    Q3 = df_CC['pm25_nova'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_CC.loc[df_CC[\"pm25_nova\"]>=Q3 + 1.5 *IQR,\"pm25_nova\"]=np.nan\n",
    "    \n",
    "    Q1 = df_CC['temperatura'].quantile(0.25)\n",
    "    Q3 = df_CC['temperatura'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_CC.loc[df_CC[\"temperatura\"]>=Q3 + 1.5 *IQR,\"temperatura\"]=np.nan\n",
    "    \n",
    "    Q1 = df_CC['humedad_relativa'].quantile(0.25)\n",
    "    Q3 = df_CC['humedad_relativa'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_CC.loc[df_CC[\"humedad_relativa\"]>=Q3 + 1.5 *IQR,\"humedad_relativa\"]=np.nan\n",
    "    \n",
    "    #CREATE A DICTINARY CONTAINING THE DATASETS PER NODE\n",
    "    grouped=df_CC.groupby(df_CC.codigoSerial)\n",
    "    CC={}\n",
    "    print(\"Citizen Scientist: \", sorted(list(df_CC.codigoSerial.unique())))\n",
    "    for i in df_CC.codigoSerial.unique():\n",
    "        CC[i] = grouped.get_group(i).sort_values(by=['fechaHora'],ignore_index=True)\n",
    "    \n",
    "    #ClEAN AND SORT THE SIATA STATIONS DATASET\n",
    "    #Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999]\n",
    "    df_SS=df_SS.copy()\n",
    "    df_SS.loc[df_SS[\"pm25\"]>999,\"pm25\"]=np.nan\n",
    "    df_SS.loc[df_SS[\"pm25\"]<0,\"pm25\"]=np.nan\n",
    "\n",
    "    \n",
    "    #Remove data above the whiskers of the boxplot: i.e. anomaly data\n",
    "    Q1 = df_SS['pm25'].quantile(0.25)\n",
    "    Q3 = df_SS['pm25'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_SS.loc[df_SS[\"pm25\"]>=Q3 + 1.5 *IQR,\"pm25\"]=np.nan\n",
    "    \n",
    "    #CREATE A DICTINARY CONTAINING THE DATASETS PER SIATA STATION\n",
    "    grouped=df_SS.groupby(df_SS.codigoSerial)\n",
    "    SS={}\n",
    "    print(\"Siata Stations: \", list(df_SS.codigoSerial.unique()))\n",
    "    for j in df_SS.codigoSerial.unique():\n",
    "        SS[j] = grouped.get_group(j).sort_values(by=['Fecha_Hora'],ignore_index=True)\n",
    "    \n",
    "    del grouped\n",
    "    return CC, SS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Precision\n",
    "def precision(window):\n",
    "\n",
    "    prec_df=1-(window['pm25_df'].std()/window['pm25_df'].mean())# 1-Coefficient of Variation (std/mean)\n",
    "    prec_nova=1-(window['pm25_nova'].std()/window['pm25_nova'].mean())# 1-Coefficient of Variation (std/mean)\n",
    "    prec_dict_time={\"precision_df_time\":prec_df,\"precision_nova_time\":prec_nova}\n",
    "    return prec_dict_time\n",
    "\n",
    "\n",
    "#Uncertainty\n",
    "def uncertainty(window):\n",
    "    uncert=1-np.sqrt((window.pm25_df-window.pm25_nova).pow(2).mean()/2)/((window.pm25_df+window.pm25_nova).mean()/2)\n",
    "    uncer_dict_time={\"uncertainty_time\":uncert}\n",
    "    return uncer_dict_time   \n",
    "\n",
    "\n",
    "#ACCURACY\n",
    "def accuracy(node,hour, window, Distances, SS):\n",
    "    \n",
    "    pm25_df_ave=window['pm25_df'].mean()\n",
    "    pm25_nova_ave=window['pm25_nova'].mean()\n",
    "    #print(pm25_df_ave, pm25_nova_ave)\n",
    "    Closest_Station=Distances.codigoSerial_ES.loc[node]    \n",
    "    Closest_Station2=Distances.codigoSerial_ES.loc[node] \n",
    "    #print(pm25_df_ave,pm25_nova_ave,Closest_Station,Closest_Station2)\n",
    "    #print(Closest_Station in SS.keys())\n",
    "    #print(hour in SS[Closest_Station].Fecha_Hora.values, hour)\n",
    "   # print(SS[Closest_Station].Fecha_Hora)\n",
    "    \n",
    "    if (Closest_Station in SS.keys()) and (hour in SS[Closest_Station].Fecha_Hora.values):\n",
    "        \n",
    "        #print(SS[Closest_Station])\n",
    "        #df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_ave']\n",
    "        v=SS[Closest_Station].loc[SS[Closest_Station].Fecha_Hora==hour,\"pm25\"].values[0]\n",
    "        #print()\n",
    "        #print(pm25_df_ave)\n",
    "        accur_df=  max(0,1-abs(pm25_df_ave-v)/v)\n",
    "        accur_nova= max(0,1-abs(pm25_nova_ave-v)/v)\n",
    "    \n",
    "    elif (Closest_Station2 in SS.keys()) and  (hour in SS[Closest_Station2].Fecha_Hora.values):\n",
    "        \n",
    "        v=SS[Closest_Station2].loc[SS[Closest_Station2].Fecha_Hora==hour,\"pm25\"].values[0]\n",
    "        #v=SS[Closest_Station2].loc[hour,\"pm25\"]\n",
    "        accur_df=  max(0,1-abs(pm25_df_ave-v)/v)\n",
    "        accur_nova= max(0,1-abs(pm25_nova_ave-v)/v)\n",
    "    else:\n",
    "        accur_df=np.nan\n",
    "        accur_nova=np.nan\n",
    "    accu_dict_time={'accuracy_df_time':accur_df, 'accuracy_nova_time':accur_nova}\n",
    "    return accu_dict_time\n",
    "\n",
    "\n",
    "#CONCORDANCE\n",
    "def concordance(node, hour, window, Distances, SS):\n",
    "    Closest_Station=Distances.codigoSerial_ES.loc[node]\n",
    "    Closest_Station2=Distances.codigoSerial_ES.loc[node]\n",
    "    window=window.copy()\n",
    "    if (Closest_Station in SS.keys()) and (hour in SS[Closest_Station].Fecha_Hora.values):\n",
    "        \n",
    "   \n",
    "        v=SS[Closest_Station].loc[SS[Closest_Station].Fecha_Hora==hour,\"pm25\"].values[0]\n",
    "        vm=window.pm25_df.mean()\n",
    "        pair=[vm,v]\n",
    "        window.loc[:,\"v_pm25\"]=v\n",
    "        \n",
    "        corr_df =   window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[0].abs()\n",
    "        corr_nova = window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[1].abs()\n",
    "        concordance_df_nova=corr_df.pm25_nova\n",
    "        concordance_df_siata=corr_df.v_pm25\n",
    "        concordance_df_hum=corr_df.humedad_relativa\n",
    "        concordance_df_temp=corr_df.temperatura\n",
    "        concordance_nova_siata=corr_nova.v_pm25\n",
    "        concordance_nova_hum=corr_nova.humedad_relativa\n",
    "        concordance_nova_temp=corr_nova.temperatura\n",
    "   \n",
    "            \n",
    "    elif (Closest_Station2 in SS.keys()) and (hour in SS[Closest_Station2].Fecha_Hora.values):\n",
    "        \n",
    "        v=SS[Closest_Station2].loc[SS[Closest_Station2].Fecha_Hora==hour,\"pm25\"].values[0]\n",
    "        vm=window.pm25_df.mean()\n",
    "        pair=[vm,v]\n",
    "        window.loc[:,\"v_pm25\"]=v\n",
    "        \n",
    "        corr_df =   window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[0].abs()\n",
    "        corr_nova = window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[1].abs()\n",
    "        concordance_df_nova=corr_df.pm25_nova\n",
    "        concordance_df_siata=corr_df.v_pm25\n",
    "        concordance_df_hum=corr_df.humedad_relativa\n",
    "        concordance_df_temp=corr_df.temperatura\n",
    "        concordance_nova_siata=corr_nova.v_pm25\n",
    "        concordance_nova_hum=corr_nova.humedad_relativa\n",
    "        concordance_nova_temp=corr_nova.temperatura\n",
    "    else:\n",
    "        concordance_df_nova=np.nan\n",
    "        concordance_df_siata=np.nan\n",
    "        concordance_df_hum=np.nan\n",
    "        concordance_df_temp=np.nan\n",
    "        concordance_nova_siata=np.nan\n",
    "        concordance_nova_hum=np.nan\n",
    "        concordance_nova_temp=np.nan\n",
    "        \n",
    "        pair=[np.nan,np.nan]\n",
    "       \n",
    "    conco_dict={\"concordance_df_nova_time\":concordance_df_nova,\n",
    "                \"concordance_df_siata\":concordance_df_siata,\n",
    "                \"concordance_df_hum_time\":concordance_df_hum,\n",
    "                \"concordance_df_temp_time\":concordance_df_temp,\n",
    "                \"concordance_nova_siata\":concordance_nova_siata,\n",
    "                \"concordance_nova_hum_time\":concordance_nova_hum,\n",
    "                \"concordance_nova_temp_time\":concordance_nova_temp,\n",
    "                \"vm\":pair[0],\n",
    "                \"v\":pair[1]}\n",
    "    return conco_dict\n",
    "\n",
    "\n",
    "#COMPLETENESS\n",
    "def completeness(node, window,start_time, end_time):\n",
    "    \n",
    "    if window.fechaHora.min()==start_time and window.fechaHora.max()==end_time:\n",
    "        ref_date_range = pd.DataFrame(pd.date_range(start_time,end_time, freq='1Min'),columns=[\"ref_fechaHora\"])\n",
    "    \n",
    "    elif window.fechaHora.min()==start_time:\n",
    "        ref_date_range = pd.DataFrame(pd.date_range(start_time, pd.Timestamp(start_time).ceil('60min')-timedelta(minutes=1), freq='1Min'),columns=[\"ref_fechaHora\"])\n",
    "        \n",
    "    elif window.fechaHora.max()==end_time:\n",
    "        ref_date_range = pd.DataFrame(pd.date_range(pd.Timestamp(end_time).floor('60min')-timedelta(minutes=1),pd.Timestamp(end_time), freq='1Min'),columns=[\"ref_fechaHora\"])\n",
    "    \n",
    "    else:\n",
    "        ref_date_range = pd.DataFrame(pd.date_range(window.fechaHora.min().floor('60min'),window.fechaHora.min().floor('60min')+timedelta(minutes=59), freq='1Min'),columns=[\"ref_fechaHora\"])\n",
    "        \n",
    "    #Check for any missing date\n",
    "    missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(window.fechaHora),\"ref_fechaHora\"]\n",
    "    \n",
    "    #Add missing date rows\n",
    "    for missing in missing_dates:\n",
    "        window=window.append({\"codigoSerial\":node,\"fechaHora\":missing}, ignore_index = True)\n",
    "    #Check for missing data\n",
    "    missing_data_df=np.count_nonzero(np.isnan(window['pm25_df']))\n",
    "    missing_data_nova=np.count_nonzero(np.isnan(window['pm25_nova']))\n",
    "    comp_df=(1-missing_data_df/np.size(window.pm25_df))\n",
    "    comp_nova=(1-missing_data_nova/np.size(window.pm25_nova))\n",
    "    comp_dict_time={'completeness_df_time':comp_df, 'completeness_nova_time':comp_nova}        \n",
    "    return comp_dict_time\n",
    "\n",
    "def duplicates(window):\n",
    "    repeated=len(window.index)-window['fechaHora'].nunique()\n",
    "    duplic=1-repeated/len(window.index)\n",
    "    dupli_dict_time={'duplicates_time':duplic} \n",
    "    return dupli_dict_time\n",
    "\n",
    "def eval_dq(arguments):\n",
    "    nodes=arguments[0]\n",
    "    CC=arguments[1]\n",
    "    SS=arguments[2]\n",
    "    Distances=arguments[3]\n",
    "    start_time=arguments[4]\n",
    "    end_time=arguments[5]\n",
    "    \n",
    "    #1. For each citizen science (CC) node, get the groups (HOURLY GROUPS).\n",
    "    #node_dataset=CC[nodes]\n",
    "    node_dataset=CC[nodes][(CC[nodes]['fechaHora'] >= start_time) & (CC[nodes]['fechaHora'] <= end_time)]\n",
    "    #print(node_dataset)\n",
    "    #times = pd.to_datetime(node_dataset.fechaHora)\n",
    "    hourly_groups=node_dataset.groupby([node_dataset.fechaHora.dt.floor('60min')])#Para agrupar por cada hora\n",
    "    #hourly_groups.groups.keys()# O grupos.groups para obtener las claves de cada grupo, es decir cada hora\n",
    "    del node_dataset\n",
    "    \n",
    "\n",
    "    dim_time = pd.DataFrame(\n",
    "        columns =[\"codigoSerial\",\n",
    "                  \"fechaHora\",\n",
    "                  \"precision_df_time\",\n",
    "                  \"precision_nova_time\",\n",
    "                  \"uncertainty_time\",\n",
    "                  \"accuracy_df_time\",\n",
    "                  \"accuracy_nova_time\",\n",
    "                  \"completeness_df_time\",\n",
    "                  \"completeness_nova_time\",\n",
    "                  \n",
    "                  \"concordance_df_nova_time\",\n",
    "                  \n",
    "                  \"concordance_df_siata\",#MAYBE NEED TO BE CALCULATED ON A DAILY BASIS\n",
    "                  \"concordance_df_hum_time\",\n",
    "                  \"concordance_df_temp_time\",\n",
    "                  \n",
    "                  \"concordance_nova_siata\",#MAYBE NEED TO BE CALCULATED ON A DAILY BASIS\n",
    "                  \"concordance_nova_hum_time\",\n",
    "                  \"concordance_nova_temp_time\",\n",
    "                  \"vm\",\n",
    "                  \"v\",\n",
    "                 \n",
    "                  \"duplicates_time\"])\n",
    "    \n",
    "    #2. For each group (hour in a CC node data), calculate the Dimension's DQ. (The functions should be applied to each group instead)\n",
    "    for hour in hourly_groups.groups.keys():\n",
    "        window=hourly_groups.get_group(hour)\n",
    "        \n",
    "        \n",
    "        preci_dict_time=precision(window)\n",
    "        uncer_dict_time=uncertainty(window)\n",
    "        accur_dict_time=accuracy(nodes, hour, window, Distances, SS)\n",
    "        conco_dict_time=concordance(nodes, hour, window, Distances, SS)\n",
    "        comp_dict_time=completeness(nodes, window,start_time, end_time)\n",
    "        dupli_dict_time=duplicates(window)\n",
    "        \n",
    "        DQ_dict_time={\"codigoSerial\":nodes,\"fechaHora\":pd.Timestamp(hour)}\n",
    "        DQ_dict_time.update(preci_dict_time)\n",
    "        DQ_dict_time.update(uncer_dict_time)\n",
    "        DQ_dict_time.update(accur_dict_time)\n",
    "        DQ_dict_time.update(conco_dict_time)\n",
    "        DQ_dict_time.update(comp_dict_time)\n",
    "        DQ_dict_time.update(dupli_dict_time)\n",
    "        \n",
    "        \n",
    "        #3. Save the result file in the form: Node, Group (hour), DQ_1, DQ_2, DQ3, ... , DQIndex  (This is new)\n",
    "        dim_time=dim_time.append(DQ_dict_time, ignore_index = True)\n",
    "    \n",
    "    dim_time['fechaHora']= pd.to_datetime(dim_time['fechaHora'])\n",
    "    daily_groups=dim_time.groupby([dim_time.fechaHora.dt.floor('1D')])#Para agrupar por cada día\n",
    "    print(\"here we go\")\n",
    "    #hola\n",
    "    for day in daily_groups.groups.keys():\n",
    "        day_window=daily_groups.get_group(day)\n",
    "        #dim_time.loc[dim_time.fechaHora<=day,\"concordance_df_siata\"]=day_window.v.corr(day_window.vm)\n",
    "        print((dim_time.fechaHora>=day.floor('1D'))*(dim_time.fechaHora<(day+timedelta(minutes=1)).ceil('1D')))\n",
    "        dim_time.loc[(dim_time.fechaHora>=day.floor('1D'))&(dim_time.fechaHora<(day+timedelta(minutes=1)).ceil('1D')),\"concordance_df_nova_time\"]=1\n",
    "\n",
    "        #day_window.v.corr(day_window.vm)\n",
    "    \n",
    "    return dim_time\n",
    "\n",
    "\n",
    "\n",
    "dim_time=eval_dq([100,CC,SS,Distances,start_time,end_time])\n",
    "dim_time[[\"codigoSerial\",\"fechaHora\",\"concordance_df_siata\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "print(z_crit) # 1.959963984540054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.DataFrame([1,2,np.nan,4,np.nan])\n",
    "n=np.count_nonzero(~np.isnan(df))\n",
    "df.std().values[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f295ec510066dddcc717ae6d4935b3ad0a45b533511561df7ff2b4bcd803903f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
