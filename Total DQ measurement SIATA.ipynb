{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to install if not installed yet\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install sklearn\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install haversine\n",
    "!pip install -U wxPython \n",
    "!pip install google\n",
    "!pip install google-api-core\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install google-cloud\n",
    "!pip install google-cloud-vision\n",
    "!pip install google.cloud.bigquery\n",
    "!pip install google.cloud.storage\n",
    "!pip install google-auth-oauthlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import seaborn as sn\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import haversine as hs\n",
    "\n",
    "import wx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citizen Scientist:  [1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 259, 261, 262, 265, 266, 267]\n",
      "Siata Stations:  [11, 12, 25, 28, 31, 37, 38, 44, 46, 48, 69, 6, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 94]\n"
     ]
    }
   ],
   "source": [
    "#Read Data from February\r\n",
    "header_CC=[\"codigoSerial\", \"fecha\", \"hora\", \"fechaHora\", \"temperatura\", \"humedad_relativa\", \"pm1_df\", \"pm10_df\", \"pm25_df\", \"pm1_nova\", \"pm10_nova\", \"pm25_nova\", \"calidad_temperatura\", \"calidad_humedad_relativa\", \"calidad_pm1_df\", \"calidad_pm10_df\", \"calidad_pm25_df\", \"calidad_pm1_nova\", \"calidad_pm10_nova\", \"calidad_pm25_nova\"]\r\n",
    "datatypes_CC={\"codigoSerial\":np.uint16, \"temperatura\":np.float16, \"humedad_relativa\":np.float16, \"pm1_df\":np.float32, \"pm10_df\":np.float32, \"pm25_df\":np.float32, \"pm1_nova\":np.float32, \"pm10_nova\":np.float32, \"pm25_nova\":np.float32}\r\n",
    "df_CC = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/SIATA_CS/SplitDatosCC/Samples/\"+\"February.csv\", header=None, names=header_CC, usecols=header_CC , dtype=datatypes_CC,parse_dates=[\"fecha\",\"hora\",\"fechaHora\"])\r\n",
    "\r\n",
    "#Data includes January, February and March\r\n",
    "header_SS=[\"Fecha_Hora\",\"codigoSerial\",\"pm25\",\"calidad_pm25\",\"pm10\",\"calidad_pm10\"]\r\n",
    "datatypes_SS={\"codigoSerial\":np.uint16,\"pm25\":np.float32,\"pm10\":np.float32}\r\n",
    "df_SS = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/SIATA Stations/PM/\"+\"SS_PM.csv\", header=None,names=header_SS, usecols=header_SS , dtype=datatypes_SS,parse_dates=[\"Fecha_Hora\"])\r\n",
    "\r\n",
    "\r\n",
    "grouped=df_CC.groupby(df_CC.codigoSerial)\r\n",
    "CC={}\r\n",
    "print(\"Citizen Scientist: \", sorted(list(df_CC.codigoSerial.unique())))\r\n",
    "for i in df_CC.codigoSerial.unique():\r\n",
    "    CC[i] = grouped.get_group(i).sort_values(by=['fechaHora'],ignore_index=True)\r\n",
    "del df_CC\r\n",
    "\r\n",
    "grouped=df_SS.groupby(df_SS.codigoSerial)\r\n",
    "SS={}\r\n",
    "print(\"Siata Stations: \", list(df_SS.codigoSerial.unique()))\r\n",
    "for j in df_SS.codigoSerial.unique():\r\n",
    "    SS[j] = grouped.get_group(j).sort_values(by=['Fecha_Hora'],ignore_index=True)\r\n",
    "del df_SS\r\n",
    "del grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DISTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitud</th>\n",
       "      <th>codigoSerial</th>\n",
       "      <th>alturaNivelPiso</th>\n",
       "      <th>municipio</th>\n",
       "      <th>alturaNivelMar</th>\n",
       "      <th>nombre</th>\n",
       "      <th>longitud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.267920</td>\n",
       "      <td>1</td>\n",
       "      <td>8.50</td>\n",
       "      <td>Medellin</td>\n",
       "      <td>1775</td>\n",
       "      <td>1</td>\n",
       "      <td>-75.545864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.255067</td>\n",
       "      <td>2</td>\n",
       "      <td>4.20</td>\n",
       "      <td>Medellin</td>\n",
       "      <td>1584</td>\n",
       "      <td>2</td>\n",
       "      <td>-75.619272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.330812</td>\n",
       "      <td>3</td>\n",
       "      <td>19.80</td>\n",
       "      <td>Bello</td>\n",
       "      <td>1525</td>\n",
       "      <td>3</td>\n",
       "      <td>-75.539352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.270411</td>\n",
       "      <td>4</td>\n",
       "      <td>9.00</td>\n",
       "      <td>Medellin</td>\n",
       "      <td>1549</td>\n",
       "      <td>4</td>\n",
       "      <td>-75.587045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.092923</td>\n",
       "      <td>5</td>\n",
       "      <td>21.75</td>\n",
       "      <td>Caldas</td>\n",
       "      <td>1725</td>\n",
       "      <td>5</td>\n",
       "      <td>-75.637325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>6.138660</td>\n",
       "      <td>271</td>\n",
       "      <td>1.92</td>\n",
       "      <td>La Estrella</td>\n",
       "      <td>1838</td>\n",
       "      <td>271</td>\n",
       "      <td>-75.648324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>6.259215</td>\n",
       "      <td>272</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Medellin</td>\n",
       "      <td>0</td>\n",
       "      <td>272</td>\n",
       "      <td>-75.588640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>6.259215</td>\n",
       "      <td>273</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Medellin</td>\n",
       "      <td>0</td>\n",
       "      <td>273</td>\n",
       "      <td>-75.588640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>6.233957</td>\n",
       "      <td>274</td>\n",
       "      <td>2.10</td>\n",
       "      <td>Medellin</td>\n",
       "      <td>1580</td>\n",
       "      <td>274</td>\n",
       "      <td>-75.627883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>6.169665</td>\n",
       "      <td>275</td>\n",
       "      <td>1.80</td>\n",
       "      <td>Envigado</td>\n",
       "      <td>1727</td>\n",
       "      <td>275</td>\n",
       "      <td>-75.569332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      latitud  codigoSerial  alturaNivelPiso    municipio  alturaNivelMar  \\\n",
       "0    6.267920             1             8.50     Medellin            1775   \n",
       "1    6.255067             2             4.20     Medellin            1584   \n",
       "2    6.330812             3            19.80        Bello            1525   \n",
       "3    6.270411             4             9.00     Medellin            1549   \n",
       "4    6.092923             5            21.75       Caldas            1725   \n",
       "..        ...           ...              ...          ...             ...   \n",
       "270  6.138660           271             1.92  La Estrella            1838   \n",
       "271  6.259215           272             0.00     Medellin               0   \n",
       "272  6.259215           273             0.00     Medellin               0   \n",
       "273  6.233957           274             2.10     Medellin            1580   \n",
       "274  6.169665           275             1.80     Envigado            1727   \n",
       "\n",
       "     nombre   longitud  \n",
       "0         1 -75.545864  \n",
       "1         2 -75.619272  \n",
       "2         3 -75.539352  \n",
       "3         4 -75.587045  \n",
       "4         5 -75.637325  \n",
       "..      ...        ...  \n",
       "270     271 -75.648324  \n",
       "271     272 -75.588640  \n",
       "272     273 -75.588640  \n",
       "273     274 -75.627883  \n",
       "274     275 -75.569332  \n",
       "\n",
       "[275 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatypesDistances={\"codigoSerial_CC\":np.uint16,\"codigoSerial_ES\":np.uint16,\"Distancia_a_ES\":np.float16,\"codigoSerial_ES2\":np.uint16}\n",
    "Distances = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/Distances and positions/Distancias_2.csv\", header=0, dtype=datatypesDistances,index_col=\"codigoSerial_CC\")\n",
    "\n",
    "\n",
    "\n",
    "datatypesCC={\"codigoSerial\":np.uint16,\"latitud\":np.float64,\"longitud\":np.float64,\"alturaNivelPiso\":np.uint32,\"alturaNivelMar\":np.uint32,\"nombre\":np.uint32,\"municipio\":str}\n",
    "Nubes_CiudadanosCientificos = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/Distances and positions/Nubes_CiudadanosCientificos.csv\", header=0)\n",
    "Nubes_CiudadanosCientificos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQ CALCULATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inicio=\"2020-02-01 00:00:00\"\r\n",
    "fin=   \"2020-02-29 23:59:00\"\r\n",
    "nube=0\r\n",
    "contador=0\r\n",
    "\r\n",
    "acc_vs_dis=[]\r\n",
    "missing_data_df=[]\r\n",
    "missing_data_nova=[]\r\n",
    "\r\n",
    "df_accur = pd.DataFrame(columns =['codigoSerial', 'dist', 'acc_nova', 'acc_df'])\r\n",
    "df_comple = pd.DataFrame(columns =[\"codigoSerial\",\"completeness_df\",\"completeness_nova\",\"completeness_group_df\",\"completeness_group_nova\"])\r\n",
    "df_preci = pd.DataFrame(columns =[\"codigoSerial\",\"precision_df\",\"precision_nova\",\"precision_group_df\",\"precision_group_nova\"])\r\n",
    "df_uncer = pd.DataFrame(columns =[\"codigoSerial\",\"uncertainty\",\"uncertainty_group\"])\r\n",
    "df_conco = pd.DataFrame(columns =[\"codigoSerial\",\"concordance_df_nova\",\"concordance_df_siata\",\"concordance_df_hum\",\"concordance_df_temp\",\"concordance_nova_siata\",\"concordance_nova_hum\",\"concordance_nova_temp\"])\r\n",
    "\r\n",
    "for nube in CC.keys():\r\n",
    "    contador+=1\r\n",
    "    CC[nube][\"v_pm25\"] = np.nan\r\n",
    "    CC[nube][\"alpha_df\"] = np.nan\r\n",
    "    CC[nube][\"alpha_nova\"] = np.nan\r\n",
    "    #del df_window\r\n",
    "    df_window=CC[nube][(CC[nube]['fechaHora'] >= inicio) & (CC[nube]['fechaHora'] <= fin)]\r\n",
    "    \r\n",
    "    #Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999]\r\n",
    "\r\n",
    "    df_window=df_window.copy()\r\n",
    "    df_window.loc[df_window[\"pm25_nova\"]>999,\"pm25_nova\"]=np.nan\r\n",
    "    df_window.loc[df_window[\"pm25_nova\"]<0,\"pm25_nova\"]=np.nan\r\n",
    "    df_window.loc[df_window[\"pm25_df\"]>999,\"pm25_df\"]=np.nan\r\n",
    "    df_window.loc[df_window[\"pm25_df\"]<0,\"pm25_df\"]=np.nan\r\n",
    "    \r\n",
    "    #Remove data above the whiskers of the boxplot\r\n",
    "    Q1 = df_window['pm25_df'].quantile(0.25)\r\n",
    "    Q3 = df_window['pm25_df'].quantile(0.75)\r\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \r\n",
    "    df_window.loc[df_window[\"pm25_df\"]>=Q3 + 1.5 *IQR,\"pm25_df\"]=np.nan\r\n",
    "    \r\n",
    "    Q1 = df_window['pm25_nova'].quantile(0.25)\r\n",
    "    Q3 = df_window['pm25_nova'].quantile(0.75)\r\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \r\n",
    "    df_window.loc[df_window[\"pm25_nova\"]>=Q3 + 1.5 *IQR,\"pm25_nova\"]=np.nan\r\n",
    "    \r\n",
    "    ref_date_range = pd.date_range(inicio, fin, freq='1Min')\r\n",
    "    ref_date_range = pd.DataFrame(ref_date_range,columns=[\"ref_fechaHora\"])\r\n",
    "    \r\n",
    "    #Hourly mean\r\n",
    "    df_window['pm25_nova_ave']=np.nan\r\n",
    "    df_window['pm25_df_ave']=np.nan\r\n",
    "    #Hourly standar deviation\r\n",
    "    df_window['pm25_nova_std']=np.nan\r\n",
    "    df_window['pm25_df_std']=np.nan\r\n",
    "    #Hourly uncertainty\r\n",
    "    df_window['pm25_unc']=np.nan\r\n",
    "    \r\n",
    "    for ts in df_window['fechaHora']:\r\n",
    "        if ts==ts.ceil('60min'):\r\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]\r\n",
    "            \r\n",
    "        else:\r\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]\r\n",
    "        #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\r\n",
    "        #print(window['pm25_nova'])\r\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_ave']=window['pm25_nova'].mean()\r\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_ave']=window['pm25_df'].mean()\r\n",
    "        \r\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_std']=100*(window['pm25_nova'].std()/window['pm25_nova'].mean())\r\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_std']=100*(window['pm25_df'].std()/window['pm25_df'].mean())\r\n",
    "        \r\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_unc']=\\\r\n",
    "        100*np.sqrt((window.pm25_df-window.pm25_nova).pow(2).mean()/2)/((window.pm25_df+window.pm25_nova).mean()/2)\r\n",
    "        \r\n",
    "    del window\r\n",
    "\r\n",
    "    prec_df=df_window.pm25_df_std.mean()\r\n",
    "    prec_nova=df_window.pm25_nova_std.mean()\r\n",
    "    uncer_df=df_window.pm25_unc.mean()\r\n",
    "    \r\n",
    "        \r\n",
    "    df_preci=df_preci.append({\"codigoSerial\":nube,\"precision_df\":prec_df,\"precision_nova\":prec_nova}, ignore_index = True)\r\n",
    "    print(\"%d. Nube: %d, Overall relative (Precision) Standard Deviation.\"%(contador,nube))\r\n",
    "    \r\n",
    "    df_uncer=df_uncer.append({\"codigoSerial\":nube,\"uncertainty\":uncer_df}, ignore_index = True)\r\n",
    "    print(\"%d. Nube: %d, Overall relative Uncertainty BS, \"%(contador,nube))\r\n",
    "    \r\n",
    "    Closest_Station=Distances.codigoSerial_ES.loc[nube]    \r\n",
    "    if Closest_Station in SS.keys():\r\n",
    "        #Clean values out of range\r\n",
    "        SS[Closest_Station].loc[SS[Closest_Station][\"pm25\"]<=0,\"pm25\"]=np.nan\r\n",
    "        for time in df_window.fechaHora:\r\n",
    "            \r\n",
    "            idx=SS[Closest_Station].Fecha_Hora.searchsorted(time,side=\"right\")\r\n",
    "            #print(idx, SS[Closest_Station].Fecha_Hora.loc[idx], time)\r\n",
    "            v=SS[Closest_Station].loc[idx,\"pm25\"]\r\n",
    "            df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\r\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_df_ave\"]\r\n",
    "            #print(time,\" : \",vm.values[0],\"________\",SS[Closest_Station].Fecha_Hora.loc[idx],\" : \",v)\r\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_df\"]=100*abs(vm-v)/v\r\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_nova_ave\"]\r\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_nova\"]=100*abs(vm-v)/v\r\n",
    "        \r\n",
    "        df_accur=df_accur.append({'codigoSerial':nube, 'dist':Distances.loc[nube,\"Distancia_a_ES\"], 'acc_df':df_window.alpha_df.mean(), 'acc_nova':df_window.alpha_nova.mean()}, ignore_index = True)\r\n",
    "        print(\"%d. Nube: %d, Accuracy,\" %(contador, nube))\r\n",
    "    \r\n",
    "    ref_date_range = pd.date_range(inicio, fin, freq='1Min')\r\n",
    "    ref_date_range = pd.DataFrame(ref_date_range,columns=[\"ref_fechaHora\"])\r\n",
    "\r\n",
    "    \r\n",
    "    #Check for any missing date\r\n",
    "    missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(df_window.fechaHora),\"ref_fechaHora\"]\r\n",
    "\r\n",
    "    #Add missing date rows\r\n",
    "    for missing in missing_dates:\r\n",
    "        df_window=df_window.append({\"codigoSerial\":nube,\"fechaHora\":missing}, ignore_index = True)\r\n",
    "    \r\n",
    "    #Check for any missing date\r\n",
    "    missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(df_window.fechaHora),\"ref_fechaHora\"]\r\n",
    "    \r\n",
    "    #Check for missing data\r\n",
    "    missing_data_df=np.count_nonzero(np.isnan(df_window['pm25_df']))\r\n",
    "    missing_data_nova=np.count_nonzero(np.isnan(df_window['pm25_nova']))\r\n",
    "    comp_df=100*(1-missing_data_df/np.size(df_window.pm25_df))\r\n",
    "    comp_nova=100*(1-missing_data_nova/np.size(df_window.pm25_nova))\r\n",
    "    \r\n",
    "    \r\n",
    "    if comp_df<75:\r\n",
    "        group_df=0\r\n",
    "    elif 75<=comp_df:\r\n",
    "        group_df=1\r\n",
    "    \r\n",
    "    if comp_nova<75:\r\n",
    "        group_nova=0\r\n",
    "    elif  75<=comp_nova:\r\n",
    "        group_nova=1     \r\n",
    "        \r\n",
    "    df_comple=df_comple.append({\"codigoSerial\":nube,\"completeness_df\":comp_df,\"completeness_nova\":comp_nova,\"completeness_group_df\":group_df,\"completeness_group_nova\":group_nova}, ignore_index = True)\r\n",
    "    \r\n",
    "    print(\"%d. Nube: %d, Completeness.\" %(contador,nube))\r\n",
    "    \r\n",
    "    corr_df = df_window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[0].abs()\r\n",
    "    corr_nova = df_window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[1].abs()\r\n",
    "  \r\n",
    "    \r\n",
    "        \r\n",
    "    df_conco=df_conco.append({\"codigoSerial\":nube,\"concordance_df_nova\":corr_df.pm25_nova,\r\n",
    "                              \"concordance_df_siata\":corr_df.v_pm25,\"concordance_df_hum\":corr_df.humedad_relativa,\"concordance_df_temp\":corr_df.temperatura,\r\n",
    "                              \"concordance_nova_siata\":corr_nova.v_pm25,\"concordance_nova_hum\":corr_nova.humedad_relativa,\"concordance_nova_temp\":corr_nova.temperatura}, \r\n",
    "                             ignore_index = True)\r\n",
    "    \r\n",
    "    print(\"%d. Nube: %d, Overall concordance, \"%(contador,nube))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_conco.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_conco.csv\",index=False)\n",
    "df_comple.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_comple.csv\",index=False)\n",
    "df_accur.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_accur.csv\",index=False)\n",
    "df_uncer.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_uncer.csv\",index=False)\n",
    "df_preci.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_preci.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPLETENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_comple = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_comple.csv\", header=0)\n",
    "print(df_comple.head(2))\n",
    "\n",
    "dic_df={}\n",
    "dic_nova={}\n",
    "grouped_df=df_comple.groupby(df_comple.completeness_group_df)\n",
    "grouped_nova=df_comple.groupby(df_comple.completeness_group_nova)\n",
    "for i in df_comple.completeness_group_df.unique():\n",
    "    dic_df[i]=grouped_df.get_group(i).sort_values(by=['codigoSerial'],ignore_index=True).codigoSerial.tolist()\n",
    "\n",
    "for i in df_comple.completeness_group_nova.unique():\n",
    "    dic_nova[i]=grouped_nova.get_group(i).sort_values(by=['codigoSerial'],ignore_index=True).codigoSerial.tolist()\n",
    "\n",
    "print(\"Completeness < 75%: Group 0, Completeness >= 75%: Group 1\")\n",
    "print(\"DF sensors\",dic_df)\n",
    "print(\"Nova sensors\",dic_nova)\n",
    "print(len(dic_df[1]),len(dic_nova[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "#plt.subplot(121)\n",
    "plt.title(\"Nodes completeness histogram\")\n",
    "plt.xlabel(\"Completeness (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "plt.ylim([0, 50])\n",
    "aux1=df_comple.completeness_df\n",
    "aux2=df_comple.completeness_nova\n",
    "w = [100*np.ones_like(df_comple.index) / len(df_comple.index),100*np.ones_like(df_comple.index) / len(df_comple.index)]\n",
    "plt.hist([aux1, aux2], bins=12, label=['completeness_df', 'completeness_nova'],weights=w,zorder=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Completeness_2.eps', format='eps',bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "#plt.subplot(122)\n",
    "#plt.title(\"nova sensors completeness histogram\")\n",
    "#plt.xlabel(\"Completeness (%)\")\n",
    "#plt.ylabel(\"% of sensors\")\n",
    "#plt.ylim([0, 45])\n",
    "#df_comple.completeness_nova.hist(bins=16,weights = 100*np.ones_like(df_comple.index) / len(df_comple.index),zorder=3)\n",
    "#plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Completeness.eps', format='eps',bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_uncer = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_uncer.csv\", header=0)\n",
    "print(df_uncer.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df_uncer.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_uncer.csv\",index=False)\n",
    "df_uncer = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_uncer.csv\", header=0)\n",
    "print(df_uncer.head(2))\n",
    "print(len(dic_df[1]))\n",
    "for nodes in dic_df[0]:\n",
    "    df_uncer.loc[df_uncer.codigoSerial==nodes,\"uncertainty\"]=np.nan\n",
    "print(np.count_nonzero(~np.isnan(df_uncer.uncertainty)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNCERTAINTY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.title(\"Nodes uncertainty histogram\")\n",
    "plt.xlabel(\"Uncertainty (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "aux=df_uncer.uncertainty.loc[~np.isnan(df_uncer.uncertainty)]\n",
    "df_uncer.uncertainty.hist(bins=16,weights = 100*np.ones_like(aux.index) / len( aux.index),zorder=3)\n",
    "\n",
    "print(np.count_nonzero(~np.isnan(df_uncer.uncertainty)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df_uncer.uncertainty.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Nodes uncertainty histogram\")\n",
    "plt.xlabel(\"Uncertainty (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "aux=df_uncer.uncertainty.loc[~np.isnan(df_uncer.uncertainty)]\n",
    "df_uncer.uncertainty.hist(bins=16,weights = 100*np.ones_like(aux.index) / len( aux.index),zorder=3)\n",
    "\n",
    "print(np.count_nonzero(~np.isnan(df_uncer.uncertainty)))\n",
    "\n",
    "nube=85\n",
    "\n",
    "inicio=\"2020-02-01 00:00:00\"\n",
    "fin=   \"2020-02-01 23:59:00\"\n",
    "CC[nube][\"v_pm25\"] = np.nan\n",
    "CC[nube][\"alpha_df\"] = np.nan\n",
    "CC[nube][\"alpha_nova\"] = np.nan\n",
    "df_window=CC[nube].loc[(CC[nube]['fechaHora'] >= inicio) & (CC[nube]['fechaHora'] <= fin)]\n",
    "#print(df_window)\n",
    "\n",
    "#Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999]\n",
    "df_window=df_window.copy()\n",
    "df_window.loc[df_window[\"pm25_nova\"]>999,\"pm25_nova\"]=np.nan\n",
    "df_window.loc[df_window[\"pm25_nova\"]<0,\"pm25_nova\"]=np.nan\n",
    "df_window.loc[df_window[\"pm25_df\"]>999,\"pm25_df\"]=np.nan\n",
    "df_window.loc[df_window[\"pm25_df\"]<0,\"pm25_df\"]=np.nan\n",
    "\n",
    "#Remove data above the whiskers of the boxplot\n",
    "Q1 = df_window['pm25_df'].quantile(0.25)\n",
    "Q3 = df_window['pm25_df'].quantile(0.75)\n",
    "IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "df_window.loc[df_window[\"pm25_df\"]>=Q3 + 1.5 *IQR,\"pm25_df\"]=np.nan\n",
    "\n",
    "Q1 = df_window['pm25_nova'].quantile(0.25)\n",
    "Q3 = df_window['pm25_nova'].quantile(0.75)\n",
    "IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "df_window.loc[df_window[\"pm25_nova\"]>=Q3 + 1.5 *IQR,\"pm25_nova\"]=np.nan\n",
    "\n",
    "#Moving average filter\n",
    "#df_window['pm25_nova_lpf'] = df_window.pm25_nova.rolling(window=60,min_periods=1).mean()\n",
    "#df_window['pm25_df_lpf'] = df_window.pm25_df.rolling(window=60,min_periods=1).mean()\n",
    "\n",
    "#Hourly mean\n",
    "df_window['pm25_nova_ave']=np.nan\n",
    "df_window['pm25_df_ave']=np.nan\n",
    "\n",
    "#print(\"Diferentes de nan df: \",np.count_nonzero(~np.isnan(df_window['pm25_df'])))\n",
    "#print(\"Diferentes de nan nova: \",np.count_nonzero(~np.isnan(df_window['pm25_nova'])))\n",
    "\n",
    "for ts in df_window['fechaHora']:\n",
    "    if ts==ts.ceil('60min'):\n",
    "        window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]\n",
    "        \n",
    "    else:\n",
    "        window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]\n",
    "    #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\n",
    "    #print(window['pm25_nova'])\n",
    "    \n",
    "    df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_ave']=window['pm25_nova'].mean()\n",
    "    df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_ave']=window['pm25_df'].mean()\n",
    "    #df_window['pm25_nova_ave'][df_window[\"fechaHora\"]==ts]=window['pm25_nova'].mean()\n",
    "    #df_window['pm25_df_ave'][df_window[\"fechaHora\"]==ts]=window['pm25_df'].mean()\n",
    "    \n",
    "    \n",
    "#del window\n",
    "\n",
    "Closest_Station=Distances.codigoSerial_ES.loc[nube]\n",
    "SS[Closest_Station].loc[SS[Closest_Station][\"pm25\"]<=0,\"pm25\"]=np.nan\n",
    "print(\"Nube: %d, Estación: %d, Distancia: %s km\" %(nube, Closest_Station, Distances.Distancia_a_ES.loc[nube]))\n",
    "\n",
    "for time in df_window.fechaHora:\n",
    "    idx=SS[Closest_Station].Fecha_Hora.searchsorted(time,side=\"right\")\n",
    "    #print(idx, SS[Closest_Station].Fecha_Hora.loc[idx], time)\n",
    "    #v=SS[Closest_Station].pm25.loc[idx]\n",
    "    v=SS[Closest_Station].loc[idx,\"pm25\"]\n",
    "    df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\n",
    "    #df_window.v_pm25[(df_window.fechaHora == time)]=v\n",
    "    vm=df_window.loc[(df_window.fechaHora == time),\"pm25_df_ave\"]\n",
    "    #print(time,\" : \",vm.values[0],\"________\",SS[Closest_Station].Fecha_Hora.loc[idx],\" : \",v)\n",
    "    df_window.loc[df_window.fechaHora == time,\"alpha_df\"]=100*abs(vm-v)/v\n",
    "    #df_window.alpha_df[(df_window.fechaHora == time)]=100*abs(vm-v)/v\n",
    "    vm=df_window.loc[(df_window.fechaHora == time),\"pm25_nova_ave\"]\n",
    "    df_window.loc[df_window.fechaHora == time,\"alpha_nova\"]=100*abs(vm-v)/v\n",
    "    #df_window.alpha_nova[(df_window.fechaHora == time)]=100*abs(vm-v)/v\n",
    "\n",
    "print(\"Average error for DF sensor: %s %% and NOVA sensor: %s %%\"%(df_window.alpha_df.mean(),df_window.alpha_nova.mean()))   \n",
    "\n",
    "#Measurement\n",
    "plt.subplot(132)\n",
    "plt.title(\"Concentration of PM2.5 over the time\")\n",
    "plt.ylabel(\"PM2.5 Concentration ($μg/m^3$)\")\n",
    "plt.xlabel(\"Timestamp\")\n",
    "df_window[\"Time\"] = [str(datetime.time(d)) for d in df_window['hora']]\n",
    "plt.xticks(np.arange(0, len(df_window[\"Time\"])+120, 120), rotation=90)\n",
    "plt.plot(df_window[\"Time\"],df_window[\"pm25_df_ave\"],\"r-\",label=\"DF Sensor: %d\"%(nube))\n",
    "plt.plot(df_window[\"Time\"],df_window[\"pm25_nova_ave\"],\"g-\",label=\"Nova Sensor: %d\"%(nube))\n",
    "#plt.plot(df_window[\"fechaHora\"],df_window[\"v_pm25\"],\"b-\",label=\"Siata Station: %d\"%(Closest_Station))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "nube=81\n",
    "\n",
    "inicio=\"2020-02-01 00:00:00\"\n",
    "fin=   \"2020-02-01 23:59:00\"\n",
    "CC[nube][\"v_pm25\"] = np.nan\n",
    "CC[nube][\"alpha_df\"] = np.nan\n",
    "CC[nube][\"alpha_nova\"] = np.nan\n",
    "df_window=CC[nube].loc[(CC[nube]['fechaHora'] >= inicio) & (CC[nube]['fechaHora'] <= fin)]\n",
    "#print(df_window)\n",
    "\n",
    "#Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999]\n",
    "df_window=df_window.copy()\n",
    "df_window.loc[df_window[\"pm25_nova\"]>999,\"pm25_nova\"]=np.nan\n",
    "df_window.loc[df_window[\"pm25_nova\"]<0,\"pm25_nova\"]=np.nan\n",
    "df_window.loc[df_window[\"pm25_df\"]>999,\"pm25_df\"]=np.nan\n",
    "df_window.loc[df_window[\"pm25_df\"]<0,\"pm25_df\"]=np.nan\n",
    "\n",
    "#Remove data above the whiskers of the boxplot\n",
    "Q1 = df_window['pm25_df'].quantile(0.25)\n",
    "Q3 = df_window['pm25_df'].quantile(0.75)\n",
    "IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "df_window.loc[df_window[\"pm25_df\"]>=Q3 + 1.5 *IQR,\"pm25_df\"]=np.nan\n",
    "\n",
    "Q1 = df_window['pm25_nova'].quantile(0.25)\n",
    "Q3 = df_window['pm25_nova'].quantile(0.75)\n",
    "IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "df_window.loc[df_window[\"pm25_nova\"]>=Q3 + 1.5 *IQR,\"pm25_nova\"]=np.nan\n",
    "\n",
    "#Moving average filter\n",
    "#df_window['pm25_nova_lpf'] = df_window.pm25_nova.rolling(window=60,min_periods=1).mean()\n",
    "#df_window['pm25_df_lpf'] = df_window.pm25_df.rolling(window=60,min_periods=1).mean()\n",
    "\n",
    "#Hourly mean\n",
    "df_window['pm25_nova_ave']=np.nan\n",
    "df_window['pm25_df_ave']=np.nan\n",
    "\n",
    "#print(\"Diferentes de nan df: \",np.count_nonzero(~np.isnan(df_window['pm25_df'])))\n",
    "#print(\"Diferentes de nan nova: \",np.count_nonzero(~np.isnan(df_window['pm25_nova'])))\n",
    "\n",
    "for ts in df_window['fechaHora']:\n",
    "    if ts==ts.ceil('60min'):\n",
    "        window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]\n",
    "        \n",
    "    else:\n",
    "        window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]\n",
    "    #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\n",
    "    #print(window['pm25_nova'])\n",
    "    \n",
    "    df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_ave']=window['pm25_nova'].mean()\n",
    "    df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_ave']=window['pm25_df'].mean()\n",
    "    #df_window['pm25_nova_ave'][df_window[\"fechaHora\"]==ts]=window['pm25_nova'].mean()\n",
    "    #df_window['pm25_df_ave'][df_window[\"fechaHora\"]==ts]=window['pm25_df'].mean()\n",
    "    \n",
    "    \n",
    "del window\n",
    "\n",
    "Closest_Station=Distances.codigoSerial_ES.loc[nube]\n",
    "SS[Closest_Station].loc[SS[Closest_Station][\"pm25\"]<=0,\"pm25\"]=np.nan\n",
    "print(\"Nube: %d, Estación: %d, Distancia: %s km\" %(nube, Closest_Station, Distances.Distancia_a_ES.loc[nube]))\n",
    "\n",
    "for time in df_window.fechaHora:\n",
    "    idx=SS[Closest_Station].Fecha_Hora.searchsorted(time,side=\"right\")\n",
    "    #print(idx, SS[Closest_Station].Fecha_Hora.loc[idx], time)\n",
    "    #v=SS[Closest_Station].pm25.loc[idx]\n",
    "    v=SS[Closest_Station].loc[idx,\"pm25\"]\n",
    "    df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\n",
    "    #df_window.v_pm25[(df_window.fechaHora == time)]=v\n",
    "    vm=df_window.loc[(df_window.fechaHora == time),\"pm25_df_ave\"]\n",
    "    #print(time,\" : \",vm.values[0],\"________\",SS[Closest_Station].Fecha_Hora.loc[idx],\" : \",v)\n",
    "    df_window.loc[df_window.fechaHora == time,\"alpha_df\"]=100*abs(vm-v)/v\n",
    "    #df_window.alpha_df[(df_window.fechaHora == time)]=100*abs(vm-v)/v\n",
    "    vm=df_window.loc[(df_window.fechaHora == time),\"pm25_nova_ave\"]\n",
    "    df_window.loc[df_window.fechaHora == time,\"alpha_nova\"]=100*abs(vm-v)/v\n",
    "    #df_window.alpha_nova[(df_window.fechaHora == time)]=100*abs(vm-v)/v\n",
    "\n",
    "print(\"Average error for DF sensor: %s %% and NOVA sensor: %s %%\"%(df_window.alpha_df.mean(),df_window.alpha_nova.mean()))   \n",
    "\n",
    "#Measurement\n",
    "plt.subplot(133)\n",
    "plt.title(\"Concentration of PM2.5 over the time\")\n",
    "plt.ylabel(\"PM2.5 Concentration ($μg/m^3$)\")\n",
    "plt.xlabel(\"Timestamp\")\n",
    "df_window[\"Time\"] = [str(datetime.time(d)) for d in df_window['hora']]\n",
    "plt.xticks(np.arange(0, len(df_window[\"Time\"])+120, 120), rotation=90)\n",
    "plt.plot(df_window[\"Time\"],df_window[\"pm25_df_ave\"],\"r-\",label=\"DF Sensor: %d\"%(nube))\n",
    "plt.plot(df_window[\"Time\"],df_window[\"pm25_nova_ave\"],\"g-\",label=\"Nova Sensor: %d\"%(nube))\n",
    "#plt.plot(df_window[\"fechaHora\"],df_window[\"v_pm25\"],\"b-\",label=\"Siata Station: %d\"%(Closest_Station))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#plt.gcf().subplots_adjust(bottom=0.8)\n",
    "plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Uncertainty.eps', format='eps',bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRECISION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df_preci.head(2))\n",
    "print(\"Number of DF sensors with completeness >75%\",len(dic_df[1]))\n",
    "print(\"Number of Nova sensors with completeness >75%\",len(dic_nova[1]))\n",
    "for nodes in dic_df[0]:\n",
    "    df_preci.loc[df_preci.codigoSerial==nodes,\"precision_df\"]=np.nan\n",
    "for nodes in dic_nova[0]:\n",
    "    df_preci.loc[df_preci.codigoSerial==nodes,\"precision_nova\"]=np.nan\n",
    "print(\"Number of DF sensors with completeness >75%\",np.count_nonzero(~np.isnan(df_preci.precision_df)))\n",
    "print(\"Number of DF sensors with completeness >75%\",np.count_nonzero(~np.isnan(df_preci.precision_nova)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "#plt.subplot(121)\n",
    "plt.title(\"Nodes precision histogram\")\n",
    "plt.xlabel(\"precision (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "plt.ylim([0, 40])\n",
    "df_preci_comp=100-df_preci.precision_df\n",
    "aux1=df_preci_comp.loc[~np.isnan(df_preci_comp)]\n",
    "df_preci_comp=100-df_preci.precision_nova\n",
    "aux2=df_preci_comp.loc[~np.isnan(df_preci_comp)]\n",
    "w = [100*np.ones_like(aux1.index) / len(aux1.index),100*np.ones_like(aux2.index) / len(aux2.index)]\n",
    "#df_preci_comp.hist(bins=16,weights = 100*np.ones_like(aux.index) / len( aux.index),zorder=3)\n",
    "plt.hist([aux1, aux2], bins=12, label=['precision_df', 'precision_acc_nova'],weights=w,zorder=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Precision_2.eps', format='eps',bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(np.count_nonzero(~np.isnan(df_preci_comp)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plt.subplot(122)\n",
    "#plt.title(\"nova sensors precision histogram\")\n",
    "#plt.xlabel(\"precision (%)\")\n",
    "#plt.ylabel(\"% of sensors\")\n",
    "#plt.ylim([0, 35])\n",
    "#df_preci_comp=100-df_preci.precision_nova\n",
    "#aux=df_preci_comp.loc[~np.isnan(df_preci_comp)]\n",
    "#df_preci_comp.hist(bins=16,weights = 100*np.ones_like(aux.index) / len( aux.index),zorder=3)\n",
    "#plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Precision.eps', format='eps',bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(100-df_preci.precision_nova).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df_accur.head(2))\n",
    "print(\"Number of DF sensors with completeness >75%\",len(dic_df[1]))\n",
    "print(\"Number of Nova sensors with completeness >75%\",len(dic_nova[1]))\n",
    "for nodes in dic_df[0]:\n",
    "    df_accur.loc[df_accur.codigoSerial==nodes,\"acc_df\"]=np.nan\n",
    "for nodes in dic_nova[0]:\n",
    "    df_accur.loc[df_accur.codigoSerial==nodes,\"acc_nova\"]=np.nan\n",
    "print(\"Number of DF sensors with completeness >75%\",np.count_nonzero(~np.isnan(df_accur.acc_df)))\n",
    "print(\"Number of Nova sensors with completeness >75%\",np.count_nonzero(~np.isnan(df_accur.acc_nova)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Distance to nearest station histogram\")\n",
    "plt.xlabel(\"Distance (km)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "plt.ylim([0, 35])\n",
    "aux=df_accur.loc[~np.isnan(df_accur['acc_nova'])| ~np.isnan(df_accur['acc_df']),\"dist\"]\n",
    "aux.hist(bins=11,weights=100*np.ones_like(aux.index) / len(aux.index),zorder=3)\n",
    "ndatos_dist=np.size(df_accur.loc[(~np.isnan(df_accur['acc_nova'])| ~np.isnan(df_accur['acc_df'])),\"dist\"])\n",
    "\n",
    "print(\"Number of sensors\",ndatos_dist)\n",
    "ndatos_dist=np.size(df_accur.loc[(~np.isnan(df_accur['acc_nova'])),\"dist\"])\n",
    "print(\"Number of sensors\",ndatos_dist)\n",
    "ndatos_dist=np.size(df_accur.loc[( ~np.isnan(df_accur['acc_df'])),\"dist\"])\n",
    "print(\"Number of sensors\",ndatos_dist)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Node Accuracy histogram\")\n",
    "plt.xlabel(\"Accuracy (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "plt.ylim([0, 35])\n",
    "#plt.xlim([0, 1300])\n",
    "\n",
    "aux1=df_accur.acc_df.loc[~np.isnan(df_accur.acc_df)]\n",
    "aux2=df_accur.acc_nova.loc[~np.isnan(df_accur.acc_nova)]\n",
    "w = [100*np.ones_like(aux1.index) / len(aux1.index),100*np.ones_like(aux2.index) / len(aux2.index)]\n",
    "plt.hist([aux1, aux2], bins=11, label=['acc_df', 'acc_nova'],weights=w,zorder=3)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Accuracy_2.eps', format='eps',bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "\n",
    "#plt.subplot(132)\n",
    "#plt.title(\"df sensors Accuracy histogram\")\n",
    "#plt.xlabel(\"Accuracy (%)\")\n",
    "#plt.ylabel(\"% of sensors\")\n",
    "#plt.ylim([0, 35])\n",
    "##plt.xlim([0, 1300])\n",
    "#aux=df_accur.acc_df.loc[~np.isnan(df_accur.acc_df)]\n",
    "#print(np.size(aux))\n",
    "#df_accur.acc_df.hist(bins=11,weights = 100*np.ones_like(aux.index) / len(aux.index),zorder=3)\n",
    "#\n",
    "#plt.subplot(133)\n",
    "#plt.title(\"nova sensors Accuracy histogram\")\n",
    "#plt.xlabel(\"Accuracy (%)\")\n",
    "#plt.ylabel(\"% of sensors\")\n",
    "#plt.ylim([0, 35])\n",
    "##plt.xlim([0, 1300])\n",
    "#aux=df_accur.acc_nova.loc[~np.isnan(df_accur.acc_nova)]\n",
    "#print(np.size(aux))\n",
    "#df_accur.acc_nova.hist(bins=11,weights = 100*np.ones_like(aux.index) / len(aux.index),zorder=3)\n",
    "#plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Accuracy.eps', format='eps',bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_accur.head(60)\n",
    "df_accur.loc[df_accur.acc_nova>400,\"acc_nova\"]=np.nan\n",
    "df_accur.acc_nova.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.title(\"Accuracy(Relative error) vs Distance\")\n",
    "plt.xlabel(\"Distance (km)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "#df_accur[\"acc_nova_2\"]=df_accur[\"acc_nova\"]\n",
    "#df_accur[\"acc_df_2\"]=df_accur[\"acc_df\"]\n",
    "#df_accur\n",
    "\n",
    "df_accur.loc[df_accur[\"acc_nova\"]==np.inf,\"acc_nova\"]=np.nan\n",
    "df_accur.loc[df_accur[\"acc_df\"]==np.inf,\"acc_df\"]=np.nan\n",
    "\n",
    "plt.plot(df_accur[\"dist\"],df_accur[\"acc_df\"],'ro',label=\"DF Sensor\")\n",
    "\n",
    "for i in df_accur.index:\n",
    "    if not np.isnan(df_accur[\"acc_df\"].loc[i]):\n",
    "        plt.text(df_accur[\"dist\"].loc[i],df_accur[\"acc_df\"].loc[i],i)\n",
    "        \n",
    "plt.plot(df_accur[\"dist\"],df_accur[\"acc_nova\"],'gx',label=\"Nova Sensor\")\n",
    "\n",
    "for i in df_accur.index:\n",
    "    if not np.isnan(df_accur[\"acc_nova\"].loc[i]):\n",
    "        plt.text(df_accur[\"dist\"].loc[i],df_accur[\"acc_nova\"].loc[i],i)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CORRELATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df_conco.head(2))\n",
    "print(\"Number of DF sensors with completeness >75%\",len(dic_df[1]))\n",
    "print(\"Number of Nova sensors with completeness >75%\",len(dic_nova[1]))\n",
    "for nodes in dic_df[0]:\n",
    "    df_conco.loc[df_conco.codigoSerial==nodes,[\"concordance_df_nova\",\"concordance_df_siata\",\"concordance_df_hum\",\"concordance_df_temp\"]]=np.nan\n",
    "    \n",
    "for nodes in dic_nova[0]:\n",
    "    df_conco.loc[df_conco.codigoSerial==nodes,[\"concordance_df_nova\",\"concordance_nova_siata\",\"concordance_nova_hum\",\"concordance_nova_temp\"]]=np.nan\n",
    "print(\"Number of DF sensors with completeness >75%\",np.count_nonzero(~np.isnan(df_conco.concordance_df_siata)))\n",
    "print(\"Number of Nova sensors with completeness >75%\",np.count_nonzero(~np.isnan(df_conco.concordance_nova_siata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.title(\"DF sensor vs Nova sensor\")\n",
    "plt.xlabel(\"Comparability (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "aux=100*df_conco.concordance_df_nova.loc[~np.isnan(df_conco.concordance_df_nova)]\n",
    "print(np.size(aux))\n",
    "aux.hist(bins=11,weights = 100*np.ones_like(aux.index) / len(aux.index),zorder=3)\n",
    "plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Concordance_DF_NOVA.eps', format='eps',bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Node sensors VS Siata station\")\n",
    "plt.xlabel(\"Comparability (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "aux1=100*df_conco.concordance_df_siata.loc[~np.isnan(df_conco.concordance_df_siata)]\n",
    "aux2=100*df_conco.concordance_nova_siata.loc[~np.isnan(df_conco.concordance_nova_siata)]\n",
    "w = [100*np.ones_like(aux1.index) / len(aux1.index),100*np.ones_like(aux2.index) / len(aux2.index)]\n",
    "plt.hist([aux1, aux2], bins=11, label=['concordance_df_siata', 'concordance_nova_siata'],weights=w,zorder=3)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(\"PM2.5 vs Humidity\")\n",
    "plt.xlabel(\"Comparability (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "plt.ylim([0, 30])\n",
    "aux1=100*df_conco.concordance_df_hum.loc[~np.isnan(df_conco.concordance_df_hum)]\n",
    "aux2=100*df_conco.concordance_nova_hum.loc[~np.isnan(df_conco.concordance_nova_hum)]\n",
    "w = [100*np.ones_like(aux1.index) / len(aux1.index),100*np.ones_like(aux2.index) / len(aux2.index)]\n",
    "plt.hist([aux1, aux2], bins=11, label=['concordance_df_hum', 'concordance_nova_hum'],weights=w,zorder=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title(\"PM2.5 vs Temperature\")\n",
    "plt.xlabel(\"Comparability (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "plt.ylim([0, 30])\n",
    "aux1=100*df_conco.concordance_df_temp.loc[~np.isnan(df_conco.concordance_df_temp)]\n",
    "aux2=100*df_conco.concordance_nova_temp.loc[~np.isnan(df_conco.concordance_nova_temp)]\n",
    "w = [100*np.ones_like(aux1.index) / len(aux1.index),100*np.ones_like(aux2.index) / len(aux2.index)]\n",
    "plt.hist([aux1, aux2], bins=11, label=['concordance_df_temp', 'concordance_nova_temp'],weights=w,zorder=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Concordance_OtherVariables.eps', format='eps',bbox_inches = \"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with parallelitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def parse_file(filename):\n",
    "    ...\n",
    "\n",
    "def main():\n",
    "    pool = mp.Pool(processes=6)\n",
    "    pool.map(parse_file, ['my_dir/' + filename for filename in os.listdir(\"my_dir\")])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Setup file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights\n",
    "mu                =0.9\n",
    "Accuracy          =0.20\n",
    "Precision         =0.07\n",
    "Confidence        =0.16\n",
    "Completeness      =0.10\n",
    "Timeliness        =0.12\n",
    "Data_Volume       =0.16\n",
    "Data_Redundancy   =0.02\n",
    "Concordance       =0.16\n",
    "\n",
    "Utility           =0.12\n",
    "Accessibility     =0.16\n",
    "Interpretability  =0.28\n",
    "Reputation        =0.12\n",
    "Artificiality     =0.20\n",
    "Access_Security   =0.12\n",
    "\n",
    "\n",
    "#Period\n",
    "inicio=\"2020-02-03 00:00:00\"\n",
    "fin   =\"2020-02-03 23:59:00\"\n",
    "start_time =\"2020-02-01 00:00:00\"\n",
    "end_time   =\"2020-02-05 23:59:00\"\n",
    "\n",
    "#Variable Inicialization\n",
    "nube=0\n",
    "node=220\n",
    "contador=0\n",
    "counter=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow the user to select a file using the function: get_path('*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_path(wildcard, title):\n",
    "    app = wx.App(None)\n",
    "    style = wx.FD_OPEN | wx.FD_FILE_MUST_EXIST\n",
    "    dialog = wx.FileDialog(None, title, wildcard=wildcard, style=style)\n",
    "    if dialog.ShowModal() == wx.ID_OK:\n",
    "        path = dialog.GetPath()\n",
    "    else:\n",
    "        path = None\n",
    "    dialog.Destroy()\n",
    "    return path\n",
    "#filepath=get_path('*.csv',\"This is the title\")\n",
    "#print (filepath,type(filepath))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path for Citizen Science nodes data:  C:\\Users\\julio\\Documents\\UDEA\\Maestría\\DQ in IOT\\Datasets\\SIATA_CS\\SplitDatosCC\\Samples\\February.csv\n",
      "Source path for Siata Stations data:  C:\\Users\\julio\\Documents\\UDEA\\Maestría\\DQ in IOT\\Datasets\\SIATA Stations\\PM\\SS_PM.csv\n",
      "Citizen Scientist:  [1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 259, 261, 262, 265, 266, 267]\n",
      "Siata Stations:  [11, 12, 25, 28, 31, 37, 38, 44, 46, 48, 69, 6, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 94]\n"
     ]
    }
   ],
   "source": [
    "#Read Data from February\n",
    "header_CC=[\"codigoSerial\", \"fecha\", \"hora\", \"fechaHora\", \"temperatura\", \"humedad_relativa\", \"pm1_df\", \"pm10_df\", \"pm25_df\", \"pm1_nova\", \"pm10_nova\", \"pm25_nova\", \"calidad_temperatura\", \"calidad_humedad_relativa\", \"calidad_pm1_df\", \"calidad_pm10_df\", \"calidad_pm25_df\", \"calidad_pm1_nova\", \"calidad_pm10_nova\", \"calidad_pm25_nova\"]\n",
    "datatypes_CC={\"codigoSerial\":np.uint16, \"temperatura\":np.float16, \"humedad_relativa\":np.float16, \"pm1_df\":np.float32, \"pm10_df\":np.float32, \"pm25_df\":np.float32, \"pm1_nova\":np.float32, \"pm10_nova\":np.float32, \"pm25_nova\":np.float32}\n",
    "\n",
    "path_for_CC_data=get_path('*.csv',\"Select Citizen Scientist *.csv file\")\n",
    "try:\n",
    "    df_CC = pd.read_csv(path_for_CC_data, header=None, names=header_CC, usecols=header_CC , dtype=datatypes_CC,parse_dates=[\"fecha\",\"hora\",\"fechaHora\"])\n",
    "    print(\"Source path for Citizen Science nodes data: \",path_for_CC_data)\n",
    "\n",
    "    #Data includes January, February and March\n",
    "    header_SS=[\"Fecha_Hora\",\"codigoSerial\",\"pm25\",\"calidad_pm25\",\"pm10\",\"calidad_pm10\"]\n",
    "    datatypes_SS={\"codigoSerial\":np.uint16,\"pm25\":np.float32,\"pm10\":np.float32}\n",
    "    path_for_SS_data=get_path('*.csv',\"Select SIATA Stations *.csv file\")\n",
    "    df_SS = pd.read_csv(path_for_SS_data, header=None,names=header_SS, usecols=header_SS , dtype=datatypes_SS,parse_dates=[\"Fecha_Hora\"])\n",
    "    print(\"Source path for Siata Stations data: \",path_for_SS_data)\n",
    "    \n",
    "    grouped=df_CC.groupby(df_CC.codigoSerial)\n",
    "    CC={}\n",
    "    print(\"Citizen Scientist: \", sorted(list(df_CC.codigoSerial.unique())))\n",
    "    for i in df_CC.codigoSerial.unique():\n",
    "        CC[i] = grouped.get_group(i).sort_values(by=['fechaHora'],ignore_index=True)\n",
    "    del df_CC\n",
    "    \n",
    "    grouped=df_SS.groupby(df_SS.codigoSerial)\n",
    "    SS={}\n",
    "    print(\"Siata Stations: \", list(df_SS.codigoSerial.unique()))\n",
    "    for j in df_SS.codigoSerial.unique():\n",
    "        SS[j] = grouped.get_group(j).sort_values(by=['Fecha_Hora'],ignore_index=True)\n",
    "    del df_SS\n",
    "    del grouped\n",
    "except:\n",
    "    print(\"An exception occurred, it is possible that wrong files were chosen, please run again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data with dialogbox\n",
    "def get_path(wildcard, title):\n",
    "    app = wx.App(None)\n",
    "    style = wx.FD_OPEN | wx.FD_FILE_MUST_EXIST\n",
    "    dialog = wx.FileDialog(None, title, wildcard=wildcard, style=style)\n",
    "    if dialog.ShowModal() == wx.ID_OK:\n",
    "        path = dialog.GetPath()\n",
    "    else:\n",
    "        path = None\n",
    "    dialog.Destroy()\n",
    "    return path\n",
    "\n",
    "#DATA CLEANING\n",
    "def clean_data(node, CC, start_time, end_time):\n",
    "    \n",
    "    #define a time windows of the data to be analyzed/cleaned\n",
    "    df_window=CC[node][(CC[node]['fechaHora'] >= start_time) & (CC[node]['fechaHora'] <= end_time)]\n",
    "    #Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999]\n",
    "    df_window=df_window.copy()\n",
    "    df_window.loc[df_window[\"pm25_nova\"]>999,\"pm25_nova\"]=np.nan\n",
    "    df_window.loc[df_window[\"pm25_nova\"]<0,\"pm25_nova\"]=np.nan\n",
    "    df_window.loc[df_window[\"pm25_df\"]>999,\"pm25_df\"]=np.nan\n",
    "    df_window.loc[df_window[\"pm25_df\"]<0,\"pm25_df\"]=np.nan\n",
    "    \n",
    "    #Remove data above the whiskers of the boxplot: i.e. anomaly data\n",
    "    Q1 = df_window['pm25_df'].quantile(0.25)\n",
    "    Q3 = df_window['pm25_df'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_window.loc[df_window[\"pm25_df\"]>=Q3 + 1.5 *IQR,\"pm25_df\"]=np.nan\n",
    "    \n",
    "    Q1 = df_window['pm25_nova'].quantile(0.25)\n",
    "    Q3 = df_window['pm25_nova'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_window.loc[df_window[\"pm25_nova\"]>=Q3 + 1.5 *IQR,\"pm25_nova\"]=np.nan\n",
    "    \n",
    "    return df_window\n",
    "\n",
    "#PRECISION\n",
    "\n",
    "def precision(node, df_window):\n",
    "    #Hourly standard deviation\n",
    "    df_window['pm25_nova_pre']=np.nan\n",
    "    df_window['pm25_df_pre']=np.nan\n",
    "    for ts in df_window['fechaHora']:\n",
    "        if ts==ts.ceil('60min'):\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]# Next ONE HOUR WINDOW\n",
    "            \n",
    "        else:\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]#Current ONE HOUR window\n",
    "        #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\n",
    "        #print(window['pm25_nova'])        \n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_pre']=1-(window['pm25_nova'].std()/window['pm25_nova'].mean())# 1-Coefficient of Variation (std/mean)\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_pre']=1-(window['pm25_df'].std()/window['pm25_df'].mean())# 1-Coefficient of Variation (std/mean)\n",
    "        \n",
    "    #del window\n",
    "    prec_df=df_window.pm25_df_pre.mean()#Average precision of the whole node for DF\n",
    "    prec_nova=df_window.pm25_nova_pre.mean()  #Average precision of the whole node for NOVA  \n",
    "    preci_dict={\"codigoSerial\":node,\"precision_df\":prec_df,\"precision_nova\":prec_nova}\n",
    "    #print(\"%d. Nube: %d, Overall relative (Precision) Standard Deviation.\"%(contador,nube))\n",
    "    del prec_df\n",
    "    del prec_nova\n",
    "    return preci_dict\n",
    "\n",
    "\n",
    "#df_preci=df_preci.append(precision(node, df_window), ignore_index = True)\n",
    "#df_preci.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_preci.csv\",index=False)\n",
    "\n",
    "#UNCERTAINTY\n",
    "def uncertainty(node, df_window):\n",
    "    #Hourly standard deviation\n",
    "\n",
    "    df_window['pm25_unc']=np.nan\n",
    "    for ts in df_window['fechaHora']:\n",
    "        if ts==ts.ceil('60min'):\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]# Next ONE HOUR WINDOW\n",
    "            \n",
    "        else:\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]#Current ONE HOUR window\n",
    "        #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\n",
    "        #print(window['pm25_nova'])        \n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_unc']=\\\n",
    "        1-np.sqrt((window.pm25_df-window.pm25_nova).pow(2).mean()/2)/((window.pm25_df+window.pm25_nova).mean()/2)\n",
    "        \n",
    "    #del window\n",
    "    \n",
    "    uncer_df=df_window.pm25_unc.mean()#Average uncertainty of the whole node\n",
    "    uncer_dict={\"codigoSerial\":node,\"uncertainty\":uncer_df}\n",
    "    del uncer_df\n",
    "    \n",
    "    return uncer_dict\n",
    "\n",
    "\n",
    "#df_preci=df_preci.append(precision(node, df_window), ignore_index = True)\n",
    "#df_preci.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_preci.csv\",index=False)\n",
    "\n",
    "#ACCURACY\n",
    "\n",
    "def accuracy(node, df_window):\n",
    "    #Hourly mean\n",
    "    df_window['pm25_nova_ave']=np.nan\n",
    "    df_window['pm25_df_ave']=np.nan    \n",
    "    df_window['alpha_df']=np.nan   \n",
    "    df_window['alpha_nova']=np.nan \n",
    "    \n",
    "    \n",
    "    for ts in df_window['fechaHora']:\n",
    "        if ts==ts.ceil('60min'):\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]# Next ONE HOUR WINDOW\n",
    "            \n",
    "        else:\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]#Current ONE HOUR window\n",
    "        #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\n",
    "        #print(window['pm25_nova'])        \n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_ave']=window['pm25_nova'].mean()\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_ave']=window['pm25_df'].mean()\n",
    "\n",
    "    Closest_Station=Distances.codigoSerial_ES.loc[node]    \n",
    "    Closest_Station2=Distances.codigoSerial_ES2.loc[node] \n",
    "    if Closest_Station in SS.keys():\n",
    "        #Clean values out of range\n",
    "        SS[Closest_Station].loc[SS[Closest_Station][\"pm25\"]<=0,\"pm25\"]=np.nan\n",
    "        for time in df_window.fechaHora:\n",
    "            \n",
    "            idx=SS[Closest_Station].Fecha_Hora.searchsorted(time,side=\"right\")\n",
    "            #print(idx, SS[Closest_Station].Fecha_Hora.loc[idx], time)\n",
    "            v=SS[Closest_Station].loc[idx,\"pm25\"]\n",
    "            df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_df_ave\"]\n",
    "            #print(time,\" : \",vm.values[0],\"________\",SS[Closest_Station].Fecha_Hora.loc[idx],\" : \",v)\n",
    "            #print(v,\"<->\", vm.values[0])\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_df\"]=  max(0,1-abs(vm.values[0]-v)/v)\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_nova_ave\"]\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_nova\"]=max(0,1-abs(vm.values[0]-v)/v)\n",
    "        \n",
    "        accu_dict={'codigoSerial':node, 'dist':Distances.loc[node,\"Distancia_a_ES\"], 'acc_df':df_window.alpha_df.mean(), 'acc_nova':df_window.alpha_nova.mean()}\n",
    "    \n",
    "    elif Closest_Station2 in SS.keys():\n",
    "        #Clean values out of range\n",
    "        SS[Closest_Station2].loc[SS[Closest_Station2][\"pm25\"]<=0,\"pm25\"]=np.nan\n",
    "        for time in df_window.fechaHora:\n",
    "            \n",
    "            idx=SS[Closest_Station2].Fecha_Hora.searchsorted(time,side=\"right\")\n",
    "            #print(idx, SS[Closest_Station].Fecha_Hora.loc[idx], time)\n",
    "            v=SS[Closest_Station2].loc[idx,\"pm25\"]\n",
    "            df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_df_ave\"]\n",
    "            #print(time,\" : \",vm.values[0],\"________\",SS[Closest_Station].Fecha_Hora.loc[idx],\" : \",v)\n",
    "            #print(v, vm)\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_df\"]=  max(0,1-abs(vm.values[0]-v)/v)\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_nova_ave\"]\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_nova\"]=max(0,1-abs(vm.values[0]-v)/v)\n",
    "        \n",
    "        accu_dict={'codigoSerial':node, 'dist':Distances.loc[node,\"Distancia_a_ES\"], 'acc_df':df_window.alpha_df.mean(), 'acc_nova':df_window.alpha_nova.mean()}\n",
    "        \n",
    "    else:\n",
    "        accu_dict={'codigoSerial':node, 'dist':np.nan, 'acc_df':np.nan, 'acc_nova':np.nan}\n",
    "    return accu_dict\n",
    "\n",
    "#CONCORDANCE\n",
    "def concordance(node, df_window):\n",
    "    df_window['v_pm25']=np.nan\n",
    "    \n",
    "    Closest_Station=Distances.codigoSerial_ES.loc[node]    \n",
    "    Closest_Station2=Distances.codigoSerial_ES2.loc[node] \n",
    "    \n",
    "    #df_window=CC[node][(CC[node]['fechaHora'] >= start_time) & (CC[node]['fechaHora'] <= end_time)]\n",
    "    \n",
    "    if Closest_Station in SS.keys():\n",
    "       \n",
    "        #Clean values out of range\n",
    "        SS[Closest_Station].loc[SS[Closest_Station][\"pm25\"]<=0,\"pm25\"]=np.nan\n",
    "        aux_SS=SS[Closest_Station][(SS[Closest_Station]['Fecha_Hora'] >= start_time) & (SS[Closest_Station]['Fecha_Hora'] <= end_time)]\n",
    "        aux_SS.reset_index(drop=True, inplace=True)\n",
    "        #print(\"Estación Siata Mas Cercana\\n\",aux_SS)\n",
    "        #print(aux_SS)\n",
    "        #print(df_window)\n",
    "        \n",
    "        for time in df_window.fechaHora:\n",
    "            \n",
    "            #if aux_SS.Fecha_Hora.min()<=time<=aux_SS.Fecha_Hora.max():\n",
    "            #print(aux_SS.Fecha_Hora.min(), aux_SS.Fecha_Hora.max())\n",
    "            #print (time)\n",
    "            idx=aux_SS.Fecha_Hora.searchsorted(time,side=\"right\")\n",
    "            if idx in aux_SS.index:\n",
    "                \n",
    "                #print (idx)\n",
    "                #print(idx, time, aux_SS.Fecha_Hora.loc[idx])\n",
    "                v=aux_SS.loc[idx,\"pm25\"]\n",
    "                #print(v)\n",
    "            \n",
    "            df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\n",
    "   \n",
    "    #Comparability / Concordance\n",
    "\n",
    "    \n",
    "    corr_df =   df_window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[0].abs()\n",
    "    #print(corr_df)\n",
    "    \n",
    "    corr_nova = df_window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[1].abs()\n",
    "    #print(corr_nova)\n",
    "    #print(corr_df.v_pm25,corr_nova.v_pm25)\n",
    "    conco_dict={\"codigoSerial\":node,\"concordance_df_nova\":corr_df.pm25_nova,\n",
    "                \"concordance_df_siata\":corr_df.v_pm25,\"concordance_df_hum\":corr_df.humedad_relativa,\"concordance_df_temp\":corr_df.temperatura,\n",
    "                \"concordance_nova_siata\":corr_nova.v_pm25,\"concordance_nova_hum\":corr_nova.humedad_relativa,\"concordance_nova_temp\":corr_nova.temperatura}\n",
    "    return conco_dict\n",
    "\n",
    "#COMPLETENESS\n",
    "\n",
    "def completeness(node, df_window):\n",
    "    #ref_date_range = pd.date_range(inicio, fin, freq='1Min')\n",
    "    #ref_date_range = pd.DataFrame(ref_date_range,columns=[\"ref_fechaHora\"])\n",
    "    ref_date_range = pd.DataFrame(pd.date_range(start_time, end_time, freq='1Min'),columns=[\"ref_fechaHora\"])\n",
    "  \n",
    "    #Check for any missing date\n",
    "    missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(df_window.fechaHora),\"ref_fechaHora\"]\n",
    "\n",
    "    #Add missing date rows\n",
    "    for missing in missing_dates:\n",
    "        df_window=df_window.append({\"codigoSerial\":node,\"fechaHora\":missing}, ignore_index = True)\n",
    "    \n",
    "    #Check for any missing date\n",
    "    #missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(df_window.fechaHora),\"ref_fechaHora\"]\n",
    "    \n",
    "    #Check for missing data\n",
    "    missing_data_df=np.count_nonzero(np.isnan(df_window['pm25_df']))\n",
    "    missing_data_nova=np.count_nonzero(np.isnan(df_window['pm25_nova']))\n",
    "    comp_df=(1-missing_data_df/np.size(df_window.pm25_df))\n",
    "    comp_nova=(1-missing_data_nova/np.size(df_window.pm25_nova))\n",
    "    \n",
    "    comp_dict={\"codigoSerial\":node,\"completeness_df\":comp_df,\"completeness_nova\":comp_nova}\n",
    "    return comp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPLYING THE FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_vs_dis=[]\n",
    "missing_data_df=[]\n",
    "missing_data_nova=[]\n",
    "t0= time.time()\n",
    "df_accur = pd.DataFrame(columns =['codigoSerial', 'dist', 'acc_nova', 'acc_df'])\n",
    "df_comple = pd.DataFrame(columns =[\"codigoSerial\",\"completeness_df\",\"completeness_nova\"])\n",
    "df_preci = pd.DataFrame(columns =[\"codigoSerial\",\"precision_df\",\"precision_nova\",])\n",
    "df_uncer = pd.DataFrame(columns =[\"codigoSerial\",\"uncertainty\"])\n",
    "df_conco = pd.DataFrame(columns =[\"codigoSerial\",\"concordance_df_nova\",\"concordance_df_siata\",\"concordance_df_hum\",\"concordance_df_temp\",\"concordance_nova_siata\",\"concordance_nova_hum\",\"concordance_nova_temp\"])\n",
    "\n",
    "contador=0\n",
    "#for nubes in CC.keys():\n",
    "for nubes in range(1,3):\n",
    "    contador+=1\n",
    "    #CC[nube][\"v_pm25\"] = np.nan\n",
    "    #CC[nube][\"alpha_df\"] = np.nan\n",
    "    #CC[nube][\"alpha_nova\"] = np.nan\n",
    "    #del df_window\n",
    "    \n",
    "    df_window=clean_data(nubes, CC, start_time, end_time)\n",
    "    #PRECISION\n",
    "    df_preci=df_preci.append(precision(nubes, df_window), ignore_index = True)\n",
    "    print(\"%d. Nube: %d, Overall relative (Precision) Standard Deviation.\"%(contador,nubes))\n",
    "    \n",
    "    #UNCERTAINTY\n",
    "    df_uncer=df_uncer.append(uncertainty(nubes, df_window), ignore_index = True)\n",
    "    print(\"%d. Nube: %d, Overall relative Uncertainty BS, \"%(contador,nubes))\n",
    "    \n",
    "    #ACCURACY\n",
    "    df_accur=df_accur.append(accuracy(nubes, df_window), ignore_index = True)\n",
    "    print(\"%d. Nube: %d, Accuracy,\" %(contador, nubes))\n",
    "    \n",
    "    \n",
    "    #COMPLETENESS\n",
    "    df_comple=df_comple.append(completeness(nubes, df_window), ignore_index = True)\n",
    "    print(\"%d. Nube: %d, Completeness.\" %(contador,nubes))\n",
    "\n",
    "    #COMPARABILITY / Concordance\n",
    "    df_conco=df_conco.append(concordance(nubes,df_window),ignore_index = True)\n",
    "    print(\"%d. Nube: %d, Overall concordance, \"%(contador,nubes))\n",
    "t1 = time.time() - t0\n",
    "print(\"Time elapsed: \", t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codigoSerial</th>\n",
       "      <th>concordance_df_nova</th>\n",
       "      <th>concordance_df_siata</th>\n",
       "      <th>concordance_df_hum</th>\n",
       "      <th>concordance_df_temp</th>\n",
       "      <th>concordance_nova_siata</th>\n",
       "      <th>concordance_nova_hum</th>\n",
       "      <th>concordance_nova_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.950743</td>\n",
       "      <td>0.089981</td>\n",
       "      <td>0.555877</td>\n",
       "      <td>0.460604</td>\n",
       "      <td>0.079224</td>\n",
       "      <td>0.604821</td>\n",
       "      <td>0.558862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.912140</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>0.568322</td>\n",
       "      <td>0.460272</td>\n",
       "      <td>0.031866</td>\n",
       "      <td>0.685429</td>\n",
       "      <td>0.586430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.966554</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>0.519615</td>\n",
       "      <td>0.483295</td>\n",
       "      <td>0.019493</td>\n",
       "      <td>0.517939</td>\n",
       "      <td>0.477286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.955969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.452727</td>\n",
       "      <td>0.412620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.538445</td>\n",
       "      <td>0.522010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.950743</td>\n",
       "      <td>0.089981</td>\n",
       "      <td>0.555877</td>\n",
       "      <td>0.460604</td>\n",
       "      <td>0.079224</td>\n",
       "      <td>0.604821</td>\n",
       "      <td>0.558862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.892706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.472084</td>\n",
       "      <td>0.404870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.535065</td>\n",
       "      <td>0.531619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>0.060384</td>\n",
       "      <td>0.572648</td>\n",
       "      <td>0.542892</td>\n",
       "      <td>0.058060</td>\n",
       "      <td>0.540637</td>\n",
       "      <td>0.523513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.972327</td>\n",
       "      <td>0.016319</td>\n",
       "      <td>0.067837</td>\n",
       "      <td>0.067240</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>0.050561</td>\n",
       "      <td>0.049904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.972327</td>\n",
       "      <td>0.033135</td>\n",
       "      <td>0.067837</td>\n",
       "      <td>0.067240</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.050561</td>\n",
       "      <td>0.049904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.972327</td>\n",
       "      <td>0.033135</td>\n",
       "      <td>0.067837</td>\n",
       "      <td>0.067240</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.050561</td>\n",
       "      <td>0.049904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>0.055424</td>\n",
       "      <td>0.572648</td>\n",
       "      <td>0.542892</td>\n",
       "      <td>0.061958</td>\n",
       "      <td>0.540637</td>\n",
       "      <td>0.523513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>0.055424</td>\n",
       "      <td>0.572648</td>\n",
       "      <td>0.542892</td>\n",
       "      <td>0.061958</td>\n",
       "      <td>0.540637</td>\n",
       "      <td>0.523513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>0.055424</td>\n",
       "      <td>0.572648</td>\n",
       "      <td>0.542892</td>\n",
       "      <td>0.061958</td>\n",
       "      <td>0.540637</td>\n",
       "      <td>0.523513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    codigoSerial  concordance_df_nova  concordance_df_siata  \\\n",
       "0            5.0             0.950743              0.089981   \n",
       "1            1.0             0.912140              0.013849   \n",
       "2            2.0             0.966554              0.007825   \n",
       "3            3.0                  NaN                   NaN   \n",
       "4            4.0             0.955969                   NaN   \n",
       "5            5.0             0.950743              0.089981   \n",
       "6            9.0             0.892706                   NaN   \n",
       "7           10.0             0.975953              0.060384   \n",
       "8           11.0             0.972327              0.016319   \n",
       "9           11.0             0.972327              0.033135   \n",
       "10          11.0             0.972327              0.033135   \n",
       "11          10.0             0.975953              0.055424   \n",
       "12          10.0             0.975953              0.055424   \n",
       "13          10.0             0.975953              0.055424   \n",
       "\n",
       "    concordance_df_hum  concordance_df_temp  concordance_nova_siata  \\\n",
       "0             0.555877             0.460604                0.079224   \n",
       "1             0.568322             0.460272                0.031866   \n",
       "2             0.519615             0.483295                0.019493   \n",
       "3                  NaN                  NaN                     NaN   \n",
       "4             0.452727             0.412620                     NaN   \n",
       "5             0.555877             0.460604                0.079224   \n",
       "6             0.472084             0.404870                     NaN   \n",
       "7             0.572648             0.542892                0.058060   \n",
       "8             0.067837             0.067240                0.008759   \n",
       "9             0.067837             0.067240                0.000766   \n",
       "10            0.067837             0.067240                0.000766   \n",
       "11            0.572648             0.542892                0.061958   \n",
       "12            0.572648             0.542892                0.061958   \n",
       "13            0.572648             0.542892                0.061958   \n",
       "\n",
       "    concordance_nova_hum  concordance_nova_temp  \n",
       "0               0.604821               0.558862  \n",
       "1               0.685429               0.586430  \n",
       "2               0.517939               0.477286  \n",
       "3                    NaN                    NaN  \n",
       "4               0.538445               0.522010  \n",
       "5               0.604821               0.558862  \n",
       "6               0.535065               0.531619  \n",
       "7               0.540637               0.523513  \n",
       "8               0.050561               0.049904  \n",
       "9               0.050561               0.049904  \n",
       "10              0.050561               0.049904  \n",
       "11              0.540637               0.523513  \n",
       "12              0.540637               0.523513  \n",
       "13              0.540637               0.523513  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST CODE TO CHECK THE FUNCTIONS\n",
    "\n",
    "node=10\n",
    "df_window=clean_data(node, CC, start_time, end_time)\n",
    "#print(df_window)\n",
    "#df_preci = pd.DataFrame(columns =[\"codigoSerial\",\"precision_df\",\"precision_nova\",])\n",
    "#df_uncer = pd.DataFrame(columns =[\"codigoSerial\",\"uncertainty\",\"uncertainty_group\"])\n",
    "#df_conco = pd.DataFrame(columns =[\"codigoSerial\",\"concordance_df_nova\",\"concordance_df_siata\",\"concordance_df_hum\",\"concordance_df_temp\",\"concordance_nova_siata\",\"concordance_nova_hum\",\"concordance_nova_temp\"])\n",
    "#df_accur = pd.DataFrame(columns =['codigoSerial', 'dist', 'acc_nova', 'acc_df'])\n",
    "#df_comple = pd.DataFrame(columns =[\"codigoSerial\",\"completeness_df\",\"completeness_nova\"])\n",
    "\n",
    "\n",
    "df_conco=df_conco.append(concordance(node, df_window), ignore_index = True)\n",
    "df_conco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallelization CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path for Citizen Science nodes data:  C:\\Users\\julio\\Documents\\UDEA\\Maestría\\DQ in IOT\\Datasets\\SIATA Stations\\PM\\February.csv\n",
      "Source path for Siata Stations data:  C:\\Users\\julio\\Documents\\UDEA\\Maestría\\DQ in IOT\\Datasets\\SIATA Stations\\PM\\SS_PM.csv\n",
      "Citizen Scientist:  [1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 259, 261, 262, 265, 266, 267]\n",
      "Siata Stations:  [11, 12, 25, 28, 31, 37, 38, 44, 46, 48, 69, 6, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 94]\n",
      "Start time:  2021-09-22 22:12:06.329302\n",
      "End Time:  2021-09-22 22:15:04.991906\n",
      "Elapsed Time:  178.6782329082489\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sn\n",
    "import requests\n",
    "import json\n",
    "import haversine as hs\n",
    "import wx\n",
    "\n",
    "#This is the base code that worked when testing multiprocessing!\n",
    "#import multiprocessing as mp\n",
    "##from multiprocessing import Pool\n",
    "#import workers\n",
    "#if __name__ ==  '__main__': \n",
    "# pool=mp.Pool(processes = mp.cpu_count())\n",
    "# pool.map(workers.worker,[i for i in range(0,3000)])\n",
    "# #print(output)\n",
    "start_time =\"2020-02-20 00:00:00\"\n",
    "end_time   =\"2020-02-20 05:00:00\"\n",
    "\n",
    "#Read Data from February\n",
    "header_CC=[\"codigoSerial\", \"fecha\", \"hora\", \"fechaHora\", \"temperatura\", \"humedad_relativa\", \"pm1_df\", \"pm10_df\", \"pm25_df\", \"pm1_nova\", \"pm10_nova\", \"pm25_nova\", \"calidad_temperatura\", \"calidad_humedad_relativa\", \"calidad_pm1_df\", \"calidad_pm10_df\", \"calidad_pm25_df\", \"calidad_pm1_nova\", \"calidad_pm10_nova\", \"calidad_pm25_nova\"]\n",
    "datatypes_CC={\"codigoSerial\":np.uint16, \"temperatura\":np.float16, \"humedad_relativa\":np.float16, \"pm1_df\":np.float32, \"pm10_df\":np.float32, \"pm25_df\":np.float32, \"pm1_nova\":np.float32, \"pm10_nova\":np.float32, \"pm25_nova\":np.float32}\n",
    "\n",
    "def get_path(wildcard, title):\n",
    "    app = wx.App(None)\n",
    "    style = wx.FD_OPEN | wx.FD_FILE_MUST_EXIST\n",
    "    dialog = wx.FileDialog(None, title, wildcard=wildcard, style=style)\n",
    "    if dialog.ShowModal() == wx.ID_OK:\n",
    "        path = dialog.GetPath()\n",
    "    else:\n",
    "        path = None\n",
    "    dialog.Destroy()\n",
    "    return path\n",
    "\n",
    "path_for_CC_data=get_path('*.csv',\"Select Citizen Scientist *.csv file\")\n",
    "try:\n",
    "    df_CC = pd.read_csv(path_for_CC_data, header=None, names=header_CC, usecols=header_CC , dtype=datatypes_CC,parse_dates=[\"fecha\",\"hora\",\"fechaHora\"])\n",
    "    print(\"Source path for Citizen Science nodes data: \",path_for_CC_data)\n",
    "\n",
    "    #Data includes January, February and March\n",
    "    header_SS=[\"Fecha_Hora\",\"codigoSerial\",\"pm25\",\"calidad_pm25\",\"pm10\",\"calidad_pm10\"]\n",
    "    datatypes_SS={\"codigoSerial\":np.uint16,\"pm25\":np.float32,\"pm10\":np.float32}\n",
    "    path_for_SS_data=get_path('*.csv',\"Select SIATA Stations *.csv file\")\n",
    "    df_SS = pd.read_csv(path_for_SS_data, header=None,names=header_SS, usecols=header_SS , dtype=datatypes_SS,parse_dates=[\"Fecha_Hora\"])\n",
    "    print(\"Source path for Siata Stations data: \",path_for_SS_data)\n",
    "    \n",
    "    grouped=df_CC.groupby(df_CC.codigoSerial)\n",
    "    CC={}\n",
    "    print(\"Citizen Scientist: \", sorted(list(df_CC.codigoSerial.unique())))\n",
    "    for i in df_CC.codigoSerial.unique():\n",
    "        CC[i] = grouped.get_group(i).sort_values(by=['fechaHora'],ignore_index=True)\n",
    "    del df_CC\n",
    "    \n",
    "    grouped=df_SS.groupby(df_SS.codigoSerial)\n",
    "    SS={}\n",
    "    print(\"Siata Stations: \", list(df_SS.codigoSerial.unique()))\n",
    "    for j in df_SS.codigoSerial.unique():\n",
    "        SS[j] = grouped.get_group(j).sort_values(by=['Fecha_Hora'],ignore_index=True)\n",
    "    del df_SS\n",
    "    del grouped\n",
    "except:\n",
    "    print(\"An exception occurred, it is possible that wrong files were chosen, please run again\")\n",
    "\n",
    "datatypesDistances={\"codigoSerial_CC\":np.uint16,\"codigoSerial_ES\":np.uint16,\"Distancia_a_ES\":np.float16,\"codigoSerial_ES2\":np.uint16}\n",
    "Distances = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/Distances and positions/Distancias_2.csv\", header=0, dtype=datatypesDistances,index_col=\"codigoSerial_CC\")\n",
    "\n",
    "import DQ\n",
    "t0= time.time()\n",
    "#print(\"Start time: \", t0)\n",
    "print(\"Start time: \", datetime.fromtimestamp(t0))\n",
    "if __name__ ==  '__main__':\n",
    "    pool=mp.Pool(processes = mp.cpu_count())\n",
    "    arguments=[]\n",
    "    #results=pool.map(DQ.eval_dq,[nodes for nodes in CC.keys()])\n",
    "    results=pool.map(DQ.eval_dq,([[nodes, CC, SS, Distances, start_time, end_time] for nodes in CC.keys()]))\n",
    "    \n",
    "    \n",
    "    df_preci = pd.DataFrame(columns =[\"codigoSerial\",\"precision_df\",\"precision_nova\",])\n",
    "    df_uncer = pd.DataFrame(columns =[\"codigoSerial\",\"uncertainty\"])\n",
    "    df_conco = pd.DataFrame(columns =[\"codigoSerial\",\"concordance_df_nova\",\"concordance_df_siata\",\"concordance_df_hum\",\"concordance_df_temp\",\"concordance_nova_siata\",\"concordance_nova_hum\",\"concordance_nova_temp\"])\n",
    "    df_accur = pd.DataFrame(columns =['codigoSerial', 'dist', 'acc_nova', 'acc_df'])\n",
    "    df_comple = pd.DataFrame(columns =[\"codigoSerial\",\"completeness_df\",\"completeness_nova\"])\n",
    "    \n",
    "    for i in range(0,len(results)):\n",
    "        df_preci=df_preci.append(results[i][0], ignore_index = True)\n",
    "        df_uncer=df_uncer.append(results[i][1], ignore_index = True)\n",
    "        df_accur=df_accur.append(results[i][2], ignore_index = True)\n",
    "        df_comple=df_comple.append(results[i][3], ignore_index = True)\n",
    "        df_conco=df_conco.append(results[i][4],ignore_index = True)\n",
    "    \n",
    "    #print(df_preci.tail())\n",
    "    #print(df_uncer.tail())\n",
    "    #print(df_accur.tail())\n",
    "    #print(df_comple.tail())\n",
    "    #print(df_conco.tail())\n",
    "\n",
    "print(\"End Time: \", datetime.fromtimestamp(time.time()))\n",
    "t1 = time.time() - t0\n",
    "print(\"Elapsed Time: \", t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-02-03 10:00:00')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST CODE TO PLAY WITH STRINGS AND TIMESTAMPS FORMATS\n",
    "from datetime import datetime\n",
    "inicio=\"2020-02-03 10:59:00\"\n",
    "datetime_str = inicio\n",
    "\n",
    "ts = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "ts=pd.Timestamp(datetime_str).floor('60min')\n",
    "\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_preci.head())\n",
    "print(df_uncer.head())\n",
    "print(df_accur.head())\n",
    "print(df_comple.head())\n",
    "print(df_conco.concordance_df_siata.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPORT TO GOOGLE SHEETS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Google\n",
    "from Google import Create_Service\n",
    "\n",
    "\n",
    "CLIENT_SECRET_FILE = 'client_secret.json'\n",
    "API_SERVICE_NAME = 'sheets'\n",
    "API_VERSION = 'v4'\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "gsheetId = '1gkU9I2EqgJCLm-XemWq_oFenkOcsaHRKwDpO2kK7vd8'\n",
    "\n",
    "service = Create_Service(CLIENT_SECRET_FILE, API_SERVICE_NAME, API_VERSION, SCOPES)\n",
    "\n",
    "def Export_Data_To_Sheets(df_preci, df_uncer):\n",
    "\n",
    "    df_preci.replace(np.nan, '', inplace=True)\n",
    "\n",
    "    response_date = service.spreadsheets().values().append(\n",
    "        spreadsheetId=gsheetId,\n",
    "        valueInputOption='RAW',\n",
    "        range='PRECISION!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=df_preci.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "\n",
    "\n",
    "\n",
    "    df_uncer.replace(np.nan, '', inplace=True)\n",
    "\n",
    "    response_date = service.spreadsheets().values().append(\n",
    "        spreadsheetId=gsheetId,\n",
    "        valueInputOption='RAW',\n",
    "        range='UNCERTAINTY!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=df_uncer.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "\n",
    "Export_Data_To_Sheets(df_preci, df_uncer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "import pandas as pd\n",
    "\n",
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "# The ID and range of a sample spreadsheet.\n",
    "SAMPLE_SPREADSHEET_ID = '1gkU9I2EqgJCLm-XemWq_oFenkOcsaHRKwDpO2kK7vd8'\n",
    "#SAMPLE_RANGE_NAME = 'Class Data!A2:E'\n",
    "\n",
    "def main():\n",
    "    \"\"\"Shows basic usage of the Sheets API.\n",
    "    Prints values from a sample spreadsheet.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "    # Call the Sheets API\n",
    "#    sheet = service.spreadsheets()\n",
    "#    result = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "#                                range=SAMPLE_RANGE_NAME).execute()\n",
    "#    values = result.get('values', [])\n",
    "\n",
    "#    if not values:\n",
    "#        print('No data found.')\n",
    "#    else:\n",
    "#        print('Name, Major:')\n",
    "#        for row in values:\n",
    "#            # Print columns A and E, which correspond to indices 0 and 4.\n",
    "#            print('%s, %s' % (row[0], row[4]))\n",
    "            \n",
    "    #df_preci = pd.DataFrame(columns =[\"codigoSerial\",\"precision_df\",\"precision_nova\",])\n",
    "    #df_uncer = pd.DataFrame(columns =[\"codigoSerial\",\"uncertainty\"])\n",
    "    df_preci.replace(np.nan, '', inplace=True)\n",
    "    df_uncer.replace(np.nan, '', inplace=True)\n",
    "    df_accur.replace(np.nan, '', inplace=True)\n",
    "    df_comple.replace(np.nan, '', inplace=True)\n",
    "    df_conco.replace(np.nan, '', inplace=True)\n",
    "    \n",
    "    #PRECISION\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().update(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='PRECISION!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=df_preci.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "    \n",
    "    #UNCERTAINTY\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().update(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='UNCERTAINTY!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=df_uncer.T.reset_index().T.values.tolist())       \n",
    "    ).execute()\n",
    "    \n",
    "    #ACCURACY\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().update(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='ACCURACY!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=df_accur.T.reset_index().T.values.tolist())       \n",
    "    ).execute()\n",
    "    \n",
    "    #COMPLETENESS\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().update(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='COMPLETENESS!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=df_comple.T.reset_index().T.values.tolist())       \n",
    "    ).execute()\n",
    "    \n",
    "    #CONCORDANCE\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().update(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='CONCORDANCE!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=df_conco.T.reset_index().T.values.tolist())       \n",
    "    ).execute()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f295ec510066dddcc717ae6d4935b3ad0a45b533511561df7ff2b4bcd803903f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
