{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to install if not installed yet\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install sklearn\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install haversine\n",
    "!pip install -U wxPython \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import seaborn as sn\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import haversine as hs\n",
    "\n",
    "import wx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citizen Scientist:  [1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 259, 261, 262, 265, 266, 267]\n",
      "Siata Stations:  [11, 12, 25, 28, 31, 37, 38, 44, 46, 48, 69, 6, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 94]\n"
     ]
    }
   ],
   "source": [
    "#Read Data from February\r\n",
    "header_CC=[\"codigoSerial\", \"fecha\", \"hora\", \"fechaHora\", \"temperatura\", \"humedad_relativa\", \"pm1_df\", \"pm10_df\", \"pm25_df\", \"pm1_nova\", \"pm10_nova\", \"pm25_nova\", \"calidad_temperatura\", \"calidad_humedad_relativa\", \"calidad_pm1_df\", \"calidad_pm10_df\", \"calidad_pm25_df\", \"calidad_pm1_nova\", \"calidad_pm10_nova\", \"calidad_pm25_nova\"]\r\n",
    "datatypes_CC={\"codigoSerial\":np.uint16, \"temperatura\":np.float16, \"humedad_relativa\":np.float16, \"pm1_df\":np.float32, \"pm10_df\":np.float32, \"pm25_df\":np.float32, \"pm1_nova\":np.float32, \"pm10_nova\":np.float32, \"pm25_nova\":np.float32}\r\n",
    "df_CC = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/SIATA_CS/SplitDatosCC/Samples/\"+\"February.csv\", header=None, names=header_CC, usecols=header_CC , dtype=datatypes_CC,parse_dates=[\"fecha\",\"hora\",\"fechaHora\"])\r\n",
    "\r\n",
    "#Data includes January, February and March\r\n",
    "header_SS=[\"Fecha_Hora\",\"codigoSerial\",\"pm25\",\"calidad_pm25\",\"pm10\",\"calidad_pm10\"]\r\n",
    "datatypes_SS={\"codigoSerial\":np.uint16,\"pm25\":np.float32,\"pm10\":np.float32}\r\n",
    "df_SS = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/SIATA Stations/PM/\"+\"SS_PM.csv\", header=None,names=header_SS, usecols=header_SS , dtype=datatypes_SS,parse_dates=[\"Fecha_Hora\"])\r\n",
    "\r\n",
    "\r\n",
    "grouped=df_CC.groupby(df_CC.codigoSerial)\r\n",
    "CC={}\r\n",
    "print(\"Citizen Scientist: \", sorted(list(df_CC.codigoSerial.unique())))\r\n",
    "for i in df_CC.codigoSerial.unique():\r\n",
    "    CC[i] = grouped.get_group(i).sort_values(by=['fechaHora'],ignore_index=True)\r\n",
    "del df_CC\r\n",
    "\r\n",
    "grouped=df_SS.groupby(df_SS.codigoSerial)\r\n",
    "SS={}\r\n",
    "print(\"Siata Stations: \", list(df_SS.codigoSerial.unique()))\r\n",
    "for j in df_SS.codigoSerial.unique():\r\n",
    "    SS[j] = grouped.get_group(j).sort_values(by=['Fecha_Hora'],ignore_index=True)\r\n",
    "del df_SS\r\n",
    "del grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DISTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypesDistances={\"codigoSerial_CC\":np.uint16,\"codigoSerial_ES\":np.uint16,\"Distancia_a_ES\":np.float16,\"codigoSerial_ES2\":np.uint16}\n",
    "Distances = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/Distances and positions/Distancias_2.csv\", header=0, dtype=datatypesDistances,index_col=\"codigoSerial_CC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQ CALCULATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inicio=\"2020-02-01 00:00:00\"\r\n",
    "fin=   \"2020-02-29 23:59:00\"\r\n",
    "nube=0\r\n",
    "contador=0\r\n",
    "\r\n",
    "acc_vs_dis=[]\r\n",
    "missing_data_df=[]\r\n",
    "missing_data_nova=[]\r\n",
    "\r\n",
    "df_accur = pd.DataFrame(columns =['codigoSerial', 'dist', 'acc_nova', 'acc_df'])\r\n",
    "df_comple = pd.DataFrame(columns =[\"codigoSerial\",\"completeness_df\",\"completeness_nova\",\"completeness_group_df\",\"completeness_group_nova\"])\r\n",
    "df_preci = pd.DataFrame(columns =[\"codigoSerial\",\"precision_df\",\"precision_nova\",\"precision_group_df\",\"precision_group_nova\"])\r\n",
    "df_uncer = pd.DataFrame(columns =[\"codigoSerial\",\"uncertainty\",\"uncertainty_group\"])\r\n",
    "df_conco = pd.DataFrame(columns =[\"codigoSerial\",\"concordance_df_nova\",\"concordance_df_siata\",\"concordance_df_hum\",\"concordance_df_temp\",\"concordance_nova_siata\",\"concordance_nova_hum\",\"concordance_nova_temp\"])\r\n",
    "\r\n",
    "for nube in CC.keys():\r\n",
    "    contador+=1\r\n",
    "    CC[nube][\"v_pm25\"] = np.nan\r\n",
    "    CC[nube][\"alpha_df\"] = np.nan\r\n",
    "    CC[nube][\"alpha_nova\"] = np.nan\r\n",
    "    #del df_window\r\n",
    "    df_window=CC[nube][(CC[nube]['fechaHora'] >= inicio) & (CC[nube]['fechaHora'] <= fin)]\r\n",
    "    \r\n",
    "    #Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999]\r\n",
    "\r\n",
    "    df_window=df_window.copy()\r\n",
    "    df_window.loc[df_window[\"pm25_nova\"]>999,\"pm25_nova\"]=np.nan\r\n",
    "    df_window.loc[df_window[\"pm25_nova\"]<0,\"pm25_nova\"]=np.nan\r\n",
    "    df_window.loc[df_window[\"pm25_df\"]>999,\"pm25_df\"]=np.nan\r\n",
    "    df_window.loc[df_window[\"pm25_df\"]<0,\"pm25_df\"]=np.nan\r\n",
    "    \r\n",
    "    #Remove data above the whiskers of the boxplot\r\n",
    "    Q1 = df_window['pm25_df'].quantile(0.25)\r\n",
    "    Q3 = df_window['pm25_df'].quantile(0.75)\r\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \r\n",
    "    df_window.loc[df_window[\"pm25_df\"]>=Q3 + 1.5 *IQR,\"pm25_df\"]=np.nan\r\n",
    "    \r\n",
    "    Q1 = df_window['pm25_nova'].quantile(0.25)\r\n",
    "    Q3 = df_window['pm25_nova'].quantile(0.75)\r\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \r\n",
    "    df_window.loc[df_window[\"pm25_nova\"]>=Q3 + 1.5 *IQR,\"pm25_nova\"]=np.nan\r\n",
    "    \r\n",
    "    ref_date_range = pd.date_range(inicio, fin, freq='1Min')\r\n",
    "    ref_date_range = pd.DataFrame(ref_date_range,columns=[\"ref_fechaHora\"])\r\n",
    "    \r\n",
    "    #Hourly mean\r\n",
    "    df_window['pm25_nova_ave']=np.nan\r\n",
    "    df_window['pm25_df_ave']=np.nan\r\n",
    "    #Hourly standar deviation\r\n",
    "    df_window['pm25_nova_std']=np.nan\r\n",
    "    df_window['pm25_df_std']=np.nan\r\n",
    "    #Hourly uncertainty\r\n",
    "    df_window['pm25_unc']=np.nan\r\n",
    "    \r\n",
    "    for ts in df_window['fechaHora']:\r\n",
    "        if ts==ts.ceil('60min'):\r\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]\r\n",
    "            \r\n",
    "        else:\r\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]\r\n",
    "        #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\r\n",
    "        #print(window['pm25_nova'])\r\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_ave']=window['pm25_nova'].mean()\r\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_ave']=window['pm25_df'].mean()\r\n",
    "        \r\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_std']=100*(window['pm25_nova'].std()/window['pm25_nova'].mean())\r\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_std']=100*(window['pm25_df'].std()/window['pm25_df'].mean())\r\n",
    "        \r\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_unc']=\\\r\n",
    "        100*np.sqrt((window.pm25_df-window.pm25_nova).pow(2).mean()/2)/((window.pm25_df+window.pm25_nova).mean()/2)\r\n",
    "        \r\n",
    "    del window\r\n",
    "\r\n",
    "    prec_df=df_window.pm25_df_std.mean()\r\n",
    "    prec_nova=df_window.pm25_nova_std.mean()\r\n",
    "    uncer_df=df_window.pm25_unc.mean()\r\n",
    "    \r\n",
    "        \r\n",
    "    df_preci=df_preci.append({\"codigoSerial\":nube,\"precision_df\":prec_df,\"precision_nova\":prec_nova}, ignore_index = True)\r\n",
    "    print(\"%d. Nube: %d, Overall relative (Precision) Standard Deviation.\"%(contador,nube))\r\n",
    "    \r\n",
    "    df_uncer=df_uncer.append({\"codigoSerial\":nube,\"uncertainty\":uncer_df}, ignore_index = True)\r\n",
    "    print(\"%d. Nube: %d, Overall relative Uncertainty BS, \"%(contador,nube))\r\n",
    "    \r\n",
    "    Closest_Station=Distances.codigoSerial_ES.loc[nube]    \r\n",
    "    if Closest_Station in SS.keys():\r\n",
    "        #Clean values out of range\r\n",
    "        SS[Closest_Station].loc[SS[Closest_Station][\"pm25\"]<=0,\"pm25\"]=np.nan\r\n",
    "        for time in df_window.fechaHora:\r\n",
    "            \r\n",
    "            idx=SS[Closest_Station].Fecha_Hora.searchsorted(time,side=\"right\")\r\n",
    "            #print(idx, SS[Closest_Station].Fecha_Hora.loc[idx], time)\r\n",
    "            v=SS[Closest_Station].loc[idx,\"pm25\"]\r\n",
    "            df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\r\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_df_ave\"]\r\n",
    "            #print(time,\" : \",vm.values[0],\"________\",SS[Closest_Station].Fecha_Hora.loc[idx],\" : \",v)\r\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_df\"]=100*abs(vm-v)/v\r\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_nova_ave\"]\r\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_nova\"]=100*abs(vm-v)/v\r\n",
    "        \r\n",
    "        df_accur=df_accur.append({'codigoSerial':nube, 'dist':Distances.loc[nube,\"Distancia_a_ES\"], 'acc_df':df_window.alpha_df.mean(), 'acc_nova':df_window.alpha_nova.mean()}, ignore_index = True)\r\n",
    "        print(\"%d. Nube: %d, Accuracy,\" %(contador, nube))\r\n",
    "    \r\n",
    "    ref_date_range = pd.date_range(inicio, fin, freq='1Min')\r\n",
    "    ref_date_range = pd.DataFrame(ref_date_range,columns=[\"ref_fechaHora\"])\r\n",
    "\r\n",
    "    \r\n",
    "    #Check for any missing date\r\n",
    "    missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(df_window.fechaHora),\"ref_fechaHora\"]\r\n",
    "\r\n",
    "    #Add missing date rows\r\n",
    "    for missing in missing_dates:\r\n",
    "        df_window=df_window.append({\"codigoSerial\":nube,\"fechaHora\":missing}, ignore_index = True)\r\n",
    "    \r\n",
    "    #Check for any missing date\r\n",
    "    missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(df_window.fechaHora),\"ref_fechaHora\"]\r\n",
    "    \r\n",
    "    #Check for missing data\r\n",
    "    missing_data_df=np.count_nonzero(np.isnan(df_window['pm25_df']))\r\n",
    "    missing_data_nova=np.count_nonzero(np.isnan(df_window['pm25_nova']))\r\n",
    "    comp_df=100*(1-missing_data_df/np.size(df_window.pm25_df))\r\n",
    "    comp_nova=100*(1-missing_data_nova/np.size(df_window.pm25_nova))\r\n",
    "    \r\n",
    "    \r\n",
    "    if comp_df<75:\r\n",
    "        group_df=0\r\n",
    "    elif 75<=comp_df:\r\n",
    "        group_df=1\r\n",
    "    \r\n",
    "    if comp_nova<75:\r\n",
    "        group_nova=0\r\n",
    "    elif  75<=comp_nova:\r\n",
    "        group_nova=1     \r\n",
    "        \r\n",
    "    df_comple=df_comple.append({\"codigoSerial\":nube,\"completeness_df\":comp_df,\"completeness_nova\":comp_nova,\"completeness_group_df\":group_df,\"completeness_group_nova\":group_nova}, ignore_index = True)\r\n",
    "    \r\n",
    "    print(\"%d. Nube: %d, Completeness.\" %(contador,nube))\r\n",
    "    \r\n",
    "    corr_df = df_window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[0].abs()\r\n",
    "    corr_nova = df_window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[1].abs()\r\n",
    "  \r\n",
    "    \r\n",
    "        \r\n",
    "    df_conco=df_conco.append({\"codigoSerial\":nube,\"concordance_df_nova\":corr_df.pm25_nova,\r\n",
    "                              \"concordance_df_siata\":corr_df.v_pm25,\"concordance_df_hum\":corr_df.humedad_relativa,\"concordance_df_temp\":corr_df.temperatura,\r\n",
    "                              \"concordance_nova_siata\":corr_nova.v_pm25,\"concordance_nova_hum\":corr_nova.humedad_relativa,\"concordance_nova_temp\":corr_nova.temperatura}, \r\n",
    "                             ignore_index = True)\r\n",
    "    \r\n",
    "    print(\"%d. Nube: %d, Overall concordance, \"%(contador,nube))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_conco.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_conco.csv\",index=False)\n",
    "df_comple.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_comple.csv\",index=False)\n",
    "df_accur.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_accur.csv\",index=False)\n",
    "df_uncer.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_uncer.csv\",index=False)\n",
    "df_preci.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_preci.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPLETENESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_comple = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_comple.csv\", header=0)\n",
    "print(df_comple.head(2))\n",
    "\n",
    "dic_df={}\n",
    "dic_nova={}\n",
    "grouped_df=df_comple.groupby(df_comple.completeness_group_df)\n",
    "grouped_nova=df_comple.groupby(df_comple.completeness_group_nova)\n",
    "for i in df_comple.completeness_group_df.unique():\n",
    "    dic_df[i]=grouped_df.get_group(i).sort_values(by=['codigoSerial'],ignore_index=True).codigoSerial.tolist()\n",
    "\n",
    "for i in df_comple.completeness_group_nova.unique():\n",
    "    dic_nova[i]=grouped_nova.get_group(i).sort_values(by=['codigoSerial'],ignore_index=True).codigoSerial.tolist()\n",
    "\n",
    "print(\"Completeness < 75%: Group 0, Completeness >= 75%: Group 1\")\n",
    "print(\"DF sensors\",dic_df)\n",
    "print(\"Nova sensors\",dic_nova)\n",
    "print(len(dic_df[1]),len(dic_nova[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "#plt.subplot(121)\n",
    "plt.title(\"Nodes completeness histogram\")\n",
    "plt.xlabel(\"Completeness (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "plt.ylim([0, 50])\n",
    "aux1=df_comple.completeness_df\n",
    "aux2=df_comple.completeness_nova\n",
    "w = [100*np.ones_like(df_comple.index) / len(df_comple.index),100*np.ones_like(df_comple.index) / len(df_comple.index)]\n",
    "plt.hist([aux1, aux2], bins=12, label=['completeness_df', 'completeness_nova'],weights=w,zorder=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Completeness_2.eps', format='eps',bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "#plt.subplot(122)\n",
    "#plt.title(\"nova sensors completeness histogram\")\n",
    "#plt.xlabel(\"Completeness (%)\")\n",
    "#plt.ylabel(\"% of sensors\")\n",
    "#plt.ylim([0, 45])\n",
    "#df_comple.completeness_nova.hist(bins=16,weights = 100*np.ones_like(df_comple.index) / len(df_comple.index),zorder=3)\n",
    "#plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Completeness.eps', format='eps',bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_uncer = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_uncer.csv\", header=0)\n",
    "print(df_uncer.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df_uncer.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_uncer.csv\",index=False)\n",
    "df_uncer = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_uncer.csv\", header=0)\n",
    "print(df_uncer.head(2))\n",
    "print(len(dic_df[1]))\n",
    "for nodes in dic_df[0]:\n",
    "    df_uncer.loc[df_uncer.codigoSerial==nodes,\"uncertainty\"]=np.nan\n",
    "print(np.count_nonzero(~np.isnan(df_uncer.uncertainty)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNCERTAINTY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.title(\"Nodes uncertainty histogram\")\n",
    "plt.xlabel(\"Uncertainty (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "aux=df_uncer.uncertainty.loc[~np.isnan(df_uncer.uncertainty)]\n",
    "df_uncer.uncertainty.hist(bins=16,weights = 100*np.ones_like(aux.index) / len( aux.index),zorder=3)\n",
    "\n",
    "print(np.count_nonzero(~np.isnan(df_uncer.uncertainty)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df_uncer.uncertainty.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Nodes uncertainty histogram\")\n",
    "plt.xlabel(\"Uncertainty (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "aux=df_uncer.uncertainty.loc[~np.isnan(df_uncer.uncertainty)]\n",
    "df_uncer.uncertainty.hist(bins=16,weights = 100*np.ones_like(aux.index) / len( aux.index),zorder=3)\n",
    "\n",
    "print(np.count_nonzero(~np.isnan(df_uncer.uncertainty)))\n",
    "\n",
    "nube=85\n",
    "\n",
    "inicio=\"2020-02-01 00:00:00\"\n",
    "fin=   \"2020-02-01 23:59:00\"\n",
    "CC[nube][\"v_pm25\"] = np.nan\n",
    "CC[nube][\"alpha_df\"] = np.nan\n",
    "CC[nube][\"alpha_nova\"] = np.nan\n",
    "df_window=CC[nube].loc[(CC[nube]['fechaHora'] >= inicio) & (CC[nube]['fechaHora'] <= fin)]\n",
    "#print(df_window)\n",
    "\n",
    "#Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999]\n",
    "df_window=df_window.copy()\n",
    "df_window.loc[df_window[\"pm25_nova\"]>999,\"pm25_nova\"]=np.nan\n",
    "df_window.loc[df_window[\"pm25_nova\"]<0,\"pm25_nova\"]=np.nan\n",
    "df_window.loc[df_window[\"pm25_df\"]>999,\"pm25_df\"]=np.nan\n",
    "df_window.loc[df_window[\"pm25_df\"]<0,\"pm25_df\"]=np.nan\n",
    "\n",
    "#Remove data above the whiskers of the boxplot\n",
    "Q1 = df_window['pm25_df'].quantile(0.25)\n",
    "Q3 = df_window['pm25_df'].quantile(0.75)\n",
    "IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "df_window.loc[df_window[\"pm25_df\"]>=Q3 + 1.5 *IQR,\"pm25_df\"]=np.nan\n",
    "\n",
    "Q1 = df_window['pm25_nova'].quantile(0.25)\n",
    "Q3 = df_window['pm25_nova'].quantile(0.75)\n",
    "IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "df_window.loc[df_window[\"pm25_nova\"]>=Q3 + 1.5 *IQR,\"pm25_nova\"]=np.nan\n",
    "\n",
    "#Moving average filter\n",
    "#df_window['pm25_nova_lpf'] = df_window.pm25_nova.rolling(window=60,min_periods=1).mean()\n",
    "#df_window['pm25_df_lpf'] = df_window.pm25_df.rolling(window=60,min_periods=1).mean()\n",
    "\n",
    "#Hourly mean\n",
    "df_window['pm25_nova_ave']=np.nan\n",
    "df_window['pm25_df_ave']=np.nan\n",
    "\n",
    "#print(\"Diferentes de nan df: \",np.count_nonzero(~np.isnan(df_window['pm25_df'])))\n",
    "#print(\"Diferentes de nan nova: \",np.count_nonzero(~np.isnan(df_window['pm25_nova'])))\n",
    "\n",
    "for ts in df_window['fechaHora']:\n",
    "    if ts==ts.ceil('60min'):\n",
    "        window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]\n",
    "        \n",
    "    else:\n",
    "        window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]\n",
    "    #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\n",
    "    #print(window['pm25_nova'])\n",
    "    \n",
    "    df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_ave']=window['pm25_nova'].mean()\n",
    "    df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_ave']=window['pm25_df'].mean()\n",
    "    #df_window['pm25_nova_ave'][df_window[\"fechaHora\"]==ts]=window['pm25_nova'].mean()\n",
    "    #df_window['pm25_df_ave'][df_window[\"fechaHora\"]==ts]=window['pm25_df'].mean()\n",
    "    \n",
    "    \n",
    "#del window\n",
    "\n",
    "Closest_Station=Distances.codigoSerial_ES.loc[nube]\n",
    "SS[Closest_Station].loc[SS[Closest_Station][\"pm25\"]<=0,\"pm25\"]=np.nan\n",
    "print(\"Nube: %d, Estación: %d, Distancia: %s km\" %(nube, Closest_Station, Distances.Distancia_a_ES.loc[nube]))\n",
    "\n",
    "for time in df_window.fechaHora:\n",
    "    idx=SS[Closest_Station].Fecha_Hora.searchsorted(time,side=\"right\")\n",
    "    #print(idx, SS[Closest_Station].Fecha_Hora.loc[idx], time)\n",
    "    #v=SS[Closest_Station].pm25.loc[idx]\n",
    "    v=SS[Closest_Station].loc[idx,\"pm25\"]\n",
    "    df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\n",
    "    #df_window.v_pm25[(df_window.fechaHora == time)]=v\n",
    "    vm=df_window.loc[(df_window.fechaHora == time),\"pm25_df_ave\"]\n",
    "    #print(time,\" : \",vm.values[0],\"________\",SS[Closest_Station].Fecha_Hora.loc[idx],\" : \",v)\n",
    "    df_window.loc[df_window.fechaHora == time,\"alpha_df\"]=100*abs(vm-v)/v\n",
    "    #df_window.alpha_df[(df_window.fechaHora == time)]=100*abs(vm-v)/v\n",
    "    vm=df_window.loc[(df_window.fechaHora == time),\"pm25_nova_ave\"]\n",
    "    df_window.loc[df_window.fechaHora == time,\"alpha_nova\"]=100*abs(vm-v)/v\n",
    "    #df_window.alpha_nova[(df_window.fechaHora == time)]=100*abs(vm-v)/v\n",
    "\n",
    "print(\"Average error for DF sensor: %s %% and NOVA sensor: %s %%\"%(df_window.alpha_df.mean(),df_window.alpha_nova.mean()))   \n",
    "\n",
    "#Measurement\n",
    "plt.subplot(132)\n",
    "plt.title(\"Concentration of PM2.5 over the time\")\n",
    "plt.ylabel(\"PM2.5 Concentration ($μg/m^3$)\")\n",
    "plt.xlabel(\"Timestamp\")\n",
    "df_window[\"Time\"] = [str(datetime.time(d)) for d in df_window['hora']]\n",
    "plt.xticks(np.arange(0, len(df_window[\"Time\"])+120, 120), rotation=90)\n",
    "plt.plot(df_window[\"Time\"],df_window[\"pm25_df_ave\"],\"r-\",label=\"DF Sensor: %d\"%(nube))\n",
    "plt.plot(df_window[\"Time\"],df_window[\"pm25_nova_ave\"],\"g-\",label=\"Nova Sensor: %d\"%(nube))\n",
    "#plt.plot(df_window[\"fechaHora\"],df_window[\"v_pm25\"],\"b-\",label=\"Siata Station: %d\"%(Closest_Station))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "nube=81\n",
    "\n",
    "inicio=\"2020-02-01 00:00:00\"\n",
    "fin=   \"2020-02-01 23:59:00\"\n",
    "CC[nube][\"v_pm25\"] = np.nan\n",
    "CC[nube][\"alpha_df\"] = np.nan\n",
    "CC[nube][\"alpha_nova\"] = np.nan\n",
    "df_window=CC[nube].loc[(CC[nube]['fechaHora'] >= inicio) & (CC[nube]['fechaHora'] <= fin)]\n",
    "#print(df_window)\n",
    "\n",
    "#Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999]\n",
    "df_window=df_window.copy()\n",
    "df_window.loc[df_window[\"pm25_nova\"]>999,\"pm25_nova\"]=np.nan\n",
    "df_window.loc[df_window[\"pm25_nova\"]<0,\"pm25_nova\"]=np.nan\n",
    "df_window.loc[df_window[\"pm25_df\"]>999,\"pm25_df\"]=np.nan\n",
    "df_window.loc[df_window[\"pm25_df\"]<0,\"pm25_df\"]=np.nan\n",
    "\n",
    "#Remove data above the whiskers of the boxplot\n",
    "Q1 = df_window['pm25_df'].quantile(0.25)\n",
    "Q3 = df_window['pm25_df'].quantile(0.75)\n",
    "IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "df_window.loc[df_window[\"pm25_df\"]>=Q3 + 1.5 *IQR,\"pm25_df\"]=np.nan\n",
    "\n",
    "Q1 = df_window['pm25_nova'].quantile(0.25)\n",
    "Q3 = df_window['pm25_nova'].quantile(0.75)\n",
    "IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "df_window.loc[df_window[\"pm25_nova\"]>=Q3 + 1.5 *IQR,\"pm25_nova\"]=np.nan\n",
    "\n",
    "#Moving average filter\n",
    "#df_window['pm25_nova_lpf'] = df_window.pm25_nova.rolling(window=60,min_periods=1).mean()\n",
    "#df_window['pm25_df_lpf'] = df_window.pm25_df.rolling(window=60,min_periods=1).mean()\n",
    "\n",
    "#Hourly mean\n",
    "df_window['pm25_nova_ave']=np.nan\n",
    "df_window['pm25_df_ave']=np.nan\n",
    "\n",
    "#print(\"Diferentes de nan df: \",np.count_nonzero(~np.isnan(df_window['pm25_df'])))\n",
    "#print(\"Diferentes de nan nova: \",np.count_nonzero(~np.isnan(df_window['pm25_nova'])))\n",
    "\n",
    "for ts in df_window['fechaHora']:\n",
    "    if ts==ts.ceil('60min'):\n",
    "        window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]\n",
    "        \n",
    "    else:\n",
    "        window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]\n",
    "    #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\n",
    "    #print(window['pm25_nova'])\n",
    "    \n",
    "    df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_ave']=window['pm25_nova'].mean()\n",
    "    df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_ave']=window['pm25_df'].mean()\n",
    "    #df_window['pm25_nova_ave'][df_window[\"fechaHora\"]==ts]=window['pm25_nova'].mean()\n",
    "    #df_window['pm25_df_ave'][df_window[\"fechaHora\"]==ts]=window['pm25_df'].mean()\n",
    "    \n",
    "    \n",
    "del window\n",
    "\n",
    "Closest_Station=Distances.codigoSerial_ES.loc[nube]\n",
    "SS[Closest_Station].loc[SS[Closest_Station][\"pm25\"]<=0,\"pm25\"]=np.nan\n",
    "print(\"Nube: %d, Estación: %d, Distancia: %s km\" %(nube, Closest_Station, Distances.Distancia_a_ES.loc[nube]))\n",
    "\n",
    "for time in df_window.fechaHora:\n",
    "    idx=SS[Closest_Station].Fecha_Hora.searchsorted(time,side=\"right\")\n",
    "    #print(idx, SS[Closest_Station].Fecha_Hora.loc[idx], time)\n",
    "    #v=SS[Closest_Station].pm25.loc[idx]\n",
    "    v=SS[Closest_Station].loc[idx,\"pm25\"]\n",
    "    df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\n",
    "    #df_window.v_pm25[(df_window.fechaHora == time)]=v\n",
    "    vm=df_window.loc[(df_window.fechaHora == time),\"pm25_df_ave\"]\n",
    "    #print(time,\" : \",vm.values[0],\"________\",SS[Closest_Station].Fecha_Hora.loc[idx],\" : \",v)\n",
    "    df_window.loc[df_window.fechaHora == time,\"alpha_df\"]=100*abs(vm-v)/v\n",
    "    #df_window.alpha_df[(df_window.fechaHora == time)]=100*abs(vm-v)/v\n",
    "    vm=df_window.loc[(df_window.fechaHora == time),\"pm25_nova_ave\"]\n",
    "    df_window.loc[df_window.fechaHora == time,\"alpha_nova\"]=100*abs(vm-v)/v\n",
    "    #df_window.alpha_nova[(df_window.fechaHora == time)]=100*abs(vm-v)/v\n",
    "\n",
    "print(\"Average error for DF sensor: %s %% and NOVA sensor: %s %%\"%(df_window.alpha_df.mean(),df_window.alpha_nova.mean()))   \n",
    "\n",
    "#Measurement\n",
    "plt.subplot(133)\n",
    "plt.title(\"Concentration of PM2.5 over the time\")\n",
    "plt.ylabel(\"PM2.5 Concentration ($μg/m^3$)\")\n",
    "plt.xlabel(\"Timestamp\")\n",
    "df_window[\"Time\"] = [str(datetime.time(d)) for d in df_window['hora']]\n",
    "plt.xticks(np.arange(0, len(df_window[\"Time\"])+120, 120), rotation=90)\n",
    "plt.plot(df_window[\"Time\"],df_window[\"pm25_df_ave\"],\"r-\",label=\"DF Sensor: %d\"%(nube))\n",
    "plt.plot(df_window[\"Time\"],df_window[\"pm25_nova_ave\"],\"g-\",label=\"Nova Sensor: %d\"%(nube))\n",
    "#plt.plot(df_window[\"fechaHora\"],df_window[\"v_pm25\"],\"b-\",label=\"Siata Station: %d\"%(Closest_Station))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#plt.gcf().subplots_adjust(bottom=0.8)\n",
    "plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Uncertainty.eps', format='eps',bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRECISION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df_preci.head(2))\n",
    "print(\"Number of DF sensors with completeness >75%\",len(dic_df[1]))\n",
    "print(\"Number of Nova sensors with completeness >75%\",len(dic_nova[1]))\n",
    "for nodes in dic_df[0]:\n",
    "    df_preci.loc[df_preci.codigoSerial==nodes,\"precision_df\"]=np.nan\n",
    "for nodes in dic_nova[0]:\n",
    "    df_preci.loc[df_preci.codigoSerial==nodes,\"precision_nova\"]=np.nan\n",
    "print(\"Number of DF sensors with completeness >75%\",np.count_nonzero(~np.isnan(df_preci.precision_df)))\n",
    "print(\"Number of DF sensors with completeness >75%\",np.count_nonzero(~np.isnan(df_preci.precision_nova)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "#plt.subplot(121)\n",
    "plt.title(\"Nodes precision histogram\")\n",
    "plt.xlabel(\"precision (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "plt.ylim([0, 40])\n",
    "df_preci_comp=100-df_preci.precision_df\n",
    "aux1=df_preci_comp.loc[~np.isnan(df_preci_comp)]\n",
    "df_preci_comp=100-df_preci.precision_nova\n",
    "aux2=df_preci_comp.loc[~np.isnan(df_preci_comp)]\n",
    "w = [100*np.ones_like(aux1.index) / len(aux1.index),100*np.ones_like(aux2.index) / len(aux2.index)]\n",
    "#df_preci_comp.hist(bins=16,weights = 100*np.ones_like(aux.index) / len( aux.index),zorder=3)\n",
    "plt.hist([aux1, aux2], bins=12, label=['precision_df', 'precision_acc_nova'],weights=w,zorder=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Precision_2.eps', format='eps',bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(np.count_nonzero(~np.isnan(df_preci_comp)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plt.subplot(122)\n",
    "#plt.title(\"nova sensors precision histogram\")\n",
    "#plt.xlabel(\"precision (%)\")\n",
    "#plt.ylabel(\"% of sensors\")\n",
    "#plt.ylim([0, 35])\n",
    "#df_preci_comp=100-df_preci.precision_nova\n",
    "#aux=df_preci_comp.loc[~np.isnan(df_preci_comp)]\n",
    "#df_preci_comp.hist(bins=16,weights = 100*np.ones_like(aux.index) / len( aux.index),zorder=3)\n",
    "#plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Precision.eps', format='eps',bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(100-df_preci.precision_nova).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df_accur.head(2))\n",
    "print(\"Number of DF sensors with completeness >75%\",len(dic_df[1]))\n",
    "print(\"Number of Nova sensors with completeness >75%\",len(dic_nova[1]))\n",
    "for nodes in dic_df[0]:\n",
    "    df_accur.loc[df_accur.codigoSerial==nodes,\"acc_df\"]=np.nan\n",
    "for nodes in dic_nova[0]:\n",
    "    df_accur.loc[df_accur.codigoSerial==nodes,\"acc_nova\"]=np.nan\n",
    "print(\"Number of DF sensors with completeness >75%\",np.count_nonzero(~np.isnan(df_accur.acc_df)))\n",
    "print(\"Number of Nova sensors with completeness >75%\",np.count_nonzero(~np.isnan(df_accur.acc_nova)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Distance to nearest station histogram\")\n",
    "plt.xlabel(\"Distance (km)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "plt.ylim([0, 35])\n",
    "aux=df_accur.loc[~np.isnan(df_accur['acc_nova'])| ~np.isnan(df_accur['acc_df']),\"dist\"]\n",
    "aux.hist(bins=11,weights=100*np.ones_like(aux.index) / len(aux.index),zorder=3)\n",
    "ndatos_dist=np.size(df_accur.loc[(~np.isnan(df_accur['acc_nova'])| ~np.isnan(df_accur['acc_df'])),\"dist\"])\n",
    "\n",
    "print(\"Number of sensors\",ndatos_dist)\n",
    "ndatos_dist=np.size(df_accur.loc[(~np.isnan(df_accur['acc_nova'])),\"dist\"])\n",
    "print(\"Number of sensors\",ndatos_dist)\n",
    "ndatos_dist=np.size(df_accur.loc[( ~np.isnan(df_accur['acc_df'])),\"dist\"])\n",
    "print(\"Number of sensors\",ndatos_dist)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Node Accuracy histogram\")\n",
    "plt.xlabel(\"Accuracy (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "plt.ylim([0, 35])\n",
    "#plt.xlim([0, 1300])\n",
    "\n",
    "aux1=df_accur.acc_df.loc[~np.isnan(df_accur.acc_df)]\n",
    "aux2=df_accur.acc_nova.loc[~np.isnan(df_accur.acc_nova)]\n",
    "w = [100*np.ones_like(aux1.index) / len(aux1.index),100*np.ones_like(aux2.index) / len(aux2.index)]\n",
    "plt.hist([aux1, aux2], bins=11, label=['acc_df', 'acc_nova'],weights=w,zorder=3)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Accuracy_2.eps', format='eps',bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "\n",
    "#plt.subplot(132)\n",
    "#plt.title(\"df sensors Accuracy histogram\")\n",
    "#plt.xlabel(\"Accuracy (%)\")\n",
    "#plt.ylabel(\"% of sensors\")\n",
    "#plt.ylim([0, 35])\n",
    "##plt.xlim([0, 1300])\n",
    "#aux=df_accur.acc_df.loc[~np.isnan(df_accur.acc_df)]\n",
    "#print(np.size(aux))\n",
    "#df_accur.acc_df.hist(bins=11,weights = 100*np.ones_like(aux.index) / len(aux.index),zorder=3)\n",
    "#\n",
    "#plt.subplot(133)\n",
    "#plt.title(\"nova sensors Accuracy histogram\")\n",
    "#plt.xlabel(\"Accuracy (%)\")\n",
    "#plt.ylabel(\"% of sensors\")\n",
    "#plt.ylim([0, 35])\n",
    "##plt.xlim([0, 1300])\n",
    "#aux=df_accur.acc_nova.loc[~np.isnan(df_accur.acc_nova)]\n",
    "#print(np.size(aux))\n",
    "#df_accur.acc_nova.hist(bins=11,weights = 100*np.ones_like(aux.index) / len(aux.index),zorder=3)\n",
    "#plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Accuracy.eps', format='eps',bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_accur.head(60)\n",
    "df_accur.loc[df_accur.acc_nova>400,\"acc_nova\"]=np.nan\n",
    "df_accur.acc_nova.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.title(\"Accuracy(Relative error) vs Distance\")\n",
    "plt.xlabel(\"Distance (km)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "#df_accur[\"acc_nova_2\"]=df_accur[\"acc_nova\"]\n",
    "#df_accur[\"acc_df_2\"]=df_accur[\"acc_df\"]\n",
    "#df_accur\n",
    "\n",
    "df_accur.loc[df_accur[\"acc_nova\"]==np.inf,\"acc_nova\"]=np.nan\n",
    "df_accur.loc[df_accur[\"acc_df\"]==np.inf,\"acc_df\"]=np.nan\n",
    "\n",
    "plt.plot(df_accur[\"dist\"],df_accur[\"acc_df\"],'ro',label=\"DF Sensor\")\n",
    "\n",
    "for i in df_accur.index:\n",
    "    if not np.isnan(df_accur[\"acc_df\"].loc[i]):\n",
    "        plt.text(df_accur[\"dist\"].loc[i],df_accur[\"acc_df\"].loc[i],i)\n",
    "        \n",
    "plt.plot(df_accur[\"dist\"],df_accur[\"acc_nova\"],'gx',label=\"Nova Sensor\")\n",
    "\n",
    "for i in df_accur.index:\n",
    "    if not np.isnan(df_accur[\"acc_nova\"].loc[i]):\n",
    "        plt.text(df_accur[\"dist\"].loc[i],df_accur[\"acc_nova\"].loc[i],i)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CORRELATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df_conco.head(2))\n",
    "print(\"Number of DF sensors with completeness >75%\",len(dic_df[1]))\n",
    "print(\"Number of Nova sensors with completeness >75%\",len(dic_nova[1]))\n",
    "for nodes in dic_df[0]:\n",
    "    df_conco.loc[df_conco.codigoSerial==nodes,[\"concordance_df_nova\",\"concordance_df_siata\",\"concordance_df_hum\",\"concordance_df_temp\"]]=np.nan\n",
    "    \n",
    "for nodes in dic_nova[0]:\n",
    "    df_conco.loc[df_conco.codigoSerial==nodes,[\"concordance_df_nova\",\"concordance_nova_siata\",\"concordance_nova_hum\",\"concordance_nova_temp\"]]=np.nan\n",
    "print(\"Number of DF sensors with completeness >75%\",np.count_nonzero(~np.isnan(df_conco.concordance_df_siata)))\n",
    "print(\"Number of Nova sensors with completeness >75%\",np.count_nonzero(~np.isnan(df_conco.concordance_nova_siata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.title(\"DF sensor vs Nova sensor\")\n",
    "plt.xlabel(\"Comparability (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "aux=100*df_conco.concordance_df_nova.loc[~np.isnan(df_conco.concordance_df_nova)]\n",
    "print(np.size(aux))\n",
    "aux.hist(bins=11,weights = 100*np.ones_like(aux.index) / len(aux.index),zorder=3)\n",
    "plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Concordance_DF_NOVA.eps', format='eps',bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Node sensors VS Siata station\")\n",
    "plt.xlabel(\"Comparability (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "aux1=100*df_conco.concordance_df_siata.loc[~np.isnan(df_conco.concordance_df_siata)]\n",
    "aux2=100*df_conco.concordance_nova_siata.loc[~np.isnan(df_conco.concordance_nova_siata)]\n",
    "w = [100*np.ones_like(aux1.index) / len(aux1.index),100*np.ones_like(aux2.index) / len(aux2.index)]\n",
    "plt.hist([aux1, aux2], bins=11, label=['concordance_df_siata', 'concordance_nova_siata'],weights=w,zorder=3)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(\"PM2.5 vs Humidity\")\n",
    "plt.xlabel(\"Comparability (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "plt.ylim([0, 30])\n",
    "aux1=100*df_conco.concordance_df_hum.loc[~np.isnan(df_conco.concordance_df_hum)]\n",
    "aux2=100*df_conco.concordance_nova_hum.loc[~np.isnan(df_conco.concordance_nova_hum)]\n",
    "w = [100*np.ones_like(aux1.index) / len(aux1.index),100*np.ones_like(aux2.index) / len(aux2.index)]\n",
    "plt.hist([aux1, aux2], bins=11, label=['concordance_df_hum', 'concordance_nova_hum'],weights=w,zorder=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title(\"PM2.5 vs Temperature\")\n",
    "plt.xlabel(\"Comparability (%)\")\n",
    "plt.ylabel(\"% of sensors\")\n",
    "plt.ylim([0, 30])\n",
    "aux1=100*df_conco.concordance_df_temp.loc[~np.isnan(df_conco.concordance_df_temp)]\n",
    "aux2=100*df_conco.concordance_nova_temp.loc[~np.isnan(df_conco.concordance_nova_temp)]\n",
    "w = [100*np.ones_like(aux1.index) / len(aux1.index),100*np.ones_like(aux2.index) / len(aux2.index)]\n",
    "plt.hist([aux1, aux2], bins=11, label=['concordance_df_temp', 'concordance_nova_temp'],weights=w,zorder=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.savefig('C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/Concordance_OtherVariables.eps', format='eps',bbox_inches = \"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with parallelitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def parse_file(filename):\n",
    "    ...\n",
    "\n",
    "def main():\n",
    "    pool = mp.Pool(processes=6)\n",
    "    pool.map(parse_file, ['my_dir/' + filename for filename in os.listdir(\"my_dir\")])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Setup file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights\n",
    "mu                =0.9\n",
    "Accuracy          =0.20\n",
    "Precision         =0.07\n",
    "Confidence        =0.16\n",
    "Completeness      =0.10\n",
    "Timeliness        =0.12\n",
    "Data_Volume       =0.16\n",
    "Data_Redundancy   =0.02\n",
    "Concordance       =0.16\n",
    "\n",
    "Utility           =0.12\n",
    "Accessibility     =0.16\n",
    "Interpretability  =0.28\n",
    "Reputation        =0.12\n",
    "Artificiality     =0.20\n",
    "Access_Security   =0.12\n",
    "\n",
    "\n",
    "#Period\n",
    "inicio=\"2020-02-03 00:00:00\"\n",
    "fin   =\"2020-02-03 23:59:00\"\n",
    "start_time =\"2020-02-10 00:00:00\"\n",
    "end_time   =\"2020-02-10 05:59:00\"\n",
    "\n",
    "#Variable Inicialization\n",
    "nube=0\n",
    "node=220\n",
    "contador=0\n",
    "counter=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow the user to select a file using the function: get_path('*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_path(wildcard, title):\n",
    "    app = wx.App(None)\n",
    "    style = wx.FD_OPEN | wx.FD_FILE_MUST_EXIST\n",
    "    dialog = wx.FileDialog(None, title, wildcard=wildcard, style=style)\n",
    "    if dialog.ShowModal() == wx.ID_OK:\n",
    "        path = dialog.GetPath()\n",
    "    else:\n",
    "        path = None\n",
    "    dialog.Destroy()\n",
    "    return path\n",
    "#filepath=get_path('*.csv',\"This is the title\")\n",
    "#print (filepath,type(filepath))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path for Citizen Science nodes data:  C:\\Users\\julio\\Documents\\UDEA\\Maestría\\DQ in IOT\\Datasets\\SIATA_CS\\SplitDatosCC\\Samples\\February.csv\n",
      "Source path for Siata Stations data:  C:\\Users\\julio\\Documents\\UDEA\\Maestría\\DQ in IOT\\Datasets\\SIATA Stations\\PM\\SS_PM.csv\n",
      "Citizen Scientist:  [1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 259, 261, 262, 265, 266, 267]\n",
      "Siata Stations:  [11, 12, 25, 28, 31, 37, 38, 44, 46, 48, 69, 6, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 94]\n"
     ]
    }
   ],
   "source": [
    "#Read Data from February\n",
    "header_CC=[\"codigoSerial\", \"fecha\", \"hora\", \"fechaHora\", \"temperatura\", \"humedad_relativa\", \"pm1_df\", \"pm10_df\", \"pm25_df\", \"pm1_nova\", \"pm10_nova\", \"pm25_nova\", \"calidad_temperatura\", \"calidad_humedad_relativa\", \"calidad_pm1_df\", \"calidad_pm10_df\", \"calidad_pm25_df\", \"calidad_pm1_nova\", \"calidad_pm10_nova\", \"calidad_pm25_nova\"]\n",
    "datatypes_CC={\"codigoSerial\":np.uint16, \"temperatura\":np.float16, \"humedad_relativa\":np.float16, \"pm1_df\":np.float32, \"pm10_df\":np.float32, \"pm25_df\":np.float32, \"pm1_nova\":np.float32, \"pm10_nova\":np.float32, \"pm25_nova\":np.float32}\n",
    "\n",
    "path_for_CC_data=get_path('*.csv',\"Select Citizen Scientist *.csv file\")\n",
    "try:\n",
    "    df_CC = pd.read_csv(path_for_CC_data, header=None, names=header_CC, usecols=header_CC , dtype=datatypes_CC,parse_dates=[\"fecha\",\"hora\",\"fechaHora\"])\n",
    "    print(\"Source path for Citizen Science nodes data: \",path_for_CC_data)\n",
    "\n",
    "    #Data includes January, February and March\n",
    "    header_SS=[\"Fecha_Hora\",\"codigoSerial\",\"pm25\",\"calidad_pm25\",\"pm10\",\"calidad_pm10\"]\n",
    "    datatypes_SS={\"codigoSerial\":np.uint16,\"pm25\":np.float32,\"pm10\":np.float32}\n",
    "    path_for_SS_data=get_path('*.csv',\"Select SIATA Stations *.csv file\")\n",
    "    df_SS = pd.read_csv(path_for_SS_data, header=None,names=header_SS, usecols=header_SS , dtype=datatypes_SS,parse_dates=[\"Fecha_Hora\"])\n",
    "    print(\"Source path for Siata Stations data: \",path_for_SS_data)\n",
    "    \n",
    "    grouped=df_CC.groupby(df_CC.codigoSerial)\n",
    "    CC={}\n",
    "    print(\"Citizen Scientist: \", sorted(list(df_CC.codigoSerial.unique())))\n",
    "    for i in df_CC.codigoSerial.unique():\n",
    "        CC[i] = grouped.get_group(i).sort_values(by=['fechaHora'],ignore_index=True)\n",
    "    del df_CC\n",
    "    \n",
    "    grouped=df_SS.groupby(df_SS.codigoSerial)\n",
    "    SS={}\n",
    "    print(\"Siata Stations: \", list(df_SS.codigoSerial.unique()))\n",
    "    for j in df_SS.codigoSerial.unique():\n",
    "        SS[j] = grouped.get_group(j).sort_values(by=['Fecha_Hora'],ignore_index=True)\n",
    "    del df_SS\n",
    "    del grouped\n",
    "except:\n",
    "    print(\"An exception occurred, it is possible that wrong files were chosen, please run again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA CLEANING\n",
    "def clean_data(node, CC, start_time, end_time):\n",
    "    \n",
    "    #define a time windows of the data to be analyzed/cleaned\n",
    "    df_window=CC[node][(CC[node]['fechaHora'] >= start_time) & (CC[node]['fechaHora'] <= end_time)]\n",
    "    #Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999]\n",
    "    df_window=df_window.copy()\n",
    "    df_window.loc[df_window[\"pm25_nova\"]>999,\"pm25_nova\"]=np.nan\n",
    "    df_window.loc[df_window[\"pm25_nova\"]<0,\"pm25_nova\"]=np.nan\n",
    "    df_window.loc[df_window[\"pm25_df\"]>999,\"pm25_df\"]=np.nan\n",
    "    df_window.loc[df_window[\"pm25_df\"]<0,\"pm25_df\"]=np.nan\n",
    "    \n",
    "    #Remove data above the whiskers of the boxplot: i.e. anomaly data\n",
    "    Q1 = df_window['pm25_df'].quantile(0.25)\n",
    "    Q3 = df_window['pm25_df'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_window.loc[df_window[\"pm25_df\"]>=Q3 + 1.5 *IQR,\"pm25_df\"]=np.nan\n",
    "    \n",
    "    Q1 = df_window['pm25_nova'].quantile(0.25)\n",
    "    Q3 = df_window['pm25_nova'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_window.loc[df_window[\"pm25_nova\"]>=Q3 + 1.5 *IQR,\"pm25_nova\"]=np.nan\n",
    "    \n",
    "    return df_window\n",
    "\n",
    "#PRECISION\n",
    "\n",
    "def precision(node, df_window):\n",
    "    #Hourly standard deviation\n",
    "    df_window['pm25_nova_pre']=np.nan\n",
    "    df_window['pm25_df_pre']=np.nan\n",
    "    for ts in df_window['fechaHora']:\n",
    "        if ts==ts.ceil('60min'):\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]# Next ONE HOUR WINDOW\n",
    "            \n",
    "        else:\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]#Current ONE HOUR window\n",
    "        #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\n",
    "        #print(window['pm25_nova'])        \n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_pre']=1-(window['pm25_nova'].std()/window['pm25_nova'].mean())# 1-Coefficient of Variation (std/mean)\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_pre']=1-(window['pm25_df'].std()/window['pm25_df'].mean())# 1-Coefficient of Variation (std/mean)\n",
    "        \n",
    "    #del window\n",
    "    prec_df=df_window.pm25_df_pre.mean()#Average precision of the whole node for DF\n",
    "    prec_nova=df_window.pm25_nova_pre.mean()  #Average precision of the whole node for NOVA  \n",
    "    preci_dict={\"codigoSerial\":node,\"precision_df\":prec_df,\"precision_nova\":prec_nova}\n",
    "    #print(\"%d. Nube: %d, Overall relative (Precision) Standard Deviation.\"%(contador,nube))\n",
    "    del prec_df\n",
    "    del prec_nova\n",
    "    return preci_dict\n",
    "\n",
    "\n",
    "#df_preci=df_preci.append(precision(node, df_window), ignore_index = True)\n",
    "#df_preci.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_preci.csv\",index=False)\n",
    "\n",
    "#UNCERTAINTY\n",
    "def uncertainty(node, df_window):\n",
    "    #Hourly standard deviation\n",
    "\n",
    "    df_window['pm25_unc']=np.nan\n",
    "    for ts in df_window['fechaHora']:\n",
    "        if ts==ts.ceil('60min'):\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]# Next ONE HOUR WINDOW\n",
    "            \n",
    "        else:\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]#Current ONE HOUR window\n",
    "        #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\n",
    "        #print(window['pm25_nova'])        \n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_unc']=\\\n",
    "        1-np.sqrt((window.pm25_df-window.pm25_nova).pow(2).mean()/2)/((window.pm25_df+window.pm25_nova).mean()/2)\n",
    "        \n",
    "    #del window\n",
    "    \n",
    "    uncer_df=df_window.pm25_unc.mean()#Average uncertainty of the whole node\n",
    "    uncer_dict={\"codigoSerial\":node,\"uncertainty\":uncer_df}\n",
    "    del uncer_df\n",
    "    \n",
    "    return uncer_dict\n",
    "\n",
    "\n",
    "#df_preci=df_preci.append(precision(node, df_window), ignore_index = True)\n",
    "#df_preci.to_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/DQ_February/df_preci.csv\",index=False)\n",
    "\n",
    "#ACCURACY\n",
    "\n",
    "def accuracy(node, df_window):\n",
    "    #Hourly mean\n",
    "    df_window['pm25_nova_ave']=np.nan\n",
    "    df_window['pm25_df_ave']=np.nan    \n",
    "    df_window['alpha_df']=np.nan   \n",
    "    df_window['alpha_nova']=np.nan \n",
    "    \n",
    "    \n",
    "    for ts in df_window['fechaHora']:\n",
    "        if ts==ts.ceil('60min'):\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]# Next ONE HOUR WINDOW\n",
    "            \n",
    "        else:\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]#Current ONE HOUR window\n",
    "        #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\n",
    "        #print(window['pm25_nova'])        \n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_ave']=window['pm25_nova'].mean()\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_ave']=window['pm25_df'].mean()\n",
    "\n",
    "    Closest_Station=Distances.codigoSerial_ES.loc[node]    \n",
    "    Closest_Station2=Distances.codigoSerial_ES2.loc[node] \n",
    "    if Closest_Station in SS.keys():\n",
    "        #Clean values out of range\n",
    "        SS[Closest_Station].loc[SS[Closest_Station][\"pm25\"]<=0,\"pm25\"]=np.nan\n",
    "        for time in df_window.fechaHora:\n",
    "            \n",
    "            idx=SS[Closest_Station].Fecha_Hora.searchsorted(time,side=\"right\")\n",
    "            #print(idx, SS[Closest_Station].Fecha_Hora.loc[idx], time)\n",
    "            v=SS[Closest_Station].loc[idx,\"pm25\"]\n",
    "            df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_df_ave\"]\n",
    "            #print(time,\" : \",vm.values[0],\"________\",SS[Closest_Station].Fecha_Hora.loc[idx],\" : \",v)\n",
    "            #print(v,\"<->\", vm.values[0])\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_df\"]=  max(0,1-abs(vm.values[0]-v)/v)\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_nova_ave\"]\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_nova\"]=max(0,1-abs(vm.values[0]-v)/v)\n",
    "        \n",
    "        accu_dict={'codigoSerial':node, 'dist':Distances.loc[node,\"Distancia_a_ES\"], 'acc_df':df_window.alpha_df.mean(), 'acc_nova':df_window.alpha_nova.mean()}\n",
    "    \n",
    "    elif Closest_Station2 in SS.keys():\n",
    "        #Clean values out of range\n",
    "        SS[Closest_Station2].loc[SS[Closest_Station2][\"pm25\"]<=0,\"pm25\"]=np.nan\n",
    "        for time in df_window.fechaHora:\n",
    "            \n",
    "            idx=SS[Closest_Station2].Fecha_Hora.searchsorted(time,side=\"right\")\n",
    "            #print(idx, SS[Closest_Station].Fecha_Hora.loc[idx], time)\n",
    "            v=SS[Closest_Station2].loc[idx,\"pm25\"]\n",
    "            df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_df_ave\"]\n",
    "            #print(time,\" : \",vm.values[0],\"________\",SS[Closest_Station].Fecha_Hora.loc[idx],\" : \",v)\n",
    "            #print(v, vm)\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_df\"]=  max(0,1-abs(vm.values[0]-v)/v)\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_nova_ave\"]\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_nova\"]=max(0,1-abs(vm.values[0]-v)/v)\n",
    "        \n",
    "        accu_dict={'codigoSerial':node, 'dist':Distances.loc[node,\"Distancia_a_ES\"], 'acc_df':df_window.alpha_df.mean(), 'acc_nova':df_window.alpha_nova.mean()}\n",
    "        \n",
    "    else:\n",
    "        accu_dict={'codigoSerial':node, 'dist':np.nan, 'acc_df':np.nan, 'acc_nova':np.nan}\n",
    "    return accu_dict\n",
    "\n",
    "#CONCORDANCE\n",
    "def concordance(node, df_window):\n",
    "    df_window['v_pm25']=np.nan \n",
    "    \n",
    "    Closest_Station=Distances.codigoSerial_ES.loc[node]    \n",
    "    if Closest_Station in SS.keys():\n",
    "        #Clean values out of range\n",
    "        SS[Closest_Station].loc[SS[Closest_Station][\"pm25\"]<=0,\"pm25\"]=np.nan\n",
    "        for time in df_window.fechaHora:\n",
    "            \n",
    "            idx=SS[Closest_Station].Fecha_Hora.searchsorted(time,side=\"right\")\n",
    "            #print(idx, SS[Closest_Station].Fecha_Hora.loc[idx], time)\n",
    "            v=SS[Closest_Station].loc[idx,\"pm25\"]\n",
    "            df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\n",
    "   \n",
    "    #Comparability / Concordance\n",
    "    corr_df = df_window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[0].abs()\n",
    "    corr_nova = df_window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[1].abs()\n",
    "    conco_dict={\"codigoSerial\":node,\"concordance_df_nova\":corr_df.pm25_nova,\n",
    "                \"concordance_df_siata\":corr_df.v_pm25,\"concordance_df_hum\":corr_df.humedad_relativa,\"concordance_df_temp\":corr_df.temperatura,\n",
    "                \"concordance_nova_siata\":corr_nova.v_pm25,\"concordance_nova_hum\":corr_nova.humedad_relativa,\"concordance_nova_temp\":corr_nova.temperatura}\n",
    "    return conco_dict\n",
    "\n",
    "#COMPLETENESS\n",
    "\n",
    "def completeness(node, df_window):\n",
    "    #ref_date_range = pd.date_range(inicio, fin, freq='1Min')\n",
    "    #ref_date_range = pd.DataFrame(ref_date_range,columns=[\"ref_fechaHora\"])\n",
    "    ref_date_range = pd.DataFrame(pd.date_range(start_time, end_time, freq='1Min'),columns=[\"ref_fechaHora\"])\n",
    "  \n",
    "    #Check for any missing date\n",
    "    missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(df_window.fechaHora),\"ref_fechaHora\"]\n",
    "\n",
    "    #Add missing date rows\n",
    "    for missing in missing_dates:\n",
    "        df_window=df_window.append({\"codigoSerial\":node,\"fechaHora\":missing}, ignore_index = True)\n",
    "    \n",
    "    #Check for any missing date\n",
    "    #missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(df_window.fechaHora),\"ref_fechaHora\"]\n",
    "    \n",
    "    #Check for missing data\n",
    "    missing_data_df=np.count_nonzero(np.isnan(df_window['pm25_df']))\n",
    "    missing_data_nova=np.count_nonzero(np.isnan(df_window['pm25_nova']))\n",
    "    comp_df=(1-missing_data_df/np.size(df_window.pm25_df))\n",
    "    comp_nova=(1-missing_data_nova/np.size(df_window.pm25_nova))\n",
    "    \n",
    "    comp_dict={\"codigoSerial\":node,\"completeness_df\":comp_df,\"completeness_nova\":comp_nova}\n",
    "    return comp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPLYING THE FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Nube: 220, Overall relative (Precision) Standard Deviation.\n",
      "1. Nube: 220, Overall relative Uncertainty BS, \n",
      "1. Nube: 220, Accuracy,\n",
      "1. Nube: 220, Completeness.\n",
      "1. Nube: 220, Overall concordance, \n",
      "2. Nube: 1, Overall relative (Precision) Standard Deviation.\n",
      "2. Nube: 1, Overall relative Uncertainty BS, \n",
      "2. Nube: 1, Accuracy,\n",
      "2. Nube: 1, Completeness.\n",
      "2. Nube: 1, Overall concordance, \n",
      "3. Nube: 192, Overall relative (Precision) Standard Deviation.\n",
      "3. Nube: 192, Overall relative Uncertainty BS, \n",
      "3. Nube: 192, Accuracy,\n",
      "3. Nube: 192, Completeness.\n",
      "3. Nube: 192, Overall concordance, \n",
      "4. Nube: 261, Overall relative (Precision) Standard Deviation.\n",
      "4. Nube: 261, Overall relative Uncertainty BS, \n",
      "4. Nube: 261, Accuracy,\n",
      "4. Nube: 261, Completeness.\n",
      "4. Nube: 261, Overall concordance, \n",
      "5. Nube: 262, Overall relative (Precision) Standard Deviation.\n",
      "5. Nube: 262, Overall relative Uncertainty BS, \n",
      "5. Nube: 262, Accuracy,\n",
      "5. Nube: 262, Completeness.\n",
      "5. Nube: 262, Overall concordance, \n",
      "6. Nube: 104, Overall relative (Precision) Standard Deviation.\n",
      "6. Nube: 104, Overall relative Uncertainty BS, \n",
      "6. Nube: 104, Accuracy,\n",
      "6. Nube: 104, Completeness.\n",
      "6. Nube: 104, Overall concordance, \n",
      "7. Nube: 143, Overall relative (Precision) Standard Deviation.\n",
      "7. Nube: 143, Overall relative Uncertainty BS, \n",
      "7. Nube: 143, Accuracy,\n",
      "7. Nube: 143, Completeness.\n",
      "7. Nube: 143, Overall concordance, \n",
      "8. Nube: 80, Overall relative (Precision) Standard Deviation.\n",
      "8. Nube: 80, Overall relative Uncertainty BS, \n",
      "8. Nube: 80, Accuracy,\n",
      "8. Nube: 80, Completeness.\n",
      "8. Nube: 80, Overall concordance, \n",
      "9. Nube: 169, Overall relative (Precision) Standard Deviation.\n",
      "9. Nube: 169, Overall relative Uncertainty BS, \n",
      "9. Nube: 169, Accuracy,\n",
      "9. Nube: 169, Completeness.\n",
      "9. Nube: 169, Overall concordance, \n",
      "10. Nube: 138, Overall relative (Precision) Standard Deviation.\n",
      "10. Nube: 138, Overall relative Uncertainty BS, \n",
      "10. Nube: 138, Accuracy,\n",
      "10. Nube: 138, Completeness.\n",
      "10. Nube: 138, Overall concordance, \n",
      "11. Nube: 140, Overall relative (Precision) Standard Deviation.\n",
      "11. Nube: 140, Overall relative Uncertainty BS, \n",
      "11. Nube: 140, Accuracy,\n",
      "11. Nube: 140, Completeness.\n",
      "11. Nube: 140, Overall concordance, \n",
      "12. Nube: 160, Overall relative (Precision) Standard Deviation.\n",
      "12. Nube: 160, Overall relative Uncertainty BS, \n",
      "12. Nube: 160, Accuracy,\n",
      "12. Nube: 160, Completeness.\n",
      "12. Nube: 160, Overall concordance, \n",
      "13. Nube: 142, Overall relative (Precision) Standard Deviation.\n",
      "13. Nube: 142, Overall relative Uncertainty BS, \n",
      "13. Nube: 142, Accuracy,\n",
      "13. Nube: 142, Completeness.\n",
      "13. Nube: 142, Overall concordance, \n",
      "14. Nube: 129, Overall relative (Precision) Standard Deviation.\n",
      "14. Nube: 129, Overall relative Uncertainty BS, \n",
      "14. Nube: 129, Accuracy,\n",
      "14. Nube: 129, Completeness.\n",
      "14. Nube: 129, Overall concordance, \n",
      "15. Nube: 34, Overall relative (Precision) Standard Deviation.\n",
      "15. Nube: 34, Overall relative Uncertainty BS, \n",
      "15. Nube: 34, Accuracy,\n",
      "15. Nube: 34, Completeness.\n",
      "15. Nube: 34, Overall concordance, \n",
      "16. Nube: 175, Overall relative (Precision) Standard Deviation.\n",
      "16. Nube: 175, Overall relative Uncertainty BS, \n",
      "16. Nube: 175, Accuracy,\n",
      "16. Nube: 175, Completeness.\n",
      "16. Nube: 175, Overall concordance, \n",
      "17. Nube: 89, Overall relative (Precision) Standard Deviation.\n",
      "17. Nube: 89, Overall relative Uncertainty BS, \n",
      "17. Nube: 89, Accuracy,\n",
      "17. Nube: 89, Completeness.\n",
      "17. Nube: 89, Overall concordance, \n",
      "18. Nube: 49, Overall relative (Precision) Standard Deviation.\n",
      "18. Nube: 49, Overall relative Uncertainty BS, \n",
      "18. Nube: 49, Accuracy,\n",
      "18. Nube: 49, Completeness.\n",
      "18. Nube: 49, Overall concordance, \n",
      "19. Nube: 87, Overall relative (Precision) Standard Deviation.\n",
      "19. Nube: 87, Overall relative Uncertainty BS, \n",
      "19. Nube: 87, Accuracy,\n",
      "19. Nube: 87, Completeness.\n",
      "19. Nube: 87, Overall concordance, \n",
      "20. Nube: 240, Overall relative (Precision) Standard Deviation.\n",
      "20. Nube: 240, Overall relative Uncertainty BS, \n",
      "20. Nube: 240, Accuracy,\n",
      "20. Nube: 240, Completeness.\n",
      "20. Nube: 240, Overall concordance, \n",
      "21. Nube: 163, Overall relative (Precision) Standard Deviation.\n",
      "21. Nube: 163, Overall relative Uncertainty BS, \n",
      "21. Nube: 163, Accuracy,\n",
      "21. Nube: 163, Completeness.\n",
      "21. Nube: 163, Overall concordance, \n",
      "22. Nube: 18, Overall relative (Precision) Standard Deviation.\n",
      "22. Nube: 18, Overall relative Uncertainty BS, \n",
      "22. Nube: 18, Accuracy,\n",
      "22. Nube: 18, Completeness.\n",
      "22. Nube: 18, Overall concordance, \n",
      "23. Nube: 74, Overall relative (Precision) Standard Deviation.\n",
      "23. Nube: 74, Overall relative Uncertainty BS, \n",
      "23. Nube: 74, Accuracy,\n",
      "23. Nube: 74, Completeness.\n",
      "23. Nube: 74, Overall concordance, \n",
      "24. Nube: 37, Overall relative (Precision) Standard Deviation.\n",
      "24. Nube: 37, Overall relative Uncertainty BS, \n",
      "24. Nube: 37, Accuracy,\n",
      "24. Nube: 37, Completeness.\n",
      "24. Nube: 37, Overall concordance, \n",
      "25. Nube: 179, Overall relative (Precision) Standard Deviation.\n",
      "25. Nube: 179, Overall relative Uncertainty BS, \n",
      "25. Nube: 179, Accuracy,\n",
      "25. Nube: 179, Completeness.\n",
      "25. Nube: 179, Overall concordance, \n",
      "26. Nube: 234, Overall relative (Precision) Standard Deviation.\n",
      "26. Nube: 234, Overall relative Uncertainty BS, \n",
      "26. Nube: 234, Accuracy,\n",
      "26. Nube: 234, Completeness.\n",
      "26. Nube: 234, Overall concordance, \n",
      "27. Nube: 123, Overall relative (Precision) Standard Deviation.\n",
      "27. Nube: 123, Overall relative Uncertainty BS, \n",
      "27. Nube: 123, Accuracy,\n",
      "27. Nube: 123, Completeness.\n",
      "27. Nube: 123, Overall concordance, \n",
      "28. Nube: 204, Overall relative (Precision) Standard Deviation.\n",
      "28. Nube: 204, Overall relative Uncertainty BS, \n",
      "28. Nube: 204, Accuracy,\n",
      "28. Nube: 204, Completeness.\n",
      "28. Nube: 204, Overall concordance, \n",
      "29. Nube: 50, Overall relative (Precision) Standard Deviation.\n",
      "29. Nube: 50, Overall relative Uncertainty BS, \n",
      "29. Nube: 50, Accuracy,\n",
      "29. Nube: 50, Completeness.\n",
      "29. Nube: 50, Overall concordance, \n",
      "30. Nube: 141, Overall relative (Precision) Standard Deviation.\n",
      "30. Nube: 141, Overall relative Uncertainty BS, \n",
      "30. Nube: 141, Accuracy,\n",
      "30. Nube: 141, Completeness.\n",
      "30. Nube: 141, Overall concordance, \n",
      "31. Nube: 11, Overall relative (Precision) Standard Deviation.\n",
      "31. Nube: 11, Overall relative Uncertainty BS, \n",
      "31. Nube: 11, Accuracy,\n",
      "31. Nube: 11, Completeness.\n",
      "31. Nube: 11, Overall concordance, \n",
      "32. Nube: 19, Overall relative (Precision) Standard Deviation.\n",
      "32. Nube: 19, Overall relative Uncertainty BS, \n",
      "32. Nube: 19, Accuracy,\n",
      "32. Nube: 19, Completeness.\n",
      "32. Nube: 19, Overall concordance, \n",
      "33. Nube: 188, Overall relative (Precision) Standard Deviation.\n",
      "33. Nube: 188, Overall relative Uncertainty BS, \n",
      "33. Nube: 188, Accuracy,\n",
      "33. Nube: 188, Completeness.\n",
      "33. Nube: 188, Overall concordance, \n",
      "34. Nube: 82, Overall relative (Precision) Standard Deviation.\n",
      "34. Nube: 82, Overall relative Uncertainty BS, \n",
      "34. Nube: 82, Accuracy,\n",
      "34. Nube: 82, Completeness.\n",
      "34. Nube: 82, Overall concordance, \n",
      "35. Nube: 167, Overall relative (Precision) Standard Deviation.\n",
      "35. Nube: 167, Overall relative Uncertainty BS, \n",
      "35. Nube: 167, Accuracy,\n",
      "35. Nube: 167, Completeness.\n",
      "35. Nube: 167, Overall concordance, \n",
      "36. Nube: 40, Overall relative (Precision) Standard Deviation.\n",
      "36. Nube: 40, Overall relative Uncertainty BS, \n",
      "36. Nube: 40, Accuracy,\n",
      "36. Nube: 40, Completeness.\n",
      "36. Nube: 40, Overall concordance, \n",
      "37. Nube: 47, Overall relative (Precision) Standard Deviation.\n",
      "37. Nube: 47, Overall relative Uncertainty BS, \n",
      "37. Nube: 47, Accuracy,\n",
      "37. Nube: 47, Completeness.\n",
      "37. Nube: 47, Overall concordance, \n",
      "38. Nube: 197, Overall relative (Precision) Standard Deviation.\n",
      "38. Nube: 197, Overall relative Uncertainty BS, \n",
      "38. Nube: 197, Accuracy,\n",
      "38. Nube: 197, Completeness.\n",
      "38. Nube: 197, Overall concordance, \n",
      "39. Nube: 267, Overall relative (Precision) Standard Deviation.\n",
      "39. Nube: 267, Overall relative Uncertainty BS, \n",
      "39. Nube: 267, Accuracy,\n",
      "39. Nube: 267, Completeness.\n",
      "39. Nube: 267, Overall concordance, \n",
      "40. Nube: 217, Overall relative (Precision) Standard Deviation.\n",
      "40. Nube: 217, Overall relative Uncertainty BS, \n",
      "40. Nube: 217, Accuracy,\n",
      "40. Nube: 217, Completeness.\n",
      "40. Nube: 217, Overall concordance, \n",
      "41. Nube: 170, Overall relative (Precision) Standard Deviation.\n",
      "41. Nube: 170, Overall relative Uncertainty BS, \n",
      "41. Nube: 170, Accuracy,\n",
      "41. Nube: 170, Completeness.\n",
      "41. Nube: 170, Overall concordance, \n",
      "42. Nube: 16, Overall relative (Precision) Standard Deviation.\n",
      "42. Nube: 16, Overall relative Uncertainty BS, \n",
      "42. Nube: 16, Accuracy,\n",
      "42. Nube: 16, Completeness.\n",
      "42. Nube: 16, Overall concordance, \n",
      "43. Nube: 94, Overall relative (Precision) Standard Deviation.\n",
      "43. Nube: 94, Overall relative Uncertainty BS, \n",
      "43. Nube: 94, Accuracy,\n",
      "43. Nube: 94, Completeness.\n",
      "43. Nube: 94, Overall concordance, \n",
      "44. Nube: 26, Overall relative (Precision) Standard Deviation.\n",
      "44. Nube: 26, Overall relative Uncertainty BS, \n",
      "44. Nube: 26, Accuracy,\n",
      "44. Nube: 26, Completeness.\n",
      "44. Nube: 26, Overall concordance, \n",
      "45. Nube: 202, Overall relative (Precision) Standard Deviation.\n",
      "45. Nube: 202, Overall relative Uncertainty BS, \n",
      "45. Nube: 202, Accuracy,\n",
      "45. Nube: 202, Completeness.\n",
      "45. Nube: 202, Overall concordance, \n",
      "46. Nube: 31, Overall relative (Precision) Standard Deviation.\n",
      "46. Nube: 31, Overall relative Uncertainty BS, \n",
      "46. Nube: 31, Accuracy,\n",
      "46. Nube: 31, Completeness.\n",
      "46. Nube: 31, Overall concordance, \n",
      "47. Nube: 209, Overall relative (Precision) Standard Deviation.\n",
      "47. Nube: 209, Overall relative Uncertainty BS, \n",
      "47. Nube: 209, Accuracy,\n",
      "47. Nube: 209, Completeness.\n",
      "47. Nube: 209, Overall concordance, \n",
      "48. Nube: 246, Overall relative (Precision) Standard Deviation.\n",
      "48. Nube: 246, Overall relative Uncertainty BS, \n",
      "48. Nube: 246, Accuracy,\n",
      "48. Nube: 246, Completeness.\n",
      "48. Nube: 246, Overall concordance, \n",
      "49. Nube: 86, Overall relative (Precision) Standard Deviation.\n",
      "49. Nube: 86, Overall relative Uncertainty BS, \n",
      "49. Nube: 86, Accuracy,\n",
      "49. Nube: 86, Completeness.\n",
      "49. Nube: 86, Overall concordance, \n",
      "50. Nube: 110, Overall relative (Precision) Standard Deviation.\n",
      "50. Nube: 110, Overall relative Uncertainty BS, \n",
      "50. Nube: 110, Accuracy,\n",
      "50. Nube: 110, Completeness.\n",
      "50. Nube: 110, Overall concordance, \n",
      "51. Nube: 159, Overall relative (Precision) Standard Deviation.\n",
      "51. Nube: 159, Overall relative Uncertainty BS, \n",
      "51. Nube: 159, Accuracy,\n",
      "51. Nube: 159, Completeness.\n",
      "51. Nube: 159, Overall concordance, \n",
      "52. Nube: 114, Overall relative (Precision) Standard Deviation.\n",
      "52. Nube: 114, Overall relative Uncertainty BS, \n",
      "52. Nube: 114, Accuracy,\n",
      "52. Nube: 114, Completeness.\n",
      "52. Nube: 114, Overall concordance, \n",
      "53. Nube: 265, Overall relative (Precision) Standard Deviation.\n",
      "53. Nube: 265, Overall relative Uncertainty BS, \n",
      "53. Nube: 265, Accuracy,\n",
      "53. Nube: 265, Completeness.\n",
      "53. Nube: 265, Overall concordance, \n",
      "54. Nube: 10, Overall relative (Precision) Standard Deviation.\n",
      "54. Nube: 10, Overall relative Uncertainty BS, \n",
      "54. Nube: 10, Accuracy,\n",
      "54. Nube: 10, Completeness.\n",
      "54. Nube: 10, Overall concordance, \n",
      "55. Nube: 45, Overall relative (Precision) Standard Deviation.\n",
      "55. Nube: 45, Overall relative Uncertainty BS, \n",
      "55. Nube: 45, Accuracy,\n",
      "55. Nube: 45, Completeness.\n",
      "55. Nube: 45, Overall concordance, \n",
      "56. Nube: 112, Overall relative (Precision) Standard Deviation.\n",
      "56. Nube: 112, Overall relative Uncertainty BS, \n",
      "56. Nube: 112, Accuracy,\n",
      "56. Nube: 112, Completeness.\n",
      "56. Nube: 112, Overall concordance, \n",
      "57. Nube: 252, Overall relative (Precision) Standard Deviation.\n",
      "57. Nube: 252, Overall relative Uncertainty BS, \n",
      "57. Nube: 252, Accuracy,\n",
      "57. Nube: 252, Completeness.\n",
      "57. Nube: 252, Overall concordance, \n",
      "58. Nube: 120, Overall relative (Precision) Standard Deviation.\n",
      "58. Nube: 120, Overall relative Uncertainty BS, \n",
      "58. Nube: 120, Accuracy,\n",
      "58. Nube: 120, Completeness.\n",
      "58. Nube: 120, Overall concordance, \n",
      "59. Nube: 111, Overall relative (Precision) Standard Deviation.\n",
      "59. Nube: 111, Overall relative Uncertainty BS, \n",
      "59. Nube: 111, Accuracy,\n",
      "59. Nube: 111, Completeness.\n",
      "59. Nube: 111, Overall concordance, \n",
      "60. Nube: 146, Overall relative (Precision) Standard Deviation.\n",
      "60. Nube: 146, Overall relative Uncertainty BS, \n",
      "60. Nube: 146, Accuracy,\n",
      "60. Nube: 146, Completeness.\n",
      "60. Nube: 146, Overall concordance, \n",
      "61. Nube: 122, Overall relative (Precision) Standard Deviation.\n",
      "61. Nube: 122, Overall relative Uncertainty BS, \n",
      "61. Nube: 122, Accuracy,\n",
      "61. Nube: 122, Completeness.\n",
      "61. Nube: 122, Overall concordance, \n",
      "62. Nube: 92, Overall relative (Precision) Standard Deviation.\n",
      "62. Nube: 92, Overall relative Uncertainty BS, \n",
      "62. Nube: 92, Accuracy,\n",
      "62. Nube: 92, Completeness.\n",
      "62. Nube: 92, Overall concordance, \n",
      "63. Nube: 151, Overall relative (Precision) Standard Deviation.\n",
      "63. Nube: 151, Overall relative Uncertainty BS, \n",
      "63. Nube: 151, Accuracy,\n",
      "63. Nube: 151, Completeness.\n",
      "63. Nube: 151, Overall concordance, \n",
      "64. Nube: 44, Overall relative (Precision) Standard Deviation.\n",
      "64. Nube: 44, Overall relative Uncertainty BS, \n",
      "64. Nube: 44, Accuracy,\n",
      "64. Nube: 44, Completeness.\n",
      "64. Nube: 44, Overall concordance, \n",
      "65. Nube: 42, Overall relative (Precision) Standard Deviation.\n",
      "65. Nube: 42, Overall relative Uncertainty BS, \n",
      "65. Nube: 42, Accuracy,\n",
      "65. Nube: 42, Completeness.\n",
      "65. Nube: 42, Overall concordance, \n",
      "66. Nube: 215, Overall relative (Precision) Standard Deviation.\n",
      "66. Nube: 215, Overall relative Uncertainty BS, \n",
      "66. Nube: 215, Accuracy,\n",
      "66. Nube: 215, Completeness.\n",
      "66. Nube: 215, Overall concordance, \n",
      "67. Nube: 79, Overall relative (Precision) Standard Deviation.\n",
      "67. Nube: 79, Overall relative Uncertainty BS, \n",
      "67. Nube: 79, Accuracy,\n",
      "67. Nube: 79, Completeness.\n",
      "67. Nube: 79, Overall concordance, \n",
      "68. Nube: 251, Overall relative (Precision) Standard Deviation.\n",
      "68. Nube: 251, Overall relative Uncertainty BS, \n",
      "68. Nube: 251, Accuracy,\n",
      "68. Nube: 251, Completeness.\n",
      "68. Nube: 251, Overall concordance, \n",
      "69. Nube: 195, Overall relative (Precision) Standard Deviation.\n",
      "69. Nube: 195, Overall relative Uncertainty BS, \n",
      "69. Nube: 195, Accuracy,\n",
      "69. Nube: 195, Completeness.\n",
      "69. Nube: 195, Overall concordance, \n",
      "70. Nube: 13, Overall relative (Precision) Standard Deviation.\n",
      "70. Nube: 13, Overall relative Uncertainty BS, \n",
      "70. Nube: 13, Accuracy,\n",
      "70. Nube: 13, Completeness.\n",
      "70. Nube: 13, Overall concordance, \n",
      "71. Nube: 243, Overall relative (Precision) Standard Deviation.\n",
      "71. Nube: 243, Overall relative Uncertainty BS, \n",
      "71. Nube: 243, Accuracy,\n",
      "71. Nube: 243, Completeness.\n",
      "71. Nube: 243, Overall concordance, \n",
      "72. Nube: 156, Overall relative (Precision) Standard Deviation.\n",
      "72. Nube: 156, Overall relative Uncertainty BS, \n",
      "72. Nube: 156, Accuracy,\n",
      "72. Nube: 156, Completeness.\n",
      "72. Nube: 156, Overall concordance, \n",
      "73. Nube: 127, Overall relative (Precision) Standard Deviation.\n",
      "73. Nube: 127, Overall relative Uncertainty BS, \n",
      "73. Nube: 127, Accuracy,\n",
      "73. Nube: 127, Completeness.\n",
      "73. Nube: 127, Overall concordance, \n",
      "74. Nube: 52, Overall relative (Precision) Standard Deviation.\n",
      "74. Nube: 52, Overall relative Uncertainty BS, \n",
      "74. Nube: 52, Accuracy,\n",
      "74. Nube: 52, Completeness.\n",
      "74. Nube: 52, Overall concordance, \n",
      "75. Nube: 137, Overall relative (Precision) Standard Deviation.\n",
      "75. Nube: 137, Overall relative Uncertainty BS, \n",
      "75. Nube: 137, Accuracy,\n",
      "75. Nube: 137, Completeness.\n",
      "75. Nube: 137, Overall concordance, \n",
      "76. Nube: 176, Overall relative (Precision) Standard Deviation.\n",
      "76. Nube: 176, Overall relative Uncertainty BS, \n",
      "76. Nube: 176, Accuracy,\n",
      "76. Nube: 176, Completeness.\n",
      "76. Nube: 176, Overall concordance, \n",
      "77. Nube: 203, Overall relative (Precision) Standard Deviation.\n",
      "77. Nube: 203, Overall relative Uncertainty BS, \n",
      "77. Nube: 203, Accuracy,\n",
      "77. Nube: 203, Completeness.\n",
      "77. Nube: 203, Overall concordance, \n",
      "78. Nube: 105, Overall relative (Precision) Standard Deviation.\n",
      "78. Nube: 105, Overall relative Uncertainty BS, \n",
      "78. Nube: 105, Accuracy,\n",
      "78. Nube: 105, Completeness.\n",
      "78. Nube: 105, Overall concordance, \n",
      "79. Nube: 191, Overall relative (Precision) Standard Deviation.\n",
      "79. Nube: 191, Overall relative Uncertainty BS, \n",
      "79. Nube: 191, Accuracy,\n",
      "79. Nube: 191, Completeness.\n",
      "79. Nube: 191, Overall concordance, \n",
      "80. Nube: 231, Overall relative (Precision) Standard Deviation.\n",
      "80. Nube: 231, Overall relative Uncertainty BS, \n",
      "80. Nube: 231, Accuracy,\n",
      "80. Nube: 231, Completeness.\n",
      "80. Nube: 231, Overall concordance, \n",
      "81. Nube: 180, Overall relative (Precision) Standard Deviation.\n",
      "81. Nube: 180, Overall relative Uncertainty BS, \n",
      "81. Nube: 180, Accuracy,\n",
      "81. Nube: 180, Completeness.\n",
      "81. Nube: 180, Overall concordance, \n",
      "82. Nube: 182, Overall relative (Precision) Standard Deviation.\n",
      "82. Nube: 182, Overall relative Uncertainty BS, \n",
      "82. Nube: 182, Accuracy,\n",
      "82. Nube: 182, Completeness.\n",
      "82. Nube: 182, Overall concordance, \n",
      "83. Nube: 59, Overall relative (Precision) Standard Deviation.\n",
      "83. Nube: 59, Overall relative Uncertainty BS, \n",
      "83. Nube: 59, Accuracy,\n",
      "83. Nube: 59, Completeness.\n",
      "83. Nube: 59, Overall concordance, \n",
      "84. Nube: 35, Overall relative (Precision) Standard Deviation.\n",
      "84. Nube: 35, Overall relative Uncertainty BS, \n",
      "84. Nube: 35, Accuracy,\n",
      "84. Nube: 35, Completeness.\n",
      "84. Nube: 35, Overall concordance, \n",
      "85. Nube: 2, Overall relative (Precision) Standard Deviation.\n",
      "85. Nube: 2, Overall relative Uncertainty BS, \n",
      "85. Nube: 2, Accuracy,\n",
      "85. Nube: 2, Completeness.\n",
      "85. Nube: 2, Overall concordance, \n",
      "86. Nube: 241, Overall relative (Precision) Standard Deviation.\n",
      "86. Nube: 241, Overall relative Uncertainty BS, \n",
      "86. Nube: 241, Accuracy,\n",
      "86. Nube: 241, Completeness.\n",
      "86. Nube: 241, Overall concordance, \n",
      "87. Nube: 206, Overall relative (Precision) Standard Deviation.\n",
      "87. Nube: 206, Overall relative Uncertainty BS, \n",
      "87. Nube: 206, Accuracy,\n",
      "87. Nube: 206, Completeness.\n",
      "87. Nube: 206, Overall concordance, \n",
      "88. Nube: 53, Overall relative (Precision) Standard Deviation.\n",
      "88. Nube: 53, Overall relative Uncertainty BS, \n",
      "88. Nube: 53, Accuracy,\n",
      "88. Nube: 53, Completeness.\n",
      "88. Nube: 53, Overall concordance, \n",
      "89. Nube: 237, Overall relative (Precision) Standard Deviation.\n",
      "89. Nube: 237, Overall relative Uncertainty BS, \n",
      "89. Nube: 237, Accuracy,\n",
      "89. Nube: 237, Completeness.\n",
      "89. Nube: 237, Overall concordance, \n",
      "90. Nube: 239, Overall relative (Precision) Standard Deviation.\n",
      "90. Nube: 239, Overall relative Uncertainty BS, \n",
      "90. Nube: 239, Accuracy,\n",
      "90. Nube: 239, Completeness.\n",
      "90. Nube: 239, Overall concordance, \n",
      "91. Nube: 55, Overall relative (Precision) Standard Deviation.\n",
      "91. Nube: 55, Overall relative Uncertainty BS, \n",
      "91. Nube: 55, Accuracy,\n",
      "91. Nube: 55, Completeness.\n",
      "91. Nube: 55, Overall concordance, \n",
      "92. Nube: 66, Overall relative (Precision) Standard Deviation.\n",
      "92. Nube: 66, Overall relative Uncertainty BS, \n",
      "92. Nube: 66, Accuracy,\n",
      "92. Nube: 66, Completeness.\n",
      "92. Nube: 66, Overall concordance, \n",
      "93. Nube: 38, Overall relative (Precision) Standard Deviation.\n",
      "93. Nube: 38, Overall relative Uncertainty BS, \n",
      "93. Nube: 38, Accuracy,\n",
      "93. Nube: 38, Completeness.\n",
      "93. Nube: 38, Overall concordance, \n",
      "94. Nube: 27, Overall relative (Precision) Standard Deviation.\n",
      "94. Nube: 27, Overall relative Uncertainty BS, \n",
      "94. Nube: 27, Accuracy,\n",
      "94. Nube: 27, Completeness.\n",
      "94. Nube: 27, Overall concordance, \n",
      "95. Nube: 247, Overall relative (Precision) Standard Deviation.\n",
      "95. Nube: 247, Overall relative Uncertainty BS, \n",
      "95. Nube: 247, Accuracy,\n",
      "95. Nube: 247, Completeness.\n",
      "95. Nube: 247, Overall concordance, \n",
      "96. Nube: 124, Overall relative (Precision) Standard Deviation.\n",
      "96. Nube: 124, Overall relative Uncertainty BS, \n",
      "96. Nube: 124, Accuracy,\n",
      "96. Nube: 124, Completeness.\n",
      "96. Nube: 124, Overall concordance, \n",
      "97. Nube: 207, Overall relative (Precision) Standard Deviation.\n",
      "97. Nube: 207, Overall relative Uncertainty BS, \n",
      "97. Nube: 207, Accuracy,\n",
      "97. Nube: 207, Completeness.\n",
      "97. Nube: 207, Overall concordance, \n",
      "98. Nube: 248, Overall relative (Precision) Standard Deviation.\n",
      "98. Nube: 248, Overall relative Uncertainty BS, \n",
      "98. Nube: 248, Accuracy,\n",
      "98. Nube: 248, Completeness.\n",
      "98. Nube: 248, Overall concordance, \n",
      "99. Nube: 43, Overall relative (Precision) Standard Deviation.\n",
      "99. Nube: 43, Overall relative Uncertainty BS, \n",
      "99. Nube: 43, Accuracy,\n",
      "99. Nube: 43, Completeness.\n",
      "99. Nube: 43, Overall concordance, \n",
      "100. Nube: 196, Overall relative (Precision) Standard Deviation.\n",
      "100. Nube: 196, Overall relative Uncertainty BS, \n",
      "100. Nube: 196, Accuracy,\n",
      "100. Nube: 196, Completeness.\n",
      "100. Nube: 196, Overall concordance, \n",
      "101. Nube: 242, Overall relative (Precision) Standard Deviation.\n",
      "101. Nube: 242, Overall relative Uncertainty BS, \n",
      "101. Nube: 242, Accuracy,\n",
      "101. Nube: 242, Completeness.\n",
      "101. Nube: 242, Overall concordance, \n",
      "102. Nube: 187, Overall relative (Precision) Standard Deviation.\n",
      "102. Nube: 187, Overall relative Uncertainty BS, \n",
      "102. Nube: 187, Accuracy,\n",
      "102. Nube: 187, Completeness.\n",
      "102. Nube: 187, Overall concordance, \n",
      "103. Nube: 128, Overall relative (Precision) Standard Deviation.\n",
      "103. Nube: 128, Overall relative Uncertainty BS, \n",
      "103. Nube: 128, Accuracy,\n",
      "103. Nube: 128, Completeness.\n",
      "103. Nube: 128, Overall concordance, \n",
      "104. Nube: 199, Overall relative (Precision) Standard Deviation.\n",
      "104. Nube: 199, Overall relative Uncertainty BS, \n",
      "104. Nube: 199, Accuracy,\n",
      "104. Nube: 199, Completeness.\n",
      "104. Nube: 199, Overall concordance, \n",
      "105. Nube: 181, Overall relative (Precision) Standard Deviation.\n",
      "105. Nube: 181, Overall relative Uncertainty BS, \n",
      "105. Nube: 181, Accuracy,\n",
      "105. Nube: 181, Completeness.\n",
      "105. Nube: 181, Overall concordance, \n",
      "106. Nube: 100, Overall relative (Precision) Standard Deviation.\n",
      "106. Nube: 100, Overall relative Uncertainty BS, \n",
      "106. Nube: 100, Accuracy,\n",
      "106. Nube: 100, Completeness.\n",
      "106. Nube: 100, Overall concordance, \n",
      "107. Nube: 224, Overall relative (Precision) Standard Deviation.\n",
      "107. Nube: 224, Overall relative Uncertainty BS, \n",
      "107. Nube: 224, Accuracy,\n",
      "107. Nube: 224, Completeness.\n",
      "107. Nube: 224, Overall concordance, \n",
      "108. Nube: 88, Overall relative (Precision) Standard Deviation.\n",
      "108. Nube: 88, Overall relative Uncertainty BS, \n",
      "108. Nube: 88, Accuracy,\n",
      "108. Nube: 88, Completeness.\n",
      "108. Nube: 88, Overall concordance, \n",
      "109. Nube: 71, Overall relative (Precision) Standard Deviation.\n",
      "109. Nube: 71, Overall relative Uncertainty BS, \n",
      "109. Nube: 71, Accuracy,\n",
      "109. Nube: 71, Completeness.\n",
      "109. Nube: 71, Overall concordance, \n",
      "110. Nube: 259, Overall relative (Precision) Standard Deviation.\n",
      "110. Nube: 259, Overall relative Uncertainty BS, \n",
      "110. Nube: 259, Accuracy,\n",
      "110. Nube: 259, Completeness.\n",
      "110. Nube: 259, Overall concordance, \n",
      "111. Nube: 193, Overall relative (Precision) Standard Deviation.\n",
      "111. Nube: 193, Overall relative Uncertainty BS, \n",
      "111. Nube: 193, Accuracy,\n",
      "111. Nube: 193, Completeness.\n",
      "111. Nube: 193, Overall concordance, \n",
      "112. Nube: 157, Overall relative (Precision) Standard Deviation.\n",
      "112. Nube: 157, Overall relative Uncertainty BS, \n",
      "112. Nube: 157, Accuracy,\n",
      "112. Nube: 157, Completeness.\n",
      "112. Nube: 157, Overall concordance, \n",
      "113. Nube: 113, Overall relative (Precision) Standard Deviation.\n",
      "113. Nube: 113, Overall relative Uncertainty BS, \n",
      "113. Nube: 113, Accuracy,\n",
      "113. Nube: 113, Completeness.\n",
      "113. Nube: 113, Overall concordance, \n",
      "114. Nube: 226, Overall relative (Precision) Standard Deviation.\n",
      "114. Nube: 226, Overall relative Uncertainty BS, \n",
      "114. Nube: 226, Accuracy,\n",
      "114. Nube: 226, Completeness.\n",
      "114. Nube: 226, Overall concordance, \n",
      "115. Nube: 145, Overall relative (Precision) Standard Deviation.\n",
      "115. Nube: 145, Overall relative Uncertainty BS, \n",
      "115. Nube: 145, Accuracy,\n",
      "115. Nube: 145, Completeness.\n",
      "115. Nube: 145, Overall concordance, \n",
      "116. Nube: 185, Overall relative (Precision) Standard Deviation.\n",
      "116. Nube: 185, Overall relative Uncertainty BS, \n",
      "116. Nube: 185, Accuracy,\n",
      "116. Nube: 185, Completeness.\n",
      "116. Nube: 185, Overall concordance, \n",
      "117. Nube: 162, Overall relative (Precision) Standard Deviation.\n",
      "117. Nube: 162, Overall relative Uncertainty BS, \n",
      "117. Nube: 162, Accuracy,\n",
      "117. Nube: 162, Completeness.\n",
      "117. Nube: 162, Overall concordance, \n",
      "118. Nube: 81, Overall relative (Precision) Standard Deviation.\n",
      "118. Nube: 81, Overall relative Uncertainty BS, \n",
      "118. Nube: 81, Accuracy,\n",
      "118. Nube: 81, Completeness.\n",
      "118. Nube: 81, Overall concordance, \n",
      "119. Nube: 178, Overall relative (Precision) Standard Deviation.\n",
      "119. Nube: 178, Overall relative Uncertainty BS, \n",
      "119. Nube: 178, Accuracy,\n",
      "119. Nube: 178, Completeness.\n",
      "119. Nube: 178, Overall concordance, \n",
      "120. Nube: 29, Overall relative (Precision) Standard Deviation.\n",
      "120. Nube: 29, Overall relative Uncertainty BS, \n",
      "120. Nube: 29, Accuracy,\n",
      "120. Nube: 29, Completeness.\n",
      "120. Nube: 29, Overall concordance, \n",
      "121. Nube: 225, Overall relative (Precision) Standard Deviation.\n",
      "121. Nube: 225, Overall relative Uncertainty BS, \n",
      "121. Nube: 225, Accuracy,\n",
      "121. Nube: 225, Completeness.\n",
      "121. Nube: 225, Overall concordance, \n",
      "122. Nube: 221, Overall relative (Precision) Standard Deviation.\n",
      "122. Nube: 221, Overall relative Uncertainty BS, \n",
      "122. Nube: 221, Accuracy,\n",
      "122. Nube: 221, Completeness.\n",
      "122. Nube: 221, Overall concordance, \n",
      "123. Nube: 4, Overall relative (Precision) Standard Deviation.\n",
      "123. Nube: 4, Overall relative Uncertainty BS, \n",
      "123. Nube: 4, Accuracy,\n",
      "123. Nube: 4, Completeness.\n",
      "123. Nube: 4, Overall concordance, \n",
      "124. Nube: 78, Overall relative (Precision) Standard Deviation.\n",
      "124. Nube: 78, Overall relative Uncertainty BS, \n",
      "124. Nube: 78, Accuracy,\n",
      "124. Nube: 78, Completeness.\n",
      "124. Nube: 78, Overall concordance, \n",
      "125. Nube: 161, Overall relative (Precision) Standard Deviation.\n",
      "125. Nube: 161, Overall relative Uncertainty BS, \n",
      "125. Nube: 161, Accuracy,\n",
      "125. Nube: 161, Completeness.\n",
      "125. Nube: 161, Overall concordance, \n",
      "126. Nube: 62, Overall relative (Precision) Standard Deviation.\n",
      "126. Nube: 62, Overall relative Uncertainty BS, \n",
      "126. Nube: 62, Accuracy,\n",
      "126. Nube: 62, Completeness.\n",
      "126. Nube: 62, Overall concordance, \n",
      "127. Nube: 130, Overall relative (Precision) Standard Deviation.\n",
      "127. Nube: 130, Overall relative Uncertainty BS, \n",
      "127. Nube: 130, Accuracy,\n",
      "127. Nube: 130, Completeness.\n",
      "127. Nube: 130, Overall concordance, \n",
      "128. Nube: 149, Overall relative (Precision) Standard Deviation.\n",
      "128. Nube: 149, Overall relative Uncertainty BS, \n",
      "128. Nube: 149, Accuracy,\n",
      "128. Nube: 149, Completeness.\n",
      "128. Nube: 149, Overall concordance, \n",
      "129. Nube: 189, Overall relative (Precision) Standard Deviation.\n",
      "129. Nube: 189, Overall relative Uncertainty BS, \n",
      "129. Nube: 189, Accuracy,\n",
      "129. Nube: 189, Completeness.\n",
      "129. Nube: 189, Overall concordance, \n",
      "130. Nube: 116, Overall relative (Precision) Standard Deviation.\n",
      "130. Nube: 116, Overall relative Uncertainty BS, \n",
      "130. Nube: 116, Accuracy,\n",
      "130. Nube: 116, Completeness.\n",
      "130. Nube: 116, Overall concordance, \n",
      "131. Nube: 219, Overall relative (Precision) Standard Deviation.\n",
      "131. Nube: 219, Overall relative Uncertainty BS, \n",
      "131. Nube: 219, Accuracy,\n",
      "131. Nube: 219, Completeness.\n",
      "131. Nube: 219, Overall concordance, \n",
      "132. Nube: 3, Overall relative (Precision) Standard Deviation.\n",
      "132. Nube: 3, Overall relative Uncertainty BS, \n",
      "132. Nube: 3, Accuracy,\n",
      "132. Nube: 3, Completeness.\n",
      "132. Nube: 3, Overall concordance, \n",
      "133. Nube: 236, Overall relative (Precision) Standard Deviation.\n",
      "133. Nube: 236, Overall relative Uncertainty BS, \n",
      "133. Nube: 236, Accuracy,\n",
      "133. Nube: 236, Completeness.\n",
      "133. Nube: 236, Overall concordance, \n",
      "134. Nube: 238, Overall relative (Precision) Standard Deviation.\n",
      "134. Nube: 238, Overall relative Uncertainty BS, \n",
      "134. Nube: 238, Accuracy,\n",
      "134. Nube: 238, Completeness.\n",
      "134. Nube: 238, Overall concordance, \n",
      "135. Nube: 58, Overall relative (Precision) Standard Deviation.\n",
      "135. Nube: 58, Overall relative Uncertainty BS, \n",
      "135. Nube: 58, Accuracy,\n",
      "135. Nube: 58, Completeness.\n",
      "135. Nube: 58, Overall concordance, \n",
      "136. Nube: 61, Overall relative (Precision) Standard Deviation.\n",
      "136. Nube: 61, Overall relative Uncertainty BS, \n",
      "136. Nube: 61, Accuracy,\n",
      "136. Nube: 61, Completeness.\n",
      "136. Nube: 61, Overall concordance, \n",
      "137. Nube: 91, Overall relative (Precision) Standard Deviation.\n",
      "137. Nube: 91, Overall relative Uncertainty BS, \n",
      "137. Nube: 91, Accuracy,\n",
      "137. Nube: 91, Completeness.\n",
      "137. Nube: 91, Overall concordance, \n",
      "138. Nube: 154, Overall relative (Precision) Standard Deviation.\n",
      "138. Nube: 154, Overall relative Uncertainty BS, \n",
      "138. Nube: 154, Accuracy,\n",
      "138. Nube: 154, Completeness.\n",
      "138. Nube: 154, Overall concordance, \n",
      "139. Nube: 76, Overall relative (Precision) Standard Deviation.\n",
      "139. Nube: 76, Overall relative Uncertainty BS, \n",
      "139. Nube: 76, Accuracy,\n",
      "139. Nube: 76, Completeness.\n",
      "139. Nube: 76, Overall concordance, \n",
      "140. Nube: 119, Overall relative (Precision) Standard Deviation.\n",
      "140. Nube: 119, Overall relative Uncertainty BS, \n",
      "140. Nube: 119, Accuracy,\n",
      "140. Nube: 119, Completeness.\n",
      "140. Nube: 119, Overall concordance, \n",
      "141. Nube: 106, Overall relative (Precision) Standard Deviation.\n",
      "141. Nube: 106, Overall relative Uncertainty BS, \n",
      "141. Nube: 106, Accuracy,\n",
      "141. Nube: 106, Completeness.\n",
      "141. Nube: 106, Overall concordance, \n",
      "142. Nube: 109, Overall relative (Precision) Standard Deviation.\n",
      "142. Nube: 109, Overall relative Uncertainty BS, \n",
      "142. Nube: 109, Accuracy,\n",
      "142. Nube: 109, Completeness.\n",
      "142. Nube: 109, Overall concordance, \n",
      "143. Nube: 108, Overall relative (Precision) Standard Deviation.\n",
      "143. Nube: 108, Overall relative Uncertainty BS, \n",
      "143. Nube: 108, Accuracy,\n",
      "143. Nube: 108, Completeness.\n",
      "143. Nube: 108, Overall concordance, \n",
      "144. Nube: 211, Overall relative (Precision) Standard Deviation.\n",
      "144. Nube: 211, Overall relative Uncertainty BS, \n",
      "144. Nube: 211, Accuracy,\n",
      "144. Nube: 211, Completeness.\n",
      "144. Nube: 211, Overall concordance, \n",
      "145. Nube: 46, Overall relative (Precision) Standard Deviation.\n",
      "145. Nube: 46, Overall relative Uncertainty BS, \n",
      "145. Nube: 46, Accuracy,\n",
      "145. Nube: 46, Completeness.\n",
      "145. Nube: 46, Overall concordance, \n",
      "146. Nube: 65, Overall relative (Precision) Standard Deviation.\n",
      "146. Nube: 65, Overall relative Uncertainty BS, \n",
      "146. Nube: 65, Accuracy,\n",
      "146. Nube: 65, Completeness.\n",
      "146. Nube: 65, Overall concordance, \n",
      "147. Nube: 131, Overall relative (Precision) Standard Deviation.\n",
      "147. Nube: 131, Overall relative Uncertainty BS, \n",
      "147. Nube: 131, Accuracy,\n",
      "147. Nube: 131, Completeness.\n",
      "147. Nube: 131, Overall concordance, \n",
      "148. Nube: 28, Overall relative (Precision) Standard Deviation.\n",
      "148. Nube: 28, Overall relative Uncertainty BS, \n",
      "148. Nube: 28, Accuracy,\n",
      "148. Nube: 28, Completeness.\n",
      "148. Nube: 28, Overall concordance, \n",
      "149. Nube: 97, Overall relative (Precision) Standard Deviation.\n",
      "149. Nube: 97, Overall relative Uncertainty BS, \n",
      "149. Nube: 97, Accuracy,\n",
      "149. Nube: 97, Completeness.\n",
      "149. Nube: 97, Overall concordance, \n",
      "150. Nube: 95, Overall relative (Precision) Standard Deviation.\n",
      "150. Nube: 95, Overall relative Uncertainty BS, \n",
      "150. Nube: 95, Accuracy,\n",
      "150. Nube: 95, Completeness.\n",
      "150. Nube: 95, Overall concordance, \n",
      "151. Nube: 56, Overall relative (Precision) Standard Deviation.\n",
      "151. Nube: 56, Overall relative Uncertainty BS, \n",
      "151. Nube: 56, Accuracy,\n",
      "151. Nube: 56, Completeness.\n",
      "151. Nube: 56, Overall concordance, \n",
      "152. Nube: 152, Overall relative (Precision) Standard Deviation.\n",
      "152. Nube: 152, Overall relative Uncertainty BS, \n",
      "152. Nube: 152, Accuracy,\n",
      "152. Nube: 152, Completeness.\n",
      "152. Nube: 152, Overall concordance, \n",
      "153. Nube: 250, Overall relative (Precision) Standard Deviation.\n",
      "153. Nube: 250, Overall relative Uncertainty BS, \n",
      "153. Nube: 250, Accuracy,\n",
      "153. Nube: 250, Completeness.\n",
      "153. Nube: 250, Overall concordance, \n",
      "154. Nube: 5, Overall relative (Precision) Standard Deviation.\n",
      "154. Nube: 5, Overall relative Uncertainty BS, \n",
      "154. Nube: 5, Accuracy,\n",
      "154. Nube: 5, Completeness.\n",
      "154. Nube: 5, Overall concordance, \n",
      "155. Nube: 48, Overall relative (Precision) Standard Deviation.\n",
      "155. Nube: 48, Overall relative Uncertainty BS, \n",
      "155. Nube: 48, Accuracy,\n",
      "155. Nube: 48, Completeness.\n",
      "155. Nube: 48, Overall concordance, \n",
      "156. Nube: 57, Overall relative (Precision) Standard Deviation.\n",
      "156. Nube: 57, Overall relative Uncertainty BS, \n",
      "156. Nube: 57, Accuracy,\n",
      "156. Nube: 57, Completeness.\n",
      "156. Nube: 57, Overall concordance, \n",
      "157. Nube: 83, Overall relative (Precision) Standard Deviation.\n",
      "157. Nube: 83, Overall relative Uncertainty BS, \n",
      "157. Nube: 83, Accuracy,\n",
      "157. Nube: 83, Completeness.\n",
      "157. Nube: 83, Overall concordance, \n",
      "158. Nube: 84, Overall relative (Precision) Standard Deviation.\n",
      "158. Nube: 84, Overall relative Uncertainty BS, \n",
      "158. Nube: 84, Accuracy,\n",
      "158. Nube: 84, Completeness.\n",
      "158. Nube: 84, Overall concordance, \n",
      "159. Nube: 115, Overall relative (Precision) Standard Deviation.\n",
      "159. Nube: 115, Overall relative Uncertainty BS, \n",
      "159. Nube: 115, Accuracy,\n",
      "159. Nube: 115, Completeness.\n",
      "159. Nube: 115, Overall concordance, \n",
      "160. Nube: 107, Overall relative (Precision) Standard Deviation.\n",
      "160. Nube: 107, Overall relative Uncertainty BS, \n",
      "160. Nube: 107, Accuracy,\n",
      "160. Nube: 107, Completeness.\n",
      "160. Nube: 107, Overall concordance, \n",
      "161. Nube: 75, Overall relative (Precision) Standard Deviation.\n",
      "161. Nube: 75, Overall relative Uncertainty BS, \n",
      "161. Nube: 75, Accuracy,\n",
      "161. Nube: 75, Completeness.\n",
      "161. Nube: 75, Overall concordance, \n",
      "162. Nube: 90, Overall relative (Precision) Standard Deviation.\n",
      "162. Nube: 90, Overall relative Uncertainty BS, \n",
      "162. Nube: 90, Accuracy,\n",
      "162. Nube: 90, Completeness.\n",
      "162. Nube: 90, Overall concordance, \n",
      "163. Nube: 134, Overall relative (Precision) Standard Deviation.\n",
      "163. Nube: 134, Overall relative Uncertainty BS, \n",
      "163. Nube: 134, Accuracy,\n",
      "163. Nube: 134, Completeness.\n",
      "163. Nube: 134, Overall concordance, \n",
      "164. Nube: 201, Overall relative (Precision) Standard Deviation.\n",
      "164. Nube: 201, Overall relative Uncertainty BS, \n",
      "164. Nube: 201, Accuracy,\n",
      "164. Nube: 201, Completeness.\n",
      "164. Nube: 201, Overall concordance, \n",
      "165. Nube: 166, Overall relative (Precision) Standard Deviation.\n",
      "165. Nube: 166, Overall relative Uncertainty BS, \n",
      "165. Nube: 166, Accuracy,\n",
      "165. Nube: 166, Completeness.\n",
      "165. Nube: 166, Overall concordance, \n",
      "166. Nube: 30, Overall relative (Precision) Standard Deviation.\n",
      "166. Nube: 30, Overall relative Uncertainty BS, \n",
      "166. Nube: 30, Accuracy,\n",
      "166. Nube: 30, Completeness.\n",
      "166. Nube: 30, Overall concordance, \n",
      "167. Nube: 139, Overall relative (Precision) Standard Deviation.\n",
      "167. Nube: 139, Overall relative Uncertainty BS, \n",
      "167. Nube: 139, Accuracy,\n",
      "167. Nube: 139, Completeness.\n",
      "167. Nube: 139, Overall concordance, \n",
      "168. Nube: 54, Overall relative (Precision) Standard Deviation.\n",
      "168. Nube: 54, Overall relative Uncertainty BS, \n",
      "168. Nube: 54, Accuracy,\n",
      "168. Nube: 54, Completeness.\n",
      "168. Nube: 54, Overall concordance, \n",
      "169. Nube: 210, Overall relative (Precision) Standard Deviation.\n",
      "169. Nube: 210, Overall relative Uncertainty BS, \n",
      "169. Nube: 210, Accuracy,\n",
      "169. Nube: 210, Completeness.\n",
      "169. Nube: 210, Overall concordance, \n",
      "170. Nube: 205, Overall relative (Precision) Standard Deviation.\n",
      "170. Nube: 205, Overall relative Uncertainty BS, \n",
      "170. Nube: 205, Accuracy,\n",
      "170. Nube: 205, Completeness.\n",
      "170. Nube: 205, Overall concordance, \n",
      "171. Nube: 208, Overall relative (Precision) Standard Deviation.\n",
      "171. Nube: 208, Overall relative Uncertainty BS, \n",
      "171. Nube: 208, Accuracy,\n",
      "171. Nube: 208, Completeness.\n",
      "171. Nube: 208, Overall concordance, \n",
      "172. Nube: 216, Overall relative (Precision) Standard Deviation.\n",
      "172. Nube: 216, Overall relative Uncertainty BS, \n",
      "172. Nube: 216, Accuracy,\n",
      "172. Nube: 216, Completeness.\n",
      "172. Nube: 216, Overall concordance, \n",
      "173. Nube: 228, Overall relative (Precision) Standard Deviation.\n",
      "173. Nube: 228, Overall relative Uncertainty BS, \n",
      "173. Nube: 228, Accuracy,\n",
      "173. Nube: 228, Completeness.\n",
      "173. Nube: 228, Overall concordance, \n",
      "174. Nube: 233, Overall relative (Precision) Standard Deviation.\n",
      "174. Nube: 233, Overall relative Uncertainty BS, \n",
      "174. Nube: 233, Accuracy,\n",
      "174. Nube: 233, Completeness.\n",
      "174. Nube: 233, Overall concordance, \n",
      "175. Nube: 244, Overall relative (Precision) Standard Deviation.\n",
      "175. Nube: 244, Overall relative Uncertainty BS, \n",
      "175. Nube: 244, Accuracy,\n",
      "175. Nube: 244, Completeness.\n",
      "175. Nube: 244, Overall concordance, \n",
      "176. Nube: 172, Overall relative (Precision) Standard Deviation.\n",
      "176. Nube: 172, Overall relative Uncertainty BS, \n",
      "176. Nube: 172, Accuracy,\n",
      "176. Nube: 172, Completeness.\n",
      "176. Nube: 172, Overall concordance, \n",
      "177. Nube: 232, Overall relative (Precision) Standard Deviation.\n",
      "177. Nube: 232, Overall relative Uncertainty BS, \n",
      "177. Nube: 232, Accuracy,\n",
      "177. Nube: 232, Completeness.\n",
      "177. Nube: 232, Overall concordance, \n",
      "178. Nube: 125, Overall relative (Precision) Standard Deviation.\n",
      "178. Nube: 125, Overall relative Uncertainty BS, \n",
      "178. Nube: 125, Accuracy,\n",
      "178. Nube: 125, Completeness.\n",
      "178. Nube: 125, Overall concordance, \n",
      "179. Nube: 266, Overall relative (Precision) Standard Deviation.\n",
      "179. Nube: 266, Overall relative Uncertainty BS, \n",
      "179. Nube: 266, Accuracy,\n",
      "179. Nube: 266, Completeness.\n",
      "179. Nube: 266, Overall concordance, \n",
      "180. Nube: 165, Overall relative (Precision) Standard Deviation.\n",
      "180. Nube: 165, Overall relative Uncertainty BS, \n",
      "180. Nube: 165, Accuracy,\n",
      "180. Nube: 165, Completeness.\n",
      "180. Nube: 165, Overall concordance, \n",
      "181. Nube: 194, Overall relative (Precision) Standard Deviation.\n",
      "181. Nube: 194, Overall relative Uncertainty BS, \n",
      "181. Nube: 194, Accuracy,\n",
      "181. Nube: 194, Completeness.\n",
      "181. Nube: 194, Overall concordance, \n",
      "182. Nube: 168, Overall relative (Precision) Standard Deviation.\n",
      "182. Nube: 168, Overall relative Uncertainty BS, \n",
      "182. Nube: 168, Accuracy,\n",
      "182. Nube: 168, Completeness.\n",
      "182. Nube: 168, Overall concordance, \n",
      "183. Nube: 245, Overall relative (Precision) Standard Deviation.\n",
      "183. Nube: 245, Overall relative Uncertainty BS, \n",
      "183. Nube: 245, Accuracy,\n",
      "183. Nube: 245, Completeness.\n",
      "183. Nube: 245, Overall concordance, \n",
      "184. Nube: 67, Overall relative (Precision) Standard Deviation.\n",
      "184. Nube: 67, Overall relative Uncertainty BS, \n",
      "184. Nube: 67, Accuracy,\n",
      "184. Nube: 67, Completeness.\n",
      "184. Nube: 67, Overall concordance, \n",
      "185. Nube: 15, Overall relative (Precision) Standard Deviation.\n",
      "185. Nube: 15, Overall relative Uncertainty BS, \n",
      "185. Nube: 15, Accuracy,\n",
      "185. Nube: 15, Completeness.\n",
      "185. Nube: 15, Overall concordance, \n",
      "186. Nube: 14, Overall relative (Precision) Standard Deviation.\n",
      "186. Nube: 14, Overall relative Uncertainty BS, \n",
      "186. Nube: 14, Accuracy,\n",
      "186. Nube: 14, Completeness.\n",
      "186. Nube: 14, Overall concordance, \n",
      "187. Nube: 222, Overall relative (Precision) Standard Deviation.\n",
      "187. Nube: 222, Overall relative Uncertainty BS, \n",
      "187. Nube: 222, Accuracy,\n",
      "187. Nube: 222, Completeness.\n",
      "187. Nube: 222, Overall concordance, \n",
      "188. Nube: 51, Overall relative (Precision) Standard Deviation.\n",
      "188. Nube: 51, Overall relative Uncertainty BS, \n",
      "188. Nube: 51, Accuracy,\n",
      "188. Nube: 51, Completeness.\n",
      "188. Nube: 51, Overall concordance, \n",
      "189. Nube: 214, Overall relative (Precision) Standard Deviation.\n",
      "189. Nube: 214, Overall relative Uncertainty BS, \n",
      "189. Nube: 214, Accuracy,\n",
      "189. Nube: 214, Completeness.\n",
      "189. Nube: 214, Overall concordance, \n",
      "190. Nube: 39, Overall relative (Precision) Standard Deviation.\n",
      "190. Nube: 39, Overall relative Uncertainty BS, \n",
      "190. Nube: 39, Accuracy,\n",
      "190. Nube: 39, Completeness.\n",
      "190. Nube: 39, Overall concordance, \n",
      "191. Nube: 24, Overall relative (Precision) Standard Deviation.\n",
      "191. Nube: 24, Overall relative Uncertainty BS, \n",
      "191. Nube: 24, Accuracy,\n",
      "191. Nube: 24, Completeness.\n",
      "191. Nube: 24, Overall concordance, \n",
      "192. Nube: 150, Overall relative (Precision) Standard Deviation.\n",
      "192. Nube: 150, Overall relative Uncertainty BS, \n",
      "192. Nube: 150, Accuracy,\n",
      "192. Nube: 150, Completeness.\n",
      "192. Nube: 150, Overall concordance, \n",
      "193. Nube: 249, Overall relative (Precision) Standard Deviation.\n",
      "193. Nube: 249, Overall relative Uncertainty BS, \n",
      "193. Nube: 249, Accuracy,\n",
      "193. Nube: 249, Completeness.\n",
      "193. Nube: 249, Overall concordance, \n",
      "194. Nube: 136, Overall relative (Precision) Standard Deviation.\n",
      "194. Nube: 136, Overall relative Uncertainty BS, \n",
      "194. Nube: 136, Accuracy,\n",
      "194. Nube: 136, Completeness.\n",
      "194. Nube: 136, Overall concordance, \n",
      "195. Nube: 12, Overall relative (Precision) Standard Deviation.\n",
      "195. Nube: 12, Overall relative Uncertainty BS, \n",
      "195. Nube: 12, Accuracy,\n",
      "195. Nube: 12, Completeness.\n",
      "195. Nube: 12, Overall concordance, \n",
      "196. Nube: 198, Overall relative (Precision) Standard Deviation.\n",
      "196. Nube: 198, Overall relative Uncertainty BS, \n",
      "196. Nube: 198, Accuracy,\n",
      "196. Nube: 198, Completeness.\n",
      "196. Nube: 198, Overall concordance, \n",
      "197. Nube: 213, Overall relative (Precision) Standard Deviation.\n",
      "197. Nube: 213, Overall relative Uncertainty BS, \n",
      "197. Nube: 213, Accuracy,\n",
      "197. Nube: 213, Completeness.\n",
      "197. Nube: 213, Overall concordance, \n",
      "198. Nube: 68, Overall relative (Precision) Standard Deviation.\n",
      "198. Nube: 68, Overall relative Uncertainty BS, \n",
      "198. Nube: 68, Accuracy,\n",
      "198. Nube: 68, Completeness.\n",
      "198. Nube: 68, Overall concordance, \n",
      "199. Nube: 118, Overall relative (Precision) Standard Deviation.\n",
      "199. Nube: 118, Overall relative Uncertainty BS, \n",
      "199. Nube: 118, Accuracy,\n",
      "199. Nube: 118, Completeness.\n",
      "199. Nube: 118, Overall concordance, \n",
      "200. Nube: 69, Overall relative (Precision) Standard Deviation.\n",
      "200. Nube: 69, Overall relative Uncertainty BS, \n",
      "200. Nube: 69, Accuracy,\n",
      "200. Nube: 69, Completeness.\n",
      "200. Nube: 69, Overall concordance, \n",
      "201. Nube: 230, Overall relative (Precision) Standard Deviation.\n",
      "201. Nube: 230, Overall relative Uncertainty BS, \n",
      "201. Nube: 230, Accuracy,\n",
      "201. Nube: 230, Completeness.\n",
      "201. Nube: 230, Overall concordance, \n",
      "202. Nube: 121, Overall relative (Precision) Standard Deviation.\n",
      "202. Nube: 121, Overall relative Uncertainty BS, \n",
      "202. Nube: 121, Accuracy,\n",
      "202. Nube: 121, Completeness.\n",
      "202. Nube: 121, Overall concordance, \n",
      "203. Nube: 36, Overall relative (Precision) Standard Deviation.\n",
      "203. Nube: 36, Overall relative Uncertainty BS, \n",
      "203. Nube: 36, Accuracy,\n",
      "203. Nube: 36, Completeness.\n",
      "203. Nube: 36, Overall concordance, \n",
      "204. Nube: 173, Overall relative (Precision) Standard Deviation.\n",
      "204. Nube: 173, Overall relative Uncertainty BS, \n",
      "204. Nube: 173, Accuracy,\n",
      "204. Nube: 173, Completeness.\n",
      "204. Nube: 173, Overall concordance, \n",
      "205. Nube: 174, Overall relative (Precision) Standard Deviation.\n",
      "205. Nube: 174, Overall relative Uncertainty BS, \n",
      "205. Nube: 174, Accuracy,\n",
      "205. Nube: 174, Completeness.\n",
      "205. Nube: 174, Overall concordance, \n",
      "206. Nube: 190, Overall relative (Precision) Standard Deviation.\n",
      "206. Nube: 190, Overall relative Uncertainty BS, \n",
      "206. Nube: 190, Accuracy,\n",
      "206. Nube: 190, Completeness.\n",
      "206. Nube: 190, Overall concordance, \n",
      "207. Nube: 153, Overall relative (Precision) Standard Deviation.\n",
      "207. Nube: 153, Overall relative Uncertainty BS, \n",
      "207. Nube: 153, Accuracy,\n",
      "207. Nube: 153, Completeness.\n",
      "207. Nube: 153, Overall concordance, \n",
      "208. Nube: 200, Overall relative (Precision) Standard Deviation.\n",
      "208. Nube: 200, Overall relative Uncertainty BS, \n",
      "208. Nube: 200, Accuracy,\n",
      "208. Nube: 200, Completeness.\n",
      "208. Nube: 200, Overall concordance, \n",
      "209. Nube: 144, Overall relative (Precision) Standard Deviation.\n",
      "209. Nube: 144, Overall relative Uncertainty BS, \n",
      "209. Nube: 144, Accuracy,\n",
      "209. Nube: 144, Completeness.\n",
      "209. Nube: 144, Overall concordance, \n",
      "210. Nube: 171, Overall relative (Precision) Standard Deviation.\n",
      "210. Nube: 171, Overall relative Uncertainty BS, \n",
      "210. Nube: 171, Accuracy,\n",
      "210. Nube: 171, Completeness.\n",
      "210. Nube: 171, Overall concordance, \n",
      "211. Nube: 63, Overall relative (Precision) Standard Deviation.\n",
      "211. Nube: 63, Overall relative Uncertainty BS, \n",
      "211. Nube: 63, Accuracy,\n",
      "211. Nube: 63, Completeness.\n",
      "211. Nube: 63, Overall concordance, \n",
      "212. Nube: 229, Overall relative (Precision) Standard Deviation.\n",
      "212. Nube: 229, Overall relative Uncertainty BS, \n",
      "212. Nube: 229, Accuracy,\n",
      "212. Nube: 229, Completeness.\n",
      "212. Nube: 229, Overall concordance, \n",
      "213. Nube: 32, Overall relative (Precision) Standard Deviation.\n",
      "213. Nube: 32, Overall relative Uncertainty BS, \n",
      "213. Nube: 32, Accuracy,\n",
      "213. Nube: 32, Completeness.\n",
      "213. Nube: 32, Overall concordance, \n",
      "214. Nube: 77, Overall relative (Precision) Standard Deviation.\n",
      "214. Nube: 77, Overall relative Uncertainty BS, \n",
      "214. Nube: 77, Accuracy,\n",
      "214. Nube: 77, Completeness.\n",
      "214. Nube: 77, Overall concordance, \n",
      "215. Nube: 72, Overall relative (Precision) Standard Deviation.\n",
      "215. Nube: 72, Overall relative Uncertainty BS, \n",
      "215. Nube: 72, Accuracy,\n",
      "215. Nube: 72, Completeness.\n",
      "215. Nube: 72, Overall concordance, \n",
      "216. Nube: 22, Overall relative (Precision) Standard Deviation.\n",
      "216. Nube: 22, Overall relative Uncertainty BS, \n",
      "216. Nube: 22, Accuracy,\n",
      "216. Nube: 22, Completeness.\n",
      "216. Nube: 22, Overall concordance, \n",
      "217. Nube: 33, Overall relative (Precision) Standard Deviation.\n",
      "217. Nube: 33, Overall relative Uncertainty BS, \n",
      "217. Nube: 33, Accuracy,\n",
      "217. Nube: 33, Completeness.\n",
      "217. Nube: 33, Overall concordance, \n",
      "218. Nube: 9, Overall relative (Precision) Standard Deviation.\n",
      "218. Nube: 9, Overall relative Uncertainty BS, \n",
      "218. Nube: 9, Accuracy,\n",
      "218. Nube: 9, Completeness.\n",
      "218. Nube: 9, Overall concordance, \n",
      "219. Nube: 70, Overall relative (Precision) Standard Deviation.\n",
      "219. Nube: 70, Overall relative Uncertainty BS, \n",
      "219. Nube: 70, Accuracy,\n",
      "219. Nube: 70, Completeness.\n",
      "219. Nube: 70, Overall concordance, \n",
      "220. Nube: 73, Overall relative (Precision) Standard Deviation.\n",
      "220. Nube: 73, Overall relative Uncertainty BS, \n",
      "220. Nube: 73, Accuracy,\n",
      "220. Nube: 73, Completeness.\n",
      "220. Nube: 73, Overall concordance, \n",
      "221. Nube: 20, Overall relative (Precision) Standard Deviation.\n",
      "221. Nube: 20, Overall relative Uncertainty BS, \n",
      "221. Nube: 20, Accuracy,\n",
      "221. Nube: 20, Completeness.\n",
      "221. Nube: 20, Overall concordance, \n",
      "222. Nube: 96, Overall relative (Precision) Standard Deviation.\n",
      "222. Nube: 96, Overall relative Uncertainty BS, \n",
      "222. Nube: 96, Accuracy,\n",
      "222. Nube: 96, Completeness.\n",
      "222. Nube: 96, Overall concordance, \n",
      "223. Nube: 133, Overall relative (Precision) Standard Deviation.\n",
      "223. Nube: 133, Overall relative Uncertainty BS, \n",
      "223. Nube: 133, Accuracy,\n",
      "223. Nube: 133, Completeness.\n",
      "223. Nube: 133, Overall concordance, \n",
      "224. Nube: 99, Overall relative (Precision) Standard Deviation.\n",
      "224. Nube: 99, Overall relative Uncertainty BS, \n",
      "224. Nube: 99, Accuracy,\n",
      "224. Nube: 99, Completeness.\n",
      "224. Nube: 99, Overall concordance, \n",
      "225. Nube: 23, Overall relative (Precision) Standard Deviation.\n",
      "225. Nube: 23, Overall relative Uncertainty BS, \n",
      "225. Nube: 23, Accuracy,\n",
      "225. Nube: 23, Completeness.\n",
      "225. Nube: 23, Overall concordance, \n",
      "226. Nube: 223, Overall relative (Precision) Standard Deviation.\n",
      "226. Nube: 223, Overall relative Uncertainty BS, \n",
      "226. Nube: 223, Accuracy,\n",
      "226. Nube: 223, Completeness.\n",
      "226. Nube: 223, Overall concordance, \n",
      "227. Nube: 85, Overall relative (Precision) Standard Deviation.\n",
      "227. Nube: 85, Overall relative Uncertainty BS, \n",
      "227. Nube: 85, Accuracy,\n",
      "227. Nube: 85, Completeness.\n",
      "227. Nube: 85, Overall concordance, \n",
      "228. Nube: 184, Overall relative (Precision) Standard Deviation.\n",
      "228. Nube: 184, Overall relative Uncertainty BS, \n",
      "228. Nube: 184, Accuracy,\n",
      "228. Nube: 184, Completeness.\n",
      "228. Nube: 184, Overall concordance, \n",
      "229. Nube: 117, Overall relative (Precision) Standard Deviation.\n",
      "229. Nube: 117, Overall relative Uncertainty BS, \n",
      "229. Nube: 117, Accuracy,\n",
      "229. Nube: 117, Completeness.\n",
      "229. Nube: 117, Overall concordance, \n",
      "230. Nube: 227, Overall relative (Precision) Standard Deviation.\n",
      "230. Nube: 227, Overall relative Uncertainty BS, \n",
      "230. Nube: 227, Accuracy,\n",
      "230. Nube: 227, Completeness.\n",
      "230. Nube: 227, Overall concordance, \n",
      "Time elapsed:  515.9096748828888\n"
     ]
    }
   ],
   "source": [
    "acc_vs_dis=[]\n",
    "missing_data_df=[]\n",
    "missing_data_nova=[]\n",
    "t0= time.time()\n",
    "df_accur = pd.DataFrame(columns =['codigoSerial', 'dist', 'acc_nova', 'acc_df'])\n",
    "df_comple = pd.DataFrame(columns =[\"codigoSerial\",\"completeness_df\",\"completeness_nova\"])\n",
    "df_preci = pd.DataFrame(columns =[\"codigoSerial\",\"precision_df\",\"precision_nova\",])\n",
    "df_uncer = pd.DataFrame(columns =[\"codigoSerial\",\"uncertainty\"])\n",
    "df_conco = pd.DataFrame(columns =[\"codigoSerial\",\"concordance_df_nova\",\"concordance_df_siata\",\"concordance_df_hum\",\"concordance_df_temp\",\"concordance_nova_siata\",\"concordance_nova_hum\",\"concordance_nova_temp\"])\n",
    "\n",
    "contador=0\n",
    "for nubes in CC.keys():\n",
    "    contador+=1\n",
    "    #CC[nube][\"v_pm25\"] = np.nan\n",
    "    #CC[nube][\"alpha_df\"] = np.nan\n",
    "    #CC[nube][\"alpha_nova\"] = np.nan\n",
    "    #del df_window\n",
    "    \n",
    "    df_window=clean_data(nubes, CC, start_time, end_time)\n",
    "    #PRECISION\n",
    "    df_preci=df_preci.append(precision(nubes, df_window), ignore_index = True)\n",
    "    print(\"%d. Nube: %d, Overall relative (Precision) Standard Deviation.\"%(contador,nubes))\n",
    "    \n",
    "    #UNCERTAINTY\n",
    "    df_uncer=df_uncer.append(uncertainty(nubes, df_window), ignore_index = True)\n",
    "    print(\"%d. Nube: %d, Overall relative Uncertainty BS, \"%(contador,nubes))\n",
    "    \n",
    "    #ACCURACY\n",
    "    df_accur=df_accur.append(accuracy(nubes, df_window), ignore_index = True)\n",
    "    print(\"%d. Nube: %d, Accuracy,\" %(contador, nubes))\n",
    "    \n",
    "    \n",
    "    #COMPLETENESS\n",
    "    df_comple=df_comple.append(completeness(nubes, df_window), ignore_index = True)\n",
    "    print(\"%d. Nube: %d, Completeness.\" %(contador,nubes))\n",
    "\n",
    "    #COMPARABILITY / Concordance\n",
    "    df_conco=df_conco.append(concordance(nubes,df_window),ignore_index = True)\n",
    "    print(\"%d. Nube: %d, Overall concordance, \"%(contador,nubes))\n",
    "t1 = time.time() - t0\n",
    "print(\"Time elapsed: \", t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST CODE TO CHECK THE FUNCTIONS\n",
    "\n",
    "node=90\n",
    "df_window=clean_data(node, CC, start_time, end_time)\n",
    "#df_preci = pd.DataFrame(columns =[\"codigoSerial\",\"precision_df\",\"precision_nova\",])\n",
    "#df_uncer = pd.DataFrame(columns =[\"codigoSerial\",\"uncertainty\",\"uncertainty_group\"])\n",
    "#df_conco = pd.DataFrame(columns =[\"codigoSerial\",\"concordance_df_nova\",\"concordance_df_siata\",\"concordance_df_hum\",\"concordance_df_temp\",\"concordance_nova_siata\",\"concordance_nova_hum\",\"concordance_nova_temp\"])\n",
    "#df_accur = pd.DataFrame(columns =['codigoSerial', 'dist', 'acc_nova', 'acc_df'])\n",
    "#df_comple = pd.DataFrame(columns =[\"codigoSerial\",\"completeness_df\",\"completeness_nova\"])\n",
    "\n",
    "\n",
    "df_comple=df_comple.append(completeness(node, df_window), ignore_index = True)\n",
    "df_comple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallelization CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citizen Scientist:  [1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 259, 261, 262, 265, 266, 267]\n",
      "Siata Stations:  [11, 12, 25, 28, 31, 37, 38, 44, 46, 48, 69, 6, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 94]\n",
      "     codigoSerial  precision_df  precision_nova\n",
      "225         223.0           NaN             NaN\n",
      "226          85.0           NaN             NaN\n",
      "227         184.0           NaN             NaN\n",
      "228         117.0           NaN             NaN\n",
      "229         227.0           NaN             NaN\n",
      "     codigoSerial  uncertainty\n",
      "225         223.0          NaN\n",
      "226          85.0          NaN\n",
      "227         184.0          NaN\n",
      "228         117.0          NaN\n",
      "229         227.0          NaN\n",
      "     codigoSerial      dist  acc_nova  acc_df\n",
      "225         223.0  1.572266       NaN     NaN\n",
      "226          85.0  1.510742       NaN     NaN\n",
      "227         184.0  0.623047       NaN     NaN\n",
      "228         117.0  0.687012       NaN     NaN\n",
      "229         227.0  0.789551       NaN     NaN\n",
      "     codigoSerial  completeness_df  completeness_nova\n",
      "225         223.0              0.0                0.0\n",
      "226          85.0              0.0                0.0\n",
      "227         184.0              0.0                0.0\n",
      "228         117.0              0.0                0.0\n",
      "229         227.0              0.0                0.0\n",
      "     codigoSerial  concordance_df_nova  concordance_df_siata  \\\n",
      "225         223.0                  NaN                   NaN   \n",
      "226          85.0                  NaN                   NaN   \n",
      "227         184.0                  NaN                   NaN   \n",
      "228         117.0                  NaN                   NaN   \n",
      "229         227.0                  NaN                   NaN   \n",
      "\n",
      "     concordance_df_hum  concordance_df_temp  concordance_nova_siata  \\\n",
      "225                 NaN                  NaN                     NaN   \n",
      "226                 NaN                  NaN                     NaN   \n",
      "227                 NaN                  NaN                     NaN   \n",
      "228                 NaN                  NaN                     NaN   \n",
      "229                 NaN                  NaN                     NaN   \n",
      "\n",
      "     concordance_nova_hum  concordance_nova_temp  \n",
      "225                   NaN                    NaN  \n",
      "226                   NaN                    NaN  \n",
      "227                   NaN                    NaN  \n",
      "228                   NaN                    NaN  \n",
      "229                   NaN                    NaN  \n",
      "Elapsed Time :  221.39269423484802\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sn\n",
    "import requests\n",
    "import json\n",
    "import haversine as hs\n",
    "import wx\n",
    "\n",
    "#This is the base code that worked when testing multiprocessing!\n",
    "#import multiprocessing as mp\n",
    "##from multiprocessing import Pool\n",
    "#import workers\n",
    "#if __name__ ==  '__main__': \n",
    "# pool=mp.Pool(processes = mp.cpu_count())\n",
    "# pool.map(workers.worker,[i for i in range(0,3000)])\n",
    "# #print(output)\n",
    "\n",
    "import DQ\n",
    "t0= time.time()\n",
    "if __name__ ==  '__main__':\n",
    "    pool=mp.Pool(processes = mp.cpu_count())\n",
    "    results=pool.map(DQ.eval_dq,[nodes for nodes in CC.keys()])\n",
    "    \n",
    "    df_preci = pd.DataFrame(columns =[\"codigoSerial\",\"precision_df\",\"precision_nova\",])\n",
    "    df_uncer = pd.DataFrame(columns =[\"codigoSerial\",\"uncertainty\"])\n",
    "    df_conco = pd.DataFrame(columns =[\"codigoSerial\",\"concordance_df_nova\",\"concordance_df_siata\",\"concordance_df_hum\",\"concordance_df_temp\",\"concordance_nova_siata\",\"concordance_nova_hum\",\"concordance_nova_temp\"])\n",
    "    df_accur = pd.DataFrame(columns =['codigoSerial', 'dist', 'acc_nova', 'acc_df'])\n",
    "    df_comple = pd.DataFrame(columns =[\"codigoSerial\",\"completeness_df\",\"completeness_nova\"])\n",
    "    \n",
    "    for i in range(0,len(results)):\n",
    "        df_preci=df_preci.append(results[i][0], ignore_index = True)\n",
    "        df_uncer=df_uncer.append(results[i][1], ignore_index = True)\n",
    "        df_accur=df_accur.append(results[i][2], ignore_index = True)\n",
    "        df_comple=df_comple.append(results[i][3], ignore_index = True)\n",
    "        df_conco=df_conco.append(results[i][4],ignore_index = True)\n",
    "    \n",
    "    print(df_preci.tail())\n",
    "    print(df_uncer.tail())\n",
    "    print(df_accur.tail())\n",
    "    print(df_comple.tail())\n",
    "    print(df_conco.tail())\n",
    "t1 = time.time() - t0\n",
    "print(\"Elapsed Time : \", t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-02-03 10:00:00')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST CODE TO PLAY WITH STRINGS AND TIMESTAMPS FORMATS\n",
    "from datetime import datetime\n",
    "inicio=\"2020-02-03 10:59:00\"\n",
    "datetime_str = inicio\n",
    "\n",
    "ts = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "ts=pd.Timestamp(datetime_str).floor('60min')\n",
    "\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 'a'], [2, 4, 'a'], [3, 6, 'a'], [4, 8, 'a']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[i, i*2,\"a\"] for i in range(1,5)]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f295ec510066dddcc717ae6d4935b3ad0a45b533511561df7ff2b4bcd803903f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
