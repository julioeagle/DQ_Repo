{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to install if not installed yet\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install sklearn\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install haversine\n",
    "!pip install -U wxPython \n",
    "!pip install google\n",
    "!pip install google-api-core\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install google-cloud\n",
    "!pip install google-cloud-vision\n",
    "!pip install google.cloud.bigquery\n",
    "!pip install google.cloud.storage\n",
    "!pip install google-auth-oauthlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sn\n",
    "import requests\n",
    "import json\n",
    "import haversine as hs\n",
    "import wx\n",
    "import webbrowser\n",
    "\n",
    "from __future__ import print_function\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "import DQ2# Own defined\n",
    "#hola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Setup file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights\n",
    "#mu                =0.9\n",
    "#Accuracy          =0.20\n",
    "#Precision         =0.07\n",
    "#Confidence        =0.16\n",
    "#Completeness      =0.10\n",
    "#Timeliness        =0.12\n",
    "#Data_Volume       =0.16\n",
    "#Data_Redundancy   =0.02\n",
    "#Concordance       =0.16\n",
    "#\n",
    "#Utility           =0.12\n",
    "#Accessibility     =0.16\n",
    "#Interpretability  =0.28\n",
    "#Reputation        =0.12\n",
    "#Artificiality     =0.20\n",
    "#Access_Security   =0.12\n",
    "\n",
    "\n",
    "mu              = 1.0\n",
    "Waccuracy        = 0.3506311521\n",
    "Wprecision       = 0.09875987987\n",
    "Wconfidence      = 0.1880884436\n",
    "Wcompleteness    = 0.148093351\n",
    "Wdata_Redundancy = 0.03756434625\n",
    "Wconcordance     = 0.1768628272\n",
    "pcmWeights = [Waccuracy,Wprecision,Wconfidence,Wcompleteness,Wdata_Redundancy,Wconcordance]\n",
    "\n",
    "#Period\n",
    "start_time =\"2019-12-01 00:00:00\"\n",
    "end_time   =\"2020-03-29 23:59:00\"\n",
    "\n",
    "#Period for synthetic dataset 1: sinusoid-based\n",
    "#start_time =\"2021-10-03 00:00:00\"\n",
    "#end_time   =\"2021-10-04 23:59:00\"\n",
    "\n",
    "#Period for synthetic dataset 2: based on real data.\n",
    "#start_time =\"2021-10-05 00:00:00\"\n",
    "#end_time   =\"2021-10-07 23:59:00\"\n",
    "\n",
    "#Variable Inicialization\n",
    "p=99 # P for the CI used in the confidence calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps should be like:\n",
    "\n",
    "0. Clean the whole dataset (the variables of interest):DONE\n",
    "1. For each citizen science (CC) node, get the groups (HOURLY GROUPS).\n",
    "2. For each group (hour in a CC node data), calculate the Dimension's DQ. (The functions should be applied to each group instead)\n",
    "3. Save the result file in the form: Node, Group (hour), DQ_1, DQ_2, DQ3, ... , DQIndex  (This is new)\n",
    "4. Average the previous result over the whole time to get Node, DQ_1_time_mean, DQ_2_time_mean, DQ3_ave, ... , DQIndex_time_mean (This it what we have currently)\n",
    "5. Average the previous result over all the nodes to get DQ_1_node_mean, DQ_2_node_mean, DQ3_node_mean, ... , DQIndex_node_mean (This is new)\n",
    "6. Export 3, 4 and 5 to a Google Sheets page each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path for Citizen Science nodes data:  C:\\Users\\julio\\Documents\\UDEA\\Maestría\\DQ in IOT\\Datasets\\Dataset for testing\\Real\\CC_PM_January.csv\n",
      "Source path for Siata Stations data:  C:\\Users\\julio\\Documents\\UDEA\\Maestría\\DQ in IOT\\Datasets\\Dataset for testing\\Real\\SS_PM_January till March.csv\n",
      "The distance files was read\n",
      "Citizen Scientist:  [1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 157, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 179, 180, 181, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255, 257, 259, 261, 262, 265, 266, 267]\n",
      "Siata Stations:  [11, 12, 25, 28, 31, 37, 38, 44, 46, 48, 69, 6, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 94]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "#Read Data from February\n",
    "    header_CC=[\"codigoSerial\", \"fecha\", \"hora\", \"fechaHora\", \"temperatura\", \"humedad_relativa\", \"pm1_df\", \"pm10_df\", \"pm25_df\", \"pm1_nova\", \"pm10_nova\", \"pm25_nova\", \"calidad_temperatura\", \"calidad_humedad_relativa\", \"calidad_pm1_df\", \"calidad_pm10_df\", \"calidad_pm25_df\", \"calidad_pm1_nova\", \"calidad_pm10_nova\", \"calidad_pm25_nova\"]\n",
    "    datatypes_CC={\"codigoSerial\":np.uint16, \"temperatura\":np.float16, \"humedad_relativa\":np.float16, \"pm1_df\":np.float32, \"pm10_df\":np.float32, \"pm25_df\":np.float32, \"pm1_nova\":np.float32, \"pm10_nova\":np.float32, \"pm25_nova\":np.float32}\n",
    "    path_for_CC_data=DQ2.get_path('*.csv',\"Select Citizen Scientist *.csv file\")\n",
    "    #print(path_for_CC_data)\n",
    "    df_CC = pd.read_csv(path_for_CC_data, header=None, names=header_CC, usecols=header_CC , dtype=datatypes_CC,parse_dates=[\"fecha\",\"hora\",\"fechaHora\"])\n",
    "    #print(df_CC)\n",
    "    df_CC.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "    print(\"Source path for Citizen Science nodes data: \",path_for_CC_data)\n",
    "    \n",
    "    #Data includes January, February and March\n",
    "    header_SS=[\"Fecha_Hora\",\"codigoSerial\",\"pm25\",\"calidad_pm25\",\"pm10\",\"calidad_pm10\"]\n",
    "    datatypes_SS={\"codigoSerial\":np.uint16,\"pm25\":np.float32,\"pm10\":np.float32}\n",
    "    path_for_SS_data=DQ2.get_path('*.csv',\"Select SIATA Stations *.csv file\")\n",
    "    df_SS = pd.read_csv(path_for_SS_data, header=None,names=header_SS, usecols=header_SS , dtype=datatypes_SS,parse_dates=[\"Fecha_Hora\"])\n",
    "    df_SS.sort_values(by=['codigoSerial','Fecha_Hora'],ignore_index=True)\n",
    "    print(\"Source path for Siata Stations data: \",path_for_SS_data)\n",
    "    \n",
    "    \n",
    "    datatypesDistances={\"codigoSerial_CC\":np.uint16,\"codigoSerial_ES\":np.uint16,\"Distancia_a_ES\":np.float16,\"codigoSerial_ES2\":np.uint16}\n",
    "    path_for_distance_files=DQ2.get_path('*.csv',\"Select the Nodes to Siata Stations distances *.csv file\")\n",
    "    Distances = pd.read_csv(path_for_distance_files, header=0, dtype=datatypesDistances,index_col=\"codigoSerial_CC\")\n",
    "    print(\"The distance files was read\")\n",
    "    \n",
    "        \n",
    "except:\n",
    "    print(\"An exception occurred, it is possible that wrong files were chosen, please run again\")\n",
    "\n",
    "\n",
    "\n",
    "#DATA CLEANING\n",
    "CC, SS=DQ2.clean_sort_data(df_CC, df_SS)\n",
    "del df_CC\n",
    "del df_SS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Code with parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2021-10-06 21:20:58.111579\n",
      "Number of avaliable CPUs:  6\n",
      "End Time:  2021-10-06 21:27:47.068938\n",
      "Elapsed Time:  408.9729845523834  Seconds, or  6.81621640920639  Minutes\n"
     ]
    }
   ],
   "source": [
    "import DQ2# Own defined\n",
    "t0= time.time()\n",
    "print(\"Start time: \", datetime.fromtimestamp(t0))\n",
    "\n",
    "\n",
    "dim_time = pd.DataFrame(\n",
    "        columns =[\"codigoSerial\",\n",
    "                  \"fechaHora\",\n",
    "                  \"precision_df_time\",\n",
    "                  \"precision_nova_time\",\n",
    "                  \"uncertainty_time\",\n",
    "                  \"accuracy_df_time\",\n",
    "                  \"accuracy_nova_time\",\n",
    "                  \"completeness_df_time\",\n",
    "                  \"completeness_nova_time\",\n",
    "                  \"concordance_df_nova_time\",\n",
    "                  \"concordance_df_siata_time\",\n",
    "                  \"concordance_df_hum_time\",\n",
    "                  \"concordance_df_temp_time\",\n",
    "                  \"concordance_nova_siata_time\",\n",
    "                  \"concordance_nova_hum_time\",\n",
    "                  \"concordance_nova_temp_time\",\n",
    "                  \"vm_df\",\n",
    "                  \"vm_nova\",\n",
    "                  \"v\",\n",
    "                  \"duplicates_time\",\n",
    "                  \n",
    "                  \"confi_df_time\",\n",
    "                  \"confi_nova_time\"])\n",
    "\n",
    "dim_node = pd.DataFrame(\n",
    "        columns =[\"codigoSerial\",\n",
    "                  \"precision_df_node\",\n",
    "                  \"precision_nova_node\",\n",
    "                  \"uncertainty_node\",\n",
    "                  \"accuracy_df_node\",\n",
    "                  \"accuracy_nova_node\",\n",
    "                  \"completeness_df_node\",\n",
    "                  \"completeness_nova_node\",\n",
    "                  \"concordance_df_nova_node\",\n",
    "                  \"concordance_df_siata_node\",\n",
    "                  \"concordance_df_hum_node\",\n",
    "                  \"concordance_df_temp_node\",\n",
    "                  \"concordance_nova_siata_node\",\n",
    "                  \"concordance_nova_hum_node\",\n",
    "                  \"concordance_nova_temp_node\",\n",
    "                  \n",
    "                  \"duplicates_node\",\n",
    "                  \n",
    "                  \"confi_df_node\",\n",
    "                  \"confi_nova_node\",\n",
    "                  \"DQ_INDEX_NODE\"])\n",
    "\n",
    "dim_DQ = pd.DataFrame(\n",
    "        columns =[\"precision_df_total\",\n",
    "                  \"precision_nova_total\",\n",
    "                  \"uncertainty_total\",\n",
    "                  \"accuracy_df_total\",\n",
    "                  \"accuracy_nova_total\",\n",
    "                  \"completeness_df_total\",\n",
    "                  \"completeness_nova_total\",\n",
    "                  \"concordance_df_nova_total\",\n",
    "                  \"concordance_df_siata_total\",#MAYBE NEED TO BE CALCULATED ON A DAILY BASIS\n",
    "                  \"concordance_df_hum_total\",\n",
    "                  \"concordance_df_temp_total\",\n",
    "                  \"concordance_nova_siata_total\",#MAYBE NEED TO BE CALCULATED ON A DAILY BASIS\n",
    "                  \"concordance_nova_hum_total\",\n",
    "                  \"concordance_nova_temp_total\",\n",
    "                  \n",
    "                  \"duplicates_total\",\n",
    "                  \n",
    "                  \"confi_df_total\",\n",
    "                  \"confi_nova_total\",\n",
    "                  \"DQ_INDEX_TOTAL\"])\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    print(\"Number of avaliable CPUs: \",mp.cpu_count())\n",
    "    pool=mp.Pool(processes = mp.cpu_count())\n",
    "    arguments=[]\n",
    "    #results=pool.map(DQ.eval_dq,[nodes for nodes in CC.keys()])\n",
    "    results=pool.map(DQ2.eval_dq,([[nodes, CC, SS, Distances, start_time, end_time, p] for nodes in CC.keys()]))\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(0,len(results)):\n",
    "        dim_time=dim_time.append(results[i][0], ignore_index = True)\n",
    "        dim_node=dim_node.append(results[i][1], ignore_index = True)\n",
    "\n",
    "        \n",
    "    cols =[       \"precision_df_node\",\n",
    "                  \"precision_nova_node\",\n",
    "                  \"uncertainty_node\",\n",
    "                  \"accuracy_df_node\",\n",
    "                  \"accuracy_nova_node\",\n",
    "                  \"completeness_df_node\",\n",
    "                  \"completeness_nova_node\",\n",
    "                  \"concordance_df_nova_node\",\n",
    "                  \"concordance_df_siata_node\",\n",
    "                  \"concordance_df_hum_node\",\n",
    "                  \"concordance_df_temp_node\",\n",
    "                  \"concordance_nova_siata_node\",\n",
    "                  \"concordance_nova_hum_node\",\n",
    "                  \"concordance_nova_temp_node\",\n",
    "                  \n",
    "                  \"duplicates_node\",\n",
    "                  \n",
    "                  \"confi_df_node\",\n",
    "                  \"confi_nova_node\"]    \n",
    "    dim_DQ= dim_node[cols].mean()\n",
    "    dim_DQ.rename({'precision_df_node':          'precision_df_total', \n",
    "                   'precision_nova_node':        'precision_nova_total' , \n",
    "                   'uncertainty_node':           'uncertainty_total' , \n",
    "                   'accuracy_df_node':           'accuracy_df_total', \n",
    "                   'accuracy_nova_node':         'accuracy_nova_total', \n",
    "                   'completeness_df_node':       'completeness_df_total', \n",
    "                   'completeness_nova_node':     'completeness_nova_total', \n",
    "                   'concordance_df_nova_node':   'concordance_df_nova_total', \n",
    "                   'concordance_df_siata_node':  'concordance_df_siata_total', \n",
    "                   'concordance_df_hum_node':    'concordance_df_hum_total', \n",
    "                   'concordance_df_temp_node':   'concordance_df_temp_total', \n",
    "                   'concordance_nova_siata_node':'concordance_nova_siata_total', \n",
    "                   'concordance_nova_hum_node':  'concordance_nova_hum_total', \n",
    "                   'concordance_nova_temp_node': 'concordance_nova_temp_total', \n",
    "                   'duplicates_node':            'duplicates_total', \n",
    "                   'confi_df_node':              'confi_df_total', \n",
    "                   'confi_nova_node':            'confi_nova_total'        }, axis=1, inplace=True)\n",
    "    \n",
    "print(\"End Time: \", datetime.fromtimestamp(time.time()))\n",
    "t1 = time.time() - t0\n",
    "print(\"Elapsed Time: \", t1, \" Seconds, or \",t1/60,\" Minutes\")\n",
    "#print(dim_node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQ_INDEX by a weighted average function. The weights come from the Pair-Wise Comparison Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codigoSerial</th>\n",
       "      <th>fechaHora</th>\n",
       "      <th>precision_df_time</th>\n",
       "      <th>precision_nova_time</th>\n",
       "      <th>uncertainty_time</th>\n",
       "      <th>accuracy_df_time</th>\n",
       "      <th>accuracy_nova_time</th>\n",
       "      <th>completeness_df_time</th>\n",
       "      <th>completeness_nova_time</th>\n",
       "      <th>concordance_df_nova_time</th>\n",
       "      <th>concordance_df_siata_time</th>\n",
       "      <th>concordance_df_hum_time</th>\n",
       "      <th>concordance_df_temp_time</th>\n",
       "      <th>concordance_nova_siata_time</th>\n",
       "      <th>concordance_nova_hum_time</th>\n",
       "      <th>concordance_nova_temp_time</th>\n",
       "      <th>duplicates_time</th>\n",
       "      <th>confi_df_time</th>\n",
       "      <th>confi_nova_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>0.952312</td>\n",
       "      <td>0.867546</td>\n",
       "      <td>0.759337</td>\n",
       "      <td>0.76017</td>\n",
       "      <td>0.975183</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.712940</td>\n",
       "      <td>0.672423</td>\n",
       "      <td>0.570221</td>\n",
       "      <td>0.482630</td>\n",
       "      <td>0.710340</td>\n",
       "      <td>0.247317</td>\n",
       "      <td>0.393497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.878351</td>\n",
       "      <td>0.89411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172</td>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "      <td>0.945496</td>\n",
       "      <td>0.919384</td>\n",
       "      <td>0.818442</td>\n",
       "      <td>0.781845</td>\n",
       "      <td>0.976976</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.654841</td>\n",
       "      <td>0.672423</td>\n",
       "      <td>0.222529</td>\n",
       "      <td>0.454074</td>\n",
       "      <td>0.710340</td>\n",
       "      <td>0.190015</td>\n",
       "      <td>0.086152</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.87177</td>\n",
       "      <td>0.903431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172</td>\n",
       "      <td>2020-01-01 02:00:00</td>\n",
       "      <td>0.95144</td>\n",
       "      <td>0.904878</td>\n",
       "      <td>0.79786</td>\n",
       "      <td>0.801654</td>\n",
       "      <td>0.99897</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.344265</td>\n",
       "      <td>0.672423</td>\n",
       "      <td>0.133630</td>\n",
       "      <td>0.485913</td>\n",
       "      <td>0.710340</td>\n",
       "      <td>0.106588</td>\n",
       "      <td>0.523308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.851945</td>\n",
       "      <td>0.877694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>172</td>\n",
       "      <td>2020-01-01 03:00:00</td>\n",
       "      <td>0.887743</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.881169</td>\n",
       "      <td>0.627896</td>\n",
       "      <td>0.673021</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.874745</td>\n",
       "      <td>0.672423</td>\n",
       "      <td>0.174548</td>\n",
       "      <td>0.794154</td>\n",
       "      <td>0.710340</td>\n",
       "      <td>0.619027</td>\n",
       "      <td>0.721271</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.858437</td>\n",
       "      <td>0.886617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>2020-01-01 04:00:00</td>\n",
       "      <td>0.868645</td>\n",
       "      <td>0.852931</td>\n",
       "      <td>0.850105</td>\n",
       "      <td>0.314646</td>\n",
       "      <td>0.569653</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.941723</td>\n",
       "      <td>0.672423</td>\n",
       "      <td>0.106346</td>\n",
       "      <td>0.334818</td>\n",
       "      <td>0.710340</td>\n",
       "      <td>0.223057</td>\n",
       "      <td>0.168832</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916915</td>\n",
       "      <td>0.908199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124513</th>\n",
       "      <td>230</td>\n",
       "      <td>2020-01-03 14:00:00</td>\n",
       "      <td>0.95706</td>\n",
       "      <td>0.95562</td>\n",
       "      <td>0.957804</td>\n",
       "      <td>0.364509</td>\n",
       "      <td>0.352391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.354637</td>\n",
       "      <td>0.165081</td>\n",
       "      <td>0.546437</td>\n",
       "      <td>0.084186</td>\n",
       "      <td>0.317373</td>\n",
       "      <td>0.278460</td>\n",
       "      <td>0.026350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90484</td>\n",
       "      <td>0.901568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124514</th>\n",
       "      <td>230</td>\n",
       "      <td>2020-01-03 15:00:00</td>\n",
       "      <td>0.957368</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.947695</td>\n",
       "      <td>0.329242</td>\n",
       "      <td>0.311003</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521262</td>\n",
       "      <td>0.165081</td>\n",
       "      <td>0.320492</td>\n",
       "      <td>0.397024</td>\n",
       "      <td>0.317373</td>\n",
       "      <td>0.361606</td>\n",
       "      <td>0.299712</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.901998</td>\n",
       "      <td>0.89625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124515</th>\n",
       "      <td>230</td>\n",
       "      <td>2020-01-03 16:00:00</td>\n",
       "      <td>0.94349</td>\n",
       "      <td>0.966873</td>\n",
       "      <td>0.959616</td>\n",
       "      <td>0.389411</td>\n",
       "      <td>0.381866</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.374192</td>\n",
       "      <td>0.165081</td>\n",
       "      <td>0.488074</td>\n",
       "      <td>0.469310</td>\n",
       "      <td>0.317373</td>\n",
       "      <td>0.196362</td>\n",
       "      <td>0.198655</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891529</td>\n",
       "      <td>0.889385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124516</th>\n",
       "      <td>230</td>\n",
       "      <td>2020-01-03 17:00:00</td>\n",
       "      <td>0.960398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973535</td>\n",
       "      <td>0.342061</td>\n",
       "      <td>0.338157</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.165081</td>\n",
       "      <td>0.226214</td>\n",
       "      <td>0.184637</td>\n",
       "      <td>0.317373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.630236</td>\n",
       "      <td>0.625968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124517</th>\n",
       "      <td>254</td>\n",
       "      <td>2020-01-02 12:00:00</td>\n",
       "      <td>0.931683</td>\n",
       "      <td>0.965219</td>\n",
       "      <td>0.96093</td>\n",
       "      <td>0.854057</td>\n",
       "      <td>0.873123</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.912013</td>\n",
       "      <td>0.910524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124518 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       codigoSerial           fechaHora precision_df_time precision_nova_time  \\\n",
       "0               172 2020-01-01 00:00:00          0.952312            0.867546   \n",
       "1               172 2020-01-01 01:00:00          0.945496            0.919384   \n",
       "2               172 2020-01-01 02:00:00           0.95144            0.904878   \n",
       "3               172 2020-01-01 03:00:00          0.887743            0.825893   \n",
       "4               172 2020-01-01 04:00:00          0.868645            0.852931   \n",
       "...             ...                 ...               ...                 ...   \n",
       "124513          230 2020-01-03 14:00:00           0.95706             0.95562   \n",
       "124514          230 2020-01-03 15:00:00          0.957368            0.946237   \n",
       "124515          230 2020-01-03 16:00:00           0.94349            0.966873   \n",
       "124516          230 2020-01-03 17:00:00          0.960398                 1.0   \n",
       "124517          254 2020-01-02 12:00:00          0.931683            0.965219   \n",
       "\n",
       "       uncertainty_time accuracy_df_time accuracy_nova_time  \\\n",
       "0              0.759337          0.76017           0.975183   \n",
       "1              0.818442         0.781845           0.976976   \n",
       "2               0.79786         0.801654            0.99897   \n",
       "3              0.881169         0.627896           0.673021   \n",
       "4              0.850105         0.314646           0.569653   \n",
       "...                 ...              ...                ...   \n",
       "124513         0.957804         0.364509           0.352391   \n",
       "124514         0.947695         0.329242           0.311003   \n",
       "124515         0.959616         0.389411           0.381866   \n",
       "124516         0.973535         0.342061           0.338157   \n",
       "124517          0.96093         0.854057           0.873123   \n",
       "\n",
       "        completeness_df_time  completeness_nova_time  \\\n",
       "0                   0.250000                0.533333   \n",
       "1                   0.233333                0.583333   \n",
       "2                   0.166667                0.350000   \n",
       "3                   0.300000                0.500000   \n",
       "4                   0.850000                0.966667   \n",
       "...                      ...                     ...   \n",
       "124513              1.000000                1.000000   \n",
       "124514              1.000000                1.000000   \n",
       "124515              0.933333                0.933333   \n",
       "124516              0.083333                0.083333   \n",
       "124517              0.066667                0.066667   \n",
       "\n",
       "        concordance_df_nova_time  concordance_df_siata_time  \\\n",
       "0                       0.712940                   0.672423   \n",
       "1                       0.654841                   0.672423   \n",
       "2                       0.344265                   0.672423   \n",
       "3                       0.874745                   0.672423   \n",
       "4                       0.941723                   0.672423   \n",
       "...                          ...                        ...   \n",
       "124513                  0.354637                   0.165081   \n",
       "124514                  0.521262                   0.165081   \n",
       "124515                  0.374192                   0.165081   \n",
       "124516                       NaN                   0.165081   \n",
       "124517                  0.471405                        NaN   \n",
       "\n",
       "        concordance_df_hum_time  concordance_df_temp_time  \\\n",
       "0                      0.570221                  0.482630   \n",
       "1                      0.222529                  0.454074   \n",
       "2                      0.133630                  0.485913   \n",
       "3                      0.174548                  0.794154   \n",
       "4                      0.106346                  0.334818   \n",
       "...                         ...                       ...   \n",
       "124513                 0.546437                  0.084186   \n",
       "124514                 0.320492                  0.397024   \n",
       "124515                 0.488074                  0.469310   \n",
       "124516                 0.226214                  0.184637   \n",
       "124517                      NaN                       NaN   \n",
       "\n",
       "        concordance_nova_siata_time  concordance_nova_hum_time  \\\n",
       "0                          0.710340                   0.247317   \n",
       "1                          0.710340                   0.190015   \n",
       "2                          0.710340                   0.106588   \n",
       "3                          0.710340                   0.619027   \n",
       "4                          0.710340                   0.223057   \n",
       "...                             ...                        ...   \n",
       "124513                     0.317373                   0.278460   \n",
       "124514                     0.317373                   0.361606   \n",
       "124515                     0.317373                   0.196362   \n",
       "124516                     0.317373                        NaN   \n",
       "124517                          NaN                        NaN   \n",
       "\n",
       "        concordance_nova_temp_time  duplicates_time confi_df_time  \\\n",
       "0                         0.393497              1.0      0.878351   \n",
       "1                         0.086152              1.0       0.87177   \n",
       "2                         0.523308              1.0      0.851945   \n",
       "3                         0.721271              1.0      0.858437   \n",
       "4                         0.168832              1.0      0.916915   \n",
       "...                            ...              ...           ...   \n",
       "124513                    0.026350              1.0       0.90484   \n",
       "124514                    0.299712              1.0      0.901998   \n",
       "124515                    0.198655              1.0      0.891529   \n",
       "124516                         NaN              1.0      0.630236   \n",
       "124517                         NaN              1.0      0.912013   \n",
       "\n",
       "       confi_nova_time  \n",
       "0              0.89411  \n",
       "1             0.903431  \n",
       "2             0.877694  \n",
       "3             0.886617  \n",
       "4             0.908199  \n",
       "...                ...  \n",
       "124513        0.901568  \n",
       "124514         0.89625  \n",
       "124515        0.889385  \n",
       "124516        0.625968  \n",
       "124517        0.910524  \n",
       "\n",
       "[124518 rows x 19 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_node[\"DQ_INDEX_NODE\"]=  Wprecision*dim_node.loc[:,[\"precision_df_node\",\"precision_nova_node\"]].mean(axis=1)+\\\n",
    "                            Waccuracy*dim_node.loc[:,[\"accuracy_df_node\",\"accuracy_nova_node\"]].mean(axis=1)+\\\n",
    "                            Wcompleteness*dim_node.loc[:,[\"completeness_df_node\",\"completeness_nova_node\"]].mean(axis=1)+\\\n",
    "                            Wconfidence*dim_node.loc[:,[\"confi_df_node\",\"confi_nova_node\"]].mean(axis=1)+\\\n",
    "                            Wconcordance*dim_node.loc[:,[\"concordance_df_nova_node\",\"concordance_df_siata_node\",\"concordance_nova_siata_node\"]].mean(axis=1)+\\\n",
    "                            Wdata_Redundancy*dim_node.loc[:,[\"duplicates_node\"]].mean(axis=1)\n",
    "\n",
    "#dim_node[\"precision_node\"]= Wprecision*dim_node.loc[:,[\"precision_df_node\",\"precision_nova_node\"]].mean(axis=1)\n",
    "#dim_node[\"accuracy_node\"]= Waccuracy*dim_node.loc[:,[\"accuracy_df_node\",\"accuracy_nova_node\"]].mean(axis=1)\n",
    "#dim_node[\"completeness_node\"]= Wcompleteness*dim_node.loc[:,[\"completeness_df_node\",\"completeness_nova_node\"]].mean(axis=1)\n",
    "#dim_node[\"confidence_node\"]= Wconfidence*dim_node.loc[:,[\"confi_df_node\",\"confi_nova_node\"]].mean(axis=1)\n",
    "#dim_node[\"concordance_node\"]= Wconcordance*dim_node.loc[:,[\"concordance_df_nova_node\",\"concordance_df_siata_node\",\"concordance_nova_siata_node\"]].mean(axis=1)\n",
    "#dim_node[\"redundancy_node\"]= Wdata_Redundancy*dim_node.loc[:,[\"duplicates_node\"]].mean(axis=1)\n",
    "#dim_node[\"DQ_INDEX_NODE\"]=dim_node[[\"precision_node\",\"accuracy_node\",\"completeness_node\",\"confidence_node\",\"concordance_node\",\"redundancy_node\"]].sum(axis=1)\n",
    "\n",
    "dim_DQ[\"DQ_INDEX_TOTAL\"]=   Wprecision*dim_DQ[[\"precision_df_total\",\"precision_nova_total\"]].mean()+ \\\n",
    "                            Waccuracy*dim_DQ[[\"accuracy_df_total\",\"accuracy_nova_total\"]].mean()+ \\\n",
    "                            Wcompleteness*dim_DQ[[\"completeness_df_total\",\"completeness_nova_total\"]].mean()+ \\\n",
    "                            Wconfidence*dim_DQ[[\"confi_df_total\",\"confi_nova_total\"]].mean()+ \\\n",
    "                            Wconcordance*dim_DQ[[\"concordance_df_nova_total\",\"concordance_df_siata_total\",\"concordance_nova_siata_total\"]].mean()+ \\\n",
    "                            Wdata_Redundancy*dim_DQ[[\"duplicates_total\"]].mean()\n",
    "#dim_node.head(6)\n",
    "#dim_node.loc[dim_node.codigoSerial==49,]\n",
    "\n",
    "#Need to be removed to avoid the API to crash\n",
    "dim_time.pop(\"vm_df\")\n",
    "dim_time.pop(\"vm_nova\")\n",
    "dim_time.pop(\"v\")\n",
    "#dim_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=837377790403-3fihjpcni02u85p0l7muvaui9plqetn8.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A52466%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fspreadsheets&state=8I7B5EY0yJfTaScIj8Er5l8QuJgXR0&access_type=offline\n",
      "Clearing the DQ_TIME sheet in the Google Spreadsheet\n",
      "Clearing the DQ_NODE sheet in the Google Spreadsheet\n",
      "Clearing the DQ_TOTAL sheet in the Google Spreadsheet\n",
      "Exporting Data to Google Sheets\n",
      "Exporting Data Finished\n"
     ]
    }
   ],
   "source": [
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "         #\"https://www.googleapis.com/auth/spreadsheets\"\n",
    "\n",
    "# The ID and range of a sample spreadsheet.\n",
    "#SAMPLE_SPREADSHEET_ID = '1gkU9I2EqgJCLm-XemWq_oFenkOcsaHRKwDpO2kK7vd8' #udea account\n",
    "SAMPLE_SPREADSHEET_ID = '1QlPuLYvWaJV6QmOTmkUM3BzuiCvM_8mnuAtvLiEFJaI' #new account\n",
    "#SAMPLE_RANGE_NAME = 'Class Data!A2:E'\n",
    "\n",
    "def main():\n",
    "    \"\"\"Shows basic usage of the Sheets API.\n",
    "    Prints values from a sample spreadsheet.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials2.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "    # Call the Sheets API\n",
    "    \n",
    "    dim_time.replace(np.nan, '', inplace=True)\n",
    "    dim_time['fechaHora'] = dim_time['fechaHora'].astype(str)\n",
    "    #dim_time.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "    dim_time.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "    \n",
    "    dim_node.replace(np.nan, '', inplace=True)\n",
    "    #dim_time.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "    dim_node.sort_values(by=['codigoSerial'],ignore_index=True)\n",
    "    \n",
    "    dim_DQ.replace(np.nan, '', inplace=True)\n",
    "    #dim_time.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "        \n",
    "    \n",
    "    print(\"Clearing the DQ_TIME sheet in the Google Spreadsheet\")\n",
    "    sheet = service.spreadsheets()\n",
    "    #request = service.spreadsheets().values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=range_, body=clear_values_request_body)\n",
    "    result = sheet.values().clear(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        range='DQ_TIME!A1:Z1000000',\n",
    "        \n",
    "    ).execute()\n",
    "    \n",
    "    print(\"Clearing the DQ_NODE sheet in the Google Spreadsheet\")\n",
    "    sheet = service.spreadsheets()\n",
    "    #request = service.spreadsheets().values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=range_, body=clear_values_request_body)\n",
    "    result = sheet.values().clear(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        range='DQ_NODE!A1:Z1000000',\n",
    "        \n",
    "    ).execute()\n",
    "    \n",
    "    print(\"Clearing the DQ_TOTAL sheet in the Google Spreadsheet\")\n",
    "    sheet = service.spreadsheets()\n",
    "    #request = service.spreadsheets().values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=range_, body=clear_values_request_body)\n",
    "    result = sheet.values().clear(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        range='DQ_TOTAL!A1:Z1000000',\n",
    "        \n",
    "    ).execute()\n",
    "#WRITE AGAIN IN CASE THE PROBLEM IS BECAUSE IT TAKES TOO LONG\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials2.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "    print(\"Exporting Data to Google Sheets\")\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().update(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='DQ_TIME!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=dim_time.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "    \n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().update(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='DQ_NODE!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=dim_node.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "    \n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().update(\n",
    "        spreadsheetId=SAMPLE_SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='DQ_TOTAL!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=dim_DQ.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "    \n",
    "    print(\"Exporting Data Finished\")\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Open the Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#webbrowser.open('https://docs.google.com/spreadsheets/d/1gkU9I2EqgJCLm-XemWq_oFenkOcsaHRKwDpO2kK7vd8/edit?usp=sharing')#udea\n",
    "webbrowser.open('https://docs.google.com/spreadsheets/d/1QlPuLYvWaJV6QmOTmkUM3BzuiCvM_8mnuAtvLiEFJaI/edit?usp=sharing')#data quality account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Open the Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#webbrowser.open('https://datastudio.google.com/s/ietWLq_iL-8')#\"udea account\n",
    "webbrowser.open('https://datastudio.google.com/s/hy-ZhY6eEfU')#data quality account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1=\"2020-02-03 23:59:59\"\n",
    "T2=\"2020-02-03 00:00:00\"\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "print(pd.Timestamp(T1)>=pd.Timestamp(T2).floor('1D') and pd.Timestamp(T1)<(pd.Timestamp(T2)+timedelta(minutes=1)).ceil('1D'))\n",
    "print(\"Range: [\",pd.Timestamp(T2).floor('1D'),\",\", (pd.Timestamp(T2)+timedelta(minutes=1)).ceil('1D'),\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arguments=[nodes=10, CC=CC, SS=CC, Distances=Distances, start_time=start_time, end_time=end_time]\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sn\n",
    "import requests\n",
    "import json\n",
    "import haversine as hs\n",
    "import wx\n",
    "#hola\n",
    "\n",
    "#FUNCTION TO READ THE PATH WITH A DIALOG BOX\n",
    "def get_path(wildcard, title):\n",
    "    app = wx.App(None)\n",
    "    style = wx.FD_OPEN | wx.FD_FILE_MUST_EXIST\n",
    "    dialog = wx.FileDialog(None, title, wildcard=wildcard, style=style)\n",
    "    if dialog.ShowModal() == wx.ID_OK:\n",
    "        path = dialog.GetPath()\n",
    "    else:\n",
    "        path = None\n",
    "    dialog.Destroy()\n",
    "    return path\n",
    "\n",
    "#FUNCTION TO CLEAN AND SORT THE READ DATA.\n",
    "def clean_sort_data(df_CC, df_SS):\n",
    "    \n",
    "    #ClEAN AND SORT THE CITIZEN SCIENCE DATASET\n",
    "    #Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999], [-40,70], [1,100]\n",
    "    #For PM2.5, Temperature and Relative Humidity Respectively.\n",
    "    df_CC=df_CC.copy()\n",
    "    df_CC.loc[df_CC[\"pm25_nova\"]>999,\"pm25_nova\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"pm25_nova\"]<0,\"pm25_nova\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"pm25_df\"]>999,\"pm25_df\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"pm25_df\"]<0,\"pm25_df\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"temperatura\"]>70,\"temperatura\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"temperatura\"]<-40,\"temperatura\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"humedad_relativa\"]>100,\"humedad_relativa\"]=np.nan\n",
    "    df_CC.loc[df_CC[\"humedad_relativa\"]<1,\"humedad_relativa\"]=np.nan\n",
    "\n",
    "    \n",
    "    #Remove data above the whiskers of the boxplot: i.e. anomaly data\n",
    "    Q1 = df_CC['pm25_df'].quantile(0.25)\n",
    "    Q3 = df_CC['pm25_df'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_CC.loc[df_CC[\"pm25_df\"]>=Q3 + 1.5 *IQR,\"pm25_df\"]=np.nan\n",
    "    \n",
    "    Q1 = df_CC['pm25_nova'].quantile(0.25)\n",
    "    Q3 = df_CC['pm25_nova'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_CC.loc[df_CC[\"pm25_nova\"]>=Q3 + 1.5 *IQR,\"pm25_nova\"]=np.nan\n",
    "    \n",
    "    Q1 = df_CC['temperatura'].quantile(0.25)\n",
    "    Q3 = df_CC['temperatura'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_CC.loc[df_CC[\"temperatura\"]>=Q3 + 1.5 *IQR,\"temperatura\"]=np.nan\n",
    "    \n",
    "    Q1 = df_CC['humedad_relativa'].quantile(0.25)\n",
    "    Q3 = df_CC['humedad_relativa'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_CC.loc[df_CC[\"humedad_relativa\"]>=Q3 + 1.5 *IQR,\"humedad_relativa\"]=np.nan\n",
    "    \n",
    "    #CREATE A DICTINARY CONTAINING THE DATASETS PER NODE\n",
    "    grouped=df_CC.groupby(df_CC.codigoSerial)\n",
    "    CC={}\n",
    "    print(\"Citizen Scientist: \", sorted(list(df_CC.codigoSerial.unique())))\n",
    "    for i in df_CC.codigoSerial.unique():\n",
    "        CC[i] = grouped.get_group(i).sort_values(by=['fechaHora'],ignore_index=True)\n",
    "    \n",
    "    #ClEAN AND SORT THE SIATA STATIONS DATASET\n",
    "    #Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999]\n",
    "    df_SS=df_SS.copy()\n",
    "    df_SS.loc[df_SS[\"pm25\"]>999,\"pm25\"]=np.nan\n",
    "    df_SS.loc[df_SS[\"pm25\"]<0,\"pm25\"]=np.nan\n",
    "\n",
    "    \n",
    "    #Remove data above the whiskers of the boxplot: i.e. anomaly data\n",
    "    Q1 = df_SS['pm25'].quantile(0.25)\n",
    "    Q3 = df_SS['pm25'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_SS.loc[df_SS[\"pm25\"]>=Q3 + 1.5 *IQR,\"pm25\"]=np.nan\n",
    "    \n",
    "    #CREATE A DICTINARY CONTAINING THE DATASETS PER SIATA STATION\n",
    "    grouped=df_SS.groupby(df_SS.codigoSerial)\n",
    "    SS={}\n",
    "    print(\"Siata Stations: \", list(df_SS.codigoSerial.unique()))\n",
    "    for j in df_SS.codigoSerial.unique():\n",
    "        SS[j] = grouped.get_group(j).sort_values(by=['Fecha_Hora'],ignore_index=True)\n",
    "    \n",
    "    del grouped\n",
    "    return CC, SS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Precision\n",
    "def precision(window):\n",
    "\n",
    "    prec_df=1-(window['pm25_df'].std()/window['pm25_df'].mean())# 1-Coefficient of Variation (std/mean)\n",
    "    prec_nova=1-(window['pm25_nova'].std()/window['pm25_nova'].mean())# 1-Coefficient of Variation (std/mean)\n",
    "    prec_dict_time={\"precision_df_time\":prec_df,\"precision_nova_time\":prec_nova}\n",
    "    return prec_dict_time\n",
    "\n",
    "\n",
    "#Uncertainty\n",
    "def uncertainty(window):\n",
    "    uncert=1-np.sqrt((window.pm25_df-window.pm25_nova).pow(2).mean()/2)/((window.pm25_df+window.pm25_nova).mean()/2)\n",
    "    uncer_dict_time={\"uncertainty_time\":uncert}\n",
    "    return uncer_dict_time   \n",
    "\n",
    "\n",
    "#ACCURACY\n",
    "def accuracy(node,hour, window, Distances, SS):\n",
    "    \n",
    "    pm25_df_ave=window['pm25_df'].mean()\n",
    "    pm25_nova_ave=window['pm25_nova'].mean()\n",
    "    #print(pm25_df_ave, pm25_nova_ave)\n",
    "    Closest_Station=Distances.codigoSerial_ES.loc[node]    \n",
    "    Closest_Station2=Distances.codigoSerial_ES.loc[node] \n",
    "    #print(pm25_df_ave,pm25_nova_ave,Closest_Station,Closest_Station2)\n",
    "    #print(Closest_Station in SS.keys())\n",
    "    #print(hour in SS[Closest_Station].Fecha_Hora.values, hour)\n",
    "   # print(SS[Closest_Station].Fecha_Hora)\n",
    "    \n",
    "    if (Closest_Station in SS.keys()) and (hour in SS[Closest_Station].Fecha_Hora.values):\n",
    "        \n",
    "        #print(SS[Closest_Station])\n",
    "        #df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_ave']\n",
    "        v=SS[Closest_Station].loc[SS[Closest_Station].Fecha_Hora==hour,\"pm25\"].values[0]\n",
    "        #print()\n",
    "        #print(pm25_df_ave)\n",
    "        accur_df=  max(0,1-abs(pm25_df_ave-v)/v)\n",
    "        accur_nova= max(0,1-abs(pm25_nova_ave-v)/v)\n",
    "    \n",
    "    elif (Closest_Station2 in SS.keys()) and  (hour in SS[Closest_Station2].Fecha_Hora.values):\n",
    "        \n",
    "        v=SS[Closest_Station2].loc[SS[Closest_Station2].Fecha_Hora==hour,\"pm25\"].values[0]\n",
    "        #v=SS[Closest_Station2].loc[hour,\"pm25\"]\n",
    "        accur_df=  max(0,1-abs(pm25_df_ave-v)/v)\n",
    "        accur_nova= max(0,1-abs(pm25_nova_ave-v)/v)\n",
    "    else:\n",
    "        accur_df=np.nan\n",
    "        accur_nova=np.nan\n",
    "    accu_dict_time={'accuracy_df_time':accur_df, 'accuracy_nova_time':accur_nova}\n",
    "    return accu_dict_time\n",
    "\n",
    "\n",
    "#CONCORDANCE\n",
    "def concordance(node, hour, window, Distances, SS):\n",
    "    Closest_Station=Distances.codigoSerial_ES.loc[node]\n",
    "    Closest_Station2=Distances.codigoSerial_ES.loc[node]\n",
    "    window=window.copy()\n",
    "    if (Closest_Station in SS.keys()) and (hour in SS[Closest_Station].Fecha_Hora.values):\n",
    "        \n",
    "   \n",
    "        v=SS[Closest_Station].loc[SS[Closest_Station].Fecha_Hora==hour,\"pm25\"].values[0]\n",
    "        vm=window.pm25_df.mean()\n",
    "        pair=[vm,v]\n",
    "        window.loc[:,\"v_pm25\"]=v\n",
    "        \n",
    "        corr_df =   window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[0].abs()\n",
    "        corr_nova = window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[1].abs()\n",
    "        concordance_df_nova=corr_df.pm25_nova\n",
    "        concordance_df_siata=corr_df.v_pm25\n",
    "        concordance_df_hum=corr_df.humedad_relativa\n",
    "        concordance_df_temp=corr_df.temperatura\n",
    "        concordance_nova_siata=corr_nova.v_pm25\n",
    "        concordance_nova_hum=corr_nova.humedad_relativa\n",
    "        concordance_nova_temp=corr_nova.temperatura\n",
    "   \n",
    "            \n",
    "    elif (Closest_Station2 in SS.keys()) and (hour in SS[Closest_Station2].Fecha_Hora.values):\n",
    "        \n",
    "        v=SS[Closest_Station2].loc[SS[Closest_Station2].Fecha_Hora==hour,\"pm25\"].values[0]\n",
    "        vm=window.pm25_df.mean()\n",
    "        pair=[vm,v]\n",
    "        window.loc[:,\"v_pm25\"]=v\n",
    "        \n",
    "        corr_df =   window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[0].abs()\n",
    "        corr_nova = window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[1].abs()\n",
    "        concordance_df_nova=corr_df.pm25_nova\n",
    "        concordance_df_siata=corr_df.v_pm25\n",
    "        concordance_df_hum=corr_df.humedad_relativa\n",
    "        concordance_df_temp=corr_df.temperatura\n",
    "        concordance_nova_siata=corr_nova.v_pm25\n",
    "        concordance_nova_hum=corr_nova.humedad_relativa\n",
    "        concordance_nova_temp=corr_nova.temperatura\n",
    "    else:\n",
    "        concordance_df_nova=np.nan\n",
    "        concordance_df_siata=np.nan\n",
    "        concordance_df_hum=np.nan\n",
    "        concordance_df_temp=np.nan\n",
    "        concordance_nova_siata=np.nan\n",
    "        concordance_nova_hum=np.nan\n",
    "        concordance_nova_temp=np.nan\n",
    "        \n",
    "        pair=[np.nan,np.nan]\n",
    "       \n",
    "    conco_dict={\"concordance_df_nova_time\":concordance_df_nova,\n",
    "                \"concordance_df_siata\":concordance_df_siata,\n",
    "                \"concordance_df_hum_time\":concordance_df_hum,\n",
    "                \"concordance_df_temp_time\":concordance_df_temp,\n",
    "                \"concordance_nova_siata\":concordance_nova_siata,\n",
    "                \"concordance_nova_hum_time\":concordance_nova_hum,\n",
    "                \"concordance_nova_temp_time\":concordance_nova_temp,\n",
    "                \"vm\":pair[0],\n",
    "                \"v\":pair[1]}\n",
    "    return conco_dict\n",
    "\n",
    "\n",
    "#COMPLETENESS\n",
    "def completeness(node, window,start_time, end_time):\n",
    "    \n",
    "    if window.fechaHora.min()==start_time and window.fechaHora.max()==end_time:\n",
    "        ref_date_range = pd.DataFrame(pd.date_range(start_time,end_time, freq='1Min'),columns=[\"ref_fechaHora\"])\n",
    "    \n",
    "    elif window.fechaHora.min()==start_time:\n",
    "        ref_date_range = pd.DataFrame(pd.date_range(start_time, pd.Timestamp(start_time).ceil('60min')-timedelta(minutes=1), freq='1Min'),columns=[\"ref_fechaHora\"])\n",
    "        \n",
    "    elif window.fechaHora.max()==end_time:\n",
    "        ref_date_range = pd.DataFrame(pd.date_range(pd.Timestamp(end_time).floor('60min')-timedelta(minutes=1),pd.Timestamp(end_time), freq='1Min'),columns=[\"ref_fechaHora\"])\n",
    "    \n",
    "    else:\n",
    "        ref_date_range = pd.DataFrame(pd.date_range(window.fechaHora.min().floor('60min'),window.fechaHora.min().floor('60min')+timedelta(minutes=59), freq='1Min'),columns=[\"ref_fechaHora\"])\n",
    "        \n",
    "    #Check for any missing date\n",
    "    missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(window.fechaHora),\"ref_fechaHora\"]\n",
    "    \n",
    "    #Add missing date rows\n",
    "    for missing in missing_dates:\n",
    "        window=window.append({\"codigoSerial\":node,\"fechaHora\":missing}, ignore_index = True)\n",
    "    #Check for missing data\n",
    "    missing_data_df=np.count_nonzero(np.isnan(window['pm25_df']))\n",
    "    missing_data_nova=np.count_nonzero(np.isnan(window['pm25_nova']))\n",
    "    comp_df=(1-missing_data_df/np.size(window.pm25_df))\n",
    "    comp_nova=(1-missing_data_nova/np.size(window.pm25_nova))\n",
    "    comp_dict_time={'completeness_df_time':comp_df, 'completeness_nova_time':comp_nova}        \n",
    "    return comp_dict_time\n",
    "\n",
    "def duplicates(window):\n",
    "    repeated=len(window.index)-window['fechaHora'].nunique()\n",
    "    duplic=1-repeated/len(window.index)\n",
    "    dupli_dict_time={'duplicates_time':duplic} \n",
    "    return dupli_dict_time\n",
    "\n",
    "def eval_dq(arguments):\n",
    "    nodes=arguments[0]\n",
    "    CC=arguments[1]\n",
    "    SS=arguments[2]\n",
    "    Distances=arguments[3]\n",
    "    start_time=arguments[4]\n",
    "    end_time=arguments[5]\n",
    "    \n",
    "    #1. For each citizen science (CC) node, get the groups (HOURLY GROUPS).\n",
    "    #node_dataset=CC[nodes]\n",
    "    node_dataset=CC[nodes][(CC[nodes]['fechaHora'] >= start_time) & (CC[nodes]['fechaHora'] <= end_time)]\n",
    "    #print(node_dataset)\n",
    "    #times = pd.to_datetime(node_dataset.fechaHora)\n",
    "    hourly_groups=node_dataset.groupby([node_dataset.fechaHora.dt.floor('60min')])#Para agrupar por cada hora\n",
    "    #hourly_groups.groups.keys()# O grupos.groups para obtener las claves de cada grupo, es decir cada hora\n",
    "    del node_dataset\n",
    "    \n",
    "\n",
    "    dim_time = pd.DataFrame(\n",
    "        columns =[\"codigoSerial\",\n",
    "                  \"fechaHora\",\n",
    "                  \"precision_df_time\",\n",
    "                  \"precision_nova_time\",\n",
    "                  \"uncertainty_time\",\n",
    "                  \"accuracy_df_time\",\n",
    "                  \"accuracy_nova_time\",\n",
    "                  \"completeness_df_time\",\n",
    "                  \"completeness_nova_time\",\n",
    "                  \n",
    "                  \"concordance_df_nova_time\",\n",
    "                  \n",
    "                  \"concordance_df_siata\",#MAYBE NEED TO BE CALCULATED ON A DAILY BASIS\n",
    "                  \"concordance_df_hum_time\",\n",
    "                  \"concordance_df_temp_time\",\n",
    "                  \n",
    "                  \"concordance_nova_siata\",#MAYBE NEED TO BE CALCULATED ON A DAILY BASIS\n",
    "                  \"concordance_nova_hum_time\",\n",
    "                  \"concordance_nova_temp_time\",\n",
    "                  \"vm\",\n",
    "                  \"v\",\n",
    "                 \n",
    "                  \"duplicates_time\"])\n",
    "    \n",
    "    #2. For each group (hour in a CC node data), calculate the Dimension's DQ. (The functions should be applied to each group instead)\n",
    "    for hour in hourly_groups.groups.keys():\n",
    "        window=hourly_groups.get_group(hour)\n",
    "        \n",
    "        \n",
    "        preci_dict_time=precision(window)\n",
    "        uncer_dict_time=uncertainty(window)\n",
    "        accur_dict_time=accuracy(nodes, hour, window, Distances, SS)\n",
    "        conco_dict_time=concordance(nodes, hour, window, Distances, SS)\n",
    "        comp_dict_time=completeness(nodes, window,start_time, end_time)\n",
    "        dupli_dict_time=duplicates(window)\n",
    "        \n",
    "        DQ_dict_time={\"codigoSerial\":nodes,\"fechaHora\":pd.Timestamp(hour)}\n",
    "        DQ_dict_time.update(preci_dict_time)\n",
    "        DQ_dict_time.update(uncer_dict_time)\n",
    "        DQ_dict_time.update(accur_dict_time)\n",
    "        DQ_dict_time.update(conco_dict_time)\n",
    "        DQ_dict_time.update(comp_dict_time)\n",
    "        DQ_dict_time.update(dupli_dict_time)\n",
    "        \n",
    "        \n",
    "        #3. Save the result file in the form: Node, Group (hour), DQ_1, DQ_2, DQ3, ... , DQIndex  (This is new)\n",
    "        dim_time=dim_time.append(DQ_dict_time, ignore_index = True)\n",
    "    \n",
    "    dim_time['fechaHora']= pd.to_datetime(dim_time['fechaHora'])\n",
    "    daily_groups=dim_time.groupby([dim_time.fechaHora.dt.floor('1D')])#Para agrupar por cada día\n",
    "    print(\"here we go\")\n",
    "    #hola\n",
    "    for day in daily_groups.groups.keys():\n",
    "        day_window=daily_groups.get_group(day)\n",
    "        #dim_time.loc[dim_time.fechaHora<=day,\"concordance_df_siata\"]=day_window.v.corr(day_window.vm)\n",
    "        print((dim_time.fechaHora>=day.floor('1D'))*(dim_time.fechaHora<(day+timedelta(minutes=1)).ceil('1D')))\n",
    "        dim_time.loc[(dim_time.fechaHora>=day.floor('1D'))&(dim_time.fechaHora<(day+timedelta(minutes=1)).ceil('1D')),\"concordance_df_nova_time\"]=1\n",
    "\n",
    "        #day_window.v.corr(day_window.vm)\n",
    "    \n",
    "    return dim_time\n",
    "\n",
    "\n",
    "\n",
    "dim_time=eval_dq([100,CC,SS,Distances,start_time,end_time])\n",
    "dim_time[[\"codigoSerial\",\"fechaHora\",\"concordance_df_siata\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "print(z_crit) # 1.959963984540054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.DataFrame([1,2,np.nan,4,np.nan])\n",
    "n=np.count_nonzero(~np.isnan(df))\n",
    "df.std().values[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f295ec510066dddcc717ae6d4935b3ad0a45b533511561df7ff2b4bcd803903f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
