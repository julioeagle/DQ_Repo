{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Install Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to install if not installed yet\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install sklearn\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install haversine\n",
    "!pip install -U wxPython \n",
    "!pip install google\n",
    "!pip install google-api-core\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install google-cloud\n",
    "!pip install google-cloud-vision\n",
    "!pip install google.cloud.bigquery\n",
    "!pip install google.cloud.storage\n",
    "!pip install google-auth-oauthlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Import Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sn\n",
    "import requests\n",
    "import json\n",
    "import haversine as hs\n",
    "import wx\n",
    "import webbrowser\n",
    "\n",
    "from __future__ import print_function\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "import win32api\n",
    "\n",
    "\n",
    "\n",
    "import DQ2# Own defined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Setup Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights\n",
    "#mu                =0.9\n",
    "#Accuracy          =0.20\n",
    "#Precision         =0.07\n",
    "#Confidence        =0.16\n",
    "#Completeness      =0.10\n",
    "#Timeliness        =0.12\n",
    "#Data_Volume       =0.16\n",
    "#Data_Redundancy   =0.02\n",
    "#Concordance       =0.16\n",
    "#\n",
    "#Utility           =0.12\n",
    "#Accessibility     =0.16\n",
    "#Interpretability  =0.28\n",
    "#Reputation        =0.12\n",
    "#Artificiality     =0.20\n",
    "#Access_Security   =0.12\n",
    "\n",
    "\n",
    "mu               = 1.0\n",
    "Waccuracy        = 0.3506311521\n",
    "Wconfidence      = 0.1880884436\n",
    "Wconcordance     = 0.1768628272\n",
    "Wcompleteness    = 0.148093351\n",
    "Wprecision       = 0.09875987987\n",
    "Wdata_Redundancy = 0.03756434625\n",
    "\n",
    "#pcmWeights = [Waccuracy,Wprecision,Wconfidence,Wcompleteness,Wdata_Redundancy,Wconcordance]\n",
    "\n",
    "#Period\n",
    "#start_time =\"2019-12-01 00:00:00\"\n",
    "#end_time   =\"2019-12-31 23:59:00\"\n",
    "\n",
    "#Period for synthetic dataset 1: sinusoid-based\n",
    "#start_time =\"2021-10-03 00:00:00\"\n",
    "#end_time   =\"2021-10-04 23:59:00\"\n",
    "\n",
    "#Period for synthetic dataset 2: based on real data.\n",
    "start_time =\"2021-10-05 00:00:00\"\n",
    "end_time   =\"2021-10-07 23:59:00\"\n",
    "\n",
    "#Variable Inicialization\n",
    "#p=99 # P for the CI used in the confidence calculation IT WAS USED AS DEFAULT FOR ALL TESTS!\n",
    "#p=95 # P for the CI used in the confidence calculation\n",
    "p=99.0 # P for the CI used in the confidence calculation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Load Data and Clean Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path for Citizen Science nodes data:  C:\\Users\\julio\\Documents\\UDEA\\Maestría\\DQ in IOT\\Datasets\\Dataset for testing\\Synthetic\\2 from real\\CC_Synthetic2_Clean_test0.csv\n",
      "Source path for Siata Stations data:  C:\\Users\\julio\\Documents\\UDEA\\Maestría\\DQ in IOT\\Datasets\\Dataset for testing\\Synthetic\\2 from real\\SS_Synthetic_2.csv\n",
      "The distance files was read\n",
      "Citizen Scientist:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Siata Stations:  [90]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "#Read Data from February\n",
    "    header_CC=[\"codigoSerial\", \"fecha\", \"hora\", \"fechaHora\", \"temperatura\", \"humedad_relativa\", \"pm1_df\", \"pm10_df\", \"pm25_df\", \"pm1_nova\", \"pm10_nova\", \"pm25_nova\", \"calidad_temperatura\", \"calidad_humedad_relativa\", \"calidad_pm1_df\", \"calidad_pm10_df\", \"calidad_pm25_df\", \"calidad_pm1_nova\", \"calidad_pm10_nova\", \"calidad_pm25_nova\"]\n",
    "    datatypes_CC={\"codigoSerial\":np.uint16, \"temperatura\":np.float16, \"humedad_relativa\":np.float16, \"pm1_df\":np.float32, \"pm10_df\":np.float32, \"pm25_df\":np.float32, \"pm1_nova\":np.float32, \"pm10_nova\":np.float32, \"pm25_nova\":np.float32}\n",
    "    path_for_CC_data=DQ2.get_path('*.csv',\"Select Citizen Scientist *.csv file\")\n",
    "    #print(path_for_CC_data)\n",
    "    #df_CC = pd.read_csv(path_for_CC_data, header=None, names=header_CC, usecols=header_CC , dtype=datatypes_CC,parse_dates=[\"fecha\",\"hora\",\"fechaHora\"])\n",
    "    df_CC = pd.read_csv(path_for_CC_data, header=None, names=header_CC, usecols=header_CC , dtype=datatypes_CC,parse_dates=[\"fecha\",\"hora\",\"fechaHora\"],dayfirst=True)\n",
    "\n",
    "    #print(df_CC)\n",
    "    df_CC.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "    print(\"Source path for Citizen Science nodes data: \",path_for_CC_data)\n",
    "    \n",
    "    #Data includes January, February and March\n",
    "    header_SS=[\"Fecha_Hora\",\"codigoSerial\",\"pm25\",\"calidad_pm25\",\"pm10\",\"calidad_pm10\"]\n",
    "    datatypes_SS={\"codigoSerial\":np.uint16,\"pm25\":np.float32,\"pm10\":np.float32}\n",
    "    path_for_SS_data=DQ2.get_path('*.csv',\"Select SIATA Stations *.csv file\")\n",
    "    #df_SS = pd.read_csv(path_for_SS_data, header=None,names=header_SS, usecols=header_SS , dtype=datatypes_SS,parse_dates=[\"Fecha_Hora\"])\n",
    "    df_SS = pd.read_csv(path_for_SS_data, header=None,names=header_SS, usecols=header_SS , dtype=datatypes_SS,parse_dates=[\"Fecha_Hora\"],dayfirst=True)\n",
    "    df_SS.sort_values(by=['codigoSerial','Fecha_Hora'],ignore_index=True)\n",
    "    print(\"Source path for Siata Stations data: \",path_for_SS_data)\n",
    "    \n",
    "    \n",
    "    datatypesDistances={\"codigoSerial_CC\":np.uint16,\"codigoSerial_ES\":np.uint16,\"Distancia_a_ES\":np.float16,\"codigoSerial_ES2\":np.uint16}\n",
    "    path_for_distance_files=DQ2.get_path('*.csv',\"Select the Nodes to Siata Stations distances *.csv file\")\n",
    "    Distances = pd.read_csv(path_for_distance_files, header=0, dtype=datatypesDistances,index_col=\"codigoSerial_CC\")\n",
    "    print(\"The distance files was read\")\n",
    "    \n",
    "        \n",
    "except:\n",
    "    print(\"An exception occurred, it is possible that wrong files were chosen, please run again\")\n",
    "\n",
    "\n",
    "\n",
    "#DATA CLEANING\n",
    "CC, SS=DQ2.clean_sort_data(df_CC, df_SS)\n",
    "del df_CC\n",
    "del df_SS\n",
    "\n",
    "#CC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Count the Number of Records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records_CC:  43200 , Number of nodes:  10\n",
      "Records_SS:  72 , Number of nodes:  1\n"
     ]
    }
   ],
   "source": [
    "Records_CC=0\n",
    "NumberOfNodes=0\n",
    "for i in CC.keys():\n",
    "    CCdata=CC[i].loc[(CC[i].fechaHora<=end_time) & (CC[i].fechaHora>=start_time),]\n",
    "    Records_CC+=len(CCdata)\n",
    "    if len(CCdata)>0:\n",
    "        NumberOfNodes+=1\n",
    "print(\"Records_CC: \",Records_CC, \", Number of nodes: \",NumberOfNodes)\n",
    "\n",
    "Records_SS=0\n",
    "NumberOfStations=0\n",
    "for i in SS.keys():\n",
    "    SSdata=SS[i].loc[(SS[i].Fecha_Hora<=end_time) & (SS[i].Fecha_Hora>=start_time),]\n",
    "    Records_SS+=len(SSdata)\n",
    "    if len(SSdata)>0:\n",
    "        NumberOfStations+=1\n",
    "print(\"Records_SS: \",Records_SS, \", Number of nodes: \",NumberOfStations)\n",
    "\n",
    "del Records_SS\n",
    "del Records_CC\n",
    "del CCdata\n",
    "del SSdata\n",
    "del NumberOfNodes,NumberOfStations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. DQ Evaluation with Parallelization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2021-10-20 00:38:35.616961\n",
      "Number of avaliable CPUs:  6\n",
      "End Time:  2021-10-20 00:38:40.242437\n",
      "Elapsed Time:  4.625476360321045  Seconds, or  0.07709127267201742  Minutes\n"
     ]
    }
   ],
   "source": [
    "import DQ2# Own defined\n",
    "\n",
    "#testnumber=1\n",
    "#cpunumber=6\n",
    "#print(\"Test: \",testnumber,\". Number of CPUs: \",cpunumber)\n",
    "t0= time.time()\n",
    "print(\"Start time: \", datetime.fromtimestamp(t0))\n",
    "\n",
    "\n",
    "dim_time = pd.DataFrame(\n",
    "        columns =[\"codigoSerial\",\n",
    "                  \"fechaHora\",\n",
    "                  \"precision_df_time\",\n",
    "                  \"precision_nova_time\",\n",
    "                  \"uncertainty_time\",\n",
    "                  \"accuracy_df_time\",\n",
    "                  \"accuracy_nova_time\",\n",
    "                  \"completeness_df_time\",\n",
    "                  \"completeness_nova_time\",\n",
    "                  \"concordance_df_nova_time\",\n",
    "                  \"concordance_df_siata_time\",\n",
    "                  \"concordance_df_hum_time\",\n",
    "                  \"concordance_df_temp_time\",\n",
    "                  \"concordance_nova_siata_time\",\n",
    "                  \"concordance_nova_hum_time\",\n",
    "                  \"concordance_nova_temp_time\",\n",
    "                  \"vm_df\",\n",
    "                  \"vm_nova\",\n",
    "                  \"v\",\n",
    "                  \"duplicates_time\",\n",
    "                  \n",
    "                  \"confi_df_time\",\n",
    "                  \"confi_nova_time\"])\n",
    "\n",
    "dim_node = pd.DataFrame(\n",
    "        columns =[\"codigoSerial\",\n",
    "                  \"precision_df_node\",\n",
    "                  \"precision_nova_node\",\n",
    "                  \"uncertainty_node\",\n",
    "                  \"accuracy_df_node\",\n",
    "                  \"accuracy_nova_node\",\n",
    "                  \"completeness_df_node\",\n",
    "                  \"completeness_nova_node\",\n",
    "                  \"concordance_df_nova_node\",\n",
    "                  \"concordance_df_siata_node\",\n",
    "                  \"concordance_df_hum_node\",\n",
    "                  \"concordance_df_temp_node\",\n",
    "                  \"concordance_nova_siata_node\",\n",
    "                  \"concordance_nova_hum_node\",\n",
    "                  \"concordance_nova_temp_node\",\n",
    "                  \n",
    "                  \"duplicates_node\",\n",
    "                  \n",
    "                  \"confi_df_node\",\n",
    "                  \"confi_nova_node\",\n",
    "                  \"DQ_INDEX_NODE\"])\n",
    "\n",
    "dim_DQ = pd.DataFrame(\n",
    "        columns =[\"precision_df_total\",\n",
    "                  \"precision_nova_total\",\n",
    "                  \"uncertainty_total\",\n",
    "                  \"accuracy_df_total\",\n",
    "                  \"accuracy_nova_total\",\n",
    "                  \"completeness_df_total\",\n",
    "                  \"completeness_nova_total\",\n",
    "                  \"concordance_df_nova_total\",\n",
    "                  \"concordance_df_siata_total\",#MAYBE NEED TO BE CALCULATED ON A DAILY BASIS\n",
    "                  \"concordance_df_hum_total\",\n",
    "                  \"concordance_df_temp_total\",\n",
    "                  \"concordance_nova_siata_total\",#MAYBE NEED TO BE CALCULATED ON A DAILY BASIS\n",
    "                  \"concordance_nova_hum_total\",\n",
    "                  \"concordance_nova_temp_total\",\n",
    "                  \n",
    "                  \"duplicates_total\",\n",
    "                  \n",
    "                  \"confi_df_total\",\n",
    "                  \"confi_nova_total\",\n",
    "                  \"DQ_INDEX_TOTAL\"])\n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    print(\"Number of avaliable CPUs: \",mp.cpu_count())\n",
    "    pool=mp.Pool(processes = mp.cpu_count())\n",
    "    arguments=[]\n",
    "    #results=pool.map(DQ.eval_dq,[nodes for nodes in CC.keys()])\n",
    "    results=pool.map(DQ2.eval_dq,([[nodes, CC, SS, Distances, start_time, end_time, p] for nodes in CC.keys()]))\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(0,len(results)):\n",
    "        dim_time=dim_time.append(results[i][0], ignore_index = True)\n",
    "        dim_node=dim_node.append(results[i][1], ignore_index = True)\n",
    "\n",
    "        \n",
    "    cols =[       \"precision_df_node\",\n",
    "                  \"precision_nova_node\",\n",
    "                  \"uncertainty_node\",\n",
    "                  \"accuracy_df_node\",\n",
    "                  \"accuracy_nova_node\",\n",
    "                  \"completeness_df_node\",\n",
    "                  \"completeness_nova_node\",\n",
    "                  \"concordance_df_nova_node\",\n",
    "                  \"concordance_df_siata_node\",\n",
    "                  \"concordance_df_hum_node\",\n",
    "                  \"concordance_df_temp_node\",\n",
    "                  \"concordance_nova_siata_node\",\n",
    "                  \"concordance_nova_hum_node\",\n",
    "                  \"concordance_nova_temp_node\",\n",
    "                  \n",
    "                  \"duplicates_node\",\n",
    "                  \n",
    "                  \"confi_df_node\",\n",
    "                  \"confi_nova_node\"]    \n",
    "    dim_DQ= dim_node[cols].mean()\n",
    "    dim_DQ.rename({'precision_df_node':          'precision_df_total', \n",
    "                   'precision_nova_node':        'precision_nova_total' , \n",
    "                   'uncertainty_node':           'uncertainty_total' , \n",
    "                   'accuracy_df_node':           'accuracy_df_total', \n",
    "                   'accuracy_nova_node':         'accuracy_nova_total', \n",
    "                   'completeness_df_node':       'completeness_df_total', \n",
    "                   'completeness_nova_node':     'completeness_nova_total', \n",
    "                   'concordance_df_nova_node':   'concordance_df_nova_total', \n",
    "                   'concordance_df_siata_node':  'concordance_df_siata_total', \n",
    "                   'concordance_df_hum_node':    'concordance_df_hum_total', \n",
    "                   'concordance_df_temp_node':   'concordance_df_temp_total', \n",
    "                   'concordance_nova_siata_node':'concordance_nova_siata_total', \n",
    "                   'concordance_nova_hum_node':  'concordance_nova_hum_total', \n",
    "                   'concordance_nova_temp_node': 'concordance_nova_temp_total', \n",
    "                   'duplicates_node':            'duplicates_total', \n",
    "                   'confi_df_node':              'confi_df_total', \n",
    "                   'confi_nova_node':            'confi_nova_total'        }, axis=1, inplace=True)\n",
    "    \n",
    "print(\"End Time: \", datetime.fromtimestamp(time.time()))\n",
    "t1 = time.time() - t0\n",
    "print(\"Elapsed Time: \", t1, \" Seconds, or \",t1/60,\" Minutes\")\n",
    "#print(dim_node)\n",
    "win32api.Beep(2000, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. DQ_INDEX by a weighted average function**\n",
    "The weights come from the Pair-Wise Comparison Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case it needs to be calculated by node, not required for our report\n",
    "#dim_node[\"DQ_INDEX_NODE\"]=  Wprecision*dim_node.loc[:,[\"precision_df_node\",\"precision_nova_node\"]].mean(axis=1)+\\\n",
    "#                            Waccuracy*dim_node.loc[:,[\"accuracy_df_node\",\"accuracy_nova_node\"]].mean(axis=1)+\\\n",
    "#                            Wcompleteness*dim_node.loc[:,[\"completeness_df_node\",\"completeness_nova_node\"]].mean(axis=1)+\\\n",
    "#                            Wconfidence*dim_node.loc[:,[\"confi_df_node\",\"confi_nova_node\"]].mean(axis=1)+\\\n",
    "#                            Wconcordance*dim_node.loc[:,[\"concordance_df_nova_node\",\"concordance_df_siata_node\",\"concordance_nova_siata_node\"]].mean(axis=1)+\\\n",
    "#                            Wdata_Redundancy*dim_node.loc[:,[\"duplicates_node\"]].mean(axis=1)\n",
    "#\n",
    "#dim_node[\"precision_node\"]= Wprecision*dim_node.loc[:,[\"precision_df_node\",\"precision_nova_node\"]].mean(axis=1)\n",
    "#dim_node[\"accuracy_node\"]= Waccuracy*dim_node.loc[:,[\"accuracy_df_node\",\"accuracy_nova_node\"]].mean(axis=1)\n",
    "#dim_node[\"completeness_node\"]= Wcompleteness*dim_node.loc[:,[\"completeness_df_node\",\"completeness_nova_node\"]].mean(axis=1)\n",
    "#dim_node[\"confidence_node\"]= Wconfidence*dim_node.loc[:,[\"confi_df_node\",\"confi_nova_node\"]].mean(axis=1)\n",
    "#dim_node[\"concordance_node\"]= Wconcordance*dim_node.loc[:,[\"concordance_df_nova_node\",\"concordance_df_siata_node\",\"concordance_nova_siata_node\"]].mean(axis=1)\n",
    "#dim_node[\"redundancy_node\"]= Wdata_Redundancy*dim_node.loc[:,[\"duplicates_node\"]].mean(axis=1)\n",
    "#dim_node[\"DQ_INDEX_NODE\"]=dim_node[[\"precision_node\",\"accuracy_node\",\"completeness_node\",\"confidence_node\",\"concordance_node\",\"redundancy_node\"]].sum(axis=1)\n",
    "\n",
    "dim_DQ[\"DQ_INDEX_TOTAL\"]=   Wprecision*dim_DQ[[\"precision_df_total\",\"precision_nova_total\"]].mean()+ \\\n",
    "                            Waccuracy*dim_DQ[[\"accuracy_df_total\",\"accuracy_nova_total\"]].mean()+ \\\n",
    "                            Wcompleteness*dim_DQ[[\"completeness_df_total\",\"completeness_nova_total\"]].mean()+ \\\n",
    "                            Wconfidence*dim_DQ[[\"confi_df_total\",\"confi_nova_total\"]].mean()+ \\\n",
    "                            Wconcordance*dim_DQ[[\"concordance_df_nova_total\",\"concordance_df_siata_total\",\"concordance_nova_siata_total\"]].mean()+ \\\n",
    "                            Wdata_Redundancy*dim_DQ[[\"duplicates_total\"]].mean()\n",
    "#dim_node.head(6)\n",
    "#dim_node.loc[dim_node.codigoSerial==49,]\n",
    "\n",
    "#Need to be removed to avoid the API to crash\n",
    "dim_time.pop(\"vm_df\")\n",
    "dim_time.pop(\"vm_nova\")\n",
    "dim_time.pop(\"v\")\n",
    "print(dim_DQ)\n",
    "#dim_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Export to Google Sheets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing the DQ_TIME sheet in the Google Spreadsheet\n",
      "Clearing the DQ_NODE sheet in the Google Spreadsheet\n",
      "Clearing the DQ_TOTAL sheet in the Google Spreadsheet\n",
      "Exporting Data to Google Sheets\n",
      "Exporting Data Finished\n"
     ]
    }
   ],
   "source": [
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "         #\"https://www.googleapis.com/auth/spreadsheets\"\n",
    "\n",
    "# The ID and range of a sample spreadsheet.\n",
    "#SAMPLE_SPREADSHEET_ID = '1gkU9I2EqgJCLm-XemWq_oFenkOcsaHRKwDpO2kK7vd8' #udea account\n",
    "SPREADSHEET_ID = '1QlPuLYvWaJV6QmOTmkUM3BzuiCvM_8mnuAtvLiEFJaI' #new account\n",
    "#SAMPLE_RANGE_NAME = 'Class Data!A2:E'\n",
    "\n",
    "def main():\n",
    "    \"\"\"Shows basic usage of the Sheets API.\n",
    "    Prints values from a sample spreadsheet.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    # The file token.json stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials2.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "    # Replace undefined data with empty spaces\n",
    "    \n",
    "    dim_time.replace(np.nan, '', inplace=True)\n",
    "    dim_time['fechaHora'] = dim_time['fechaHora'].astype(str)\n",
    "    dim_time.sort_values(by=['codigoSerial','fechaHora'],ignore_index=True)\n",
    "    \n",
    "    dim_node.replace(np.nan, '', inplace=True)\n",
    "    dim_node.sort_values(by=['codigoSerial'],ignore_index=True)\n",
    "    \n",
    "    dim_DQ.replace(np.nan, '', inplace=True)\n",
    "\n",
    "        \n",
    "    # Call the Sheets API\n",
    "    print(\"Clearing the DQ_TIME sheet in the Google Spreadsheet\")\n",
    "    sheet = service.spreadsheets()\n",
    "    #request = service.spreadsheets().values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=range_, body=clear_values_request_body)\n",
    "    clear_sheet = sheet.values().clear(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range='DQ_TIME!A1:Z1000000',\n",
    "        \n",
    "    ).execute()\n",
    "    \n",
    "    print(\"Clearing the DQ_NODE sheet in the Google Spreadsheet\")\n",
    "    sheet = service.spreadsheets()\n",
    "    #request = service.spreadsheets().values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=range_, body=clear_values_request_body)\n",
    "    clear_sheet = sheet.values().clear(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range='DQ_NODE!A1:Z1000000',\n",
    "        \n",
    "    ).execute()\n",
    "    \n",
    "    print(\"Clearing the DQ_TOTAL sheet in the Google Spreadsheet\")\n",
    "    sheet = service.spreadsheets()\n",
    "    #request = service.spreadsheets().values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=range_, body=clear_values_request_body)\n",
    "    clear_sheet = sheet.values().clear(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range='DQ_TOTAL!A1:Z1000000',\n",
    "        \n",
    "    ).execute()\n",
    "\n",
    "    print(\"Exporting Data to Google Sheets\")\n",
    "    sheet = service.spreadsheets()\n",
    "    write_data = sheet.values().update(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='DQ_TIME!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=dim_time.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "    \n",
    "    sheet = service.spreadsheets()\n",
    "    write_data = sheet.values().update(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='DQ_NODE!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=dim_node.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "    \n",
    "    sheet = service.spreadsheets()\n",
    "    write_data = sheet.values().update(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        valueInputOption='RAW',\n",
    "        range='DQ_TOTAL!A1',\n",
    "        body=dict(\n",
    "            majorDimension='ROWS',\n",
    "            values=dim_DQ.T.reset_index().T.values.tolist())\n",
    "    ).execute()\n",
    "    \n",
    "    print(\"Exporting Data Finished\")\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    win32api.Beep(2000, 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. To Open the Spreadsheet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#webbrowser.open('https://docs.google.com/spreadsheets/d/1gkU9I2EqgJCLm-XemWq_oFenkOcsaHRKwDpO2kK7vd8/edit?usp=sharing')#udea\n",
    "webbrowser.open('https://docs.google.com/spreadsheets/d/1QlPuLYvWaJV6QmOTmkUM3BzuiCvM_8mnuAtvLiEFJaI/edit?usp=sharing')#data quality account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. To Open the Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#webbrowser.open('https://datastudio.google.com/s/ietWLq_iL-8')#\"udea account\n",
    "webbrowser.open('https://datastudio.google.com/s/hy-ZhY6eEfU')#data quality account"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f295ec510066dddcc717ae6d4935b3ad0a45b533511561df7ff2b4bcd803903f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
